[
  {
    "objectID": "posts/2021-11-13-backward-pass.html",
    "href": "posts/2021-11-13-backward-pass.html",
    "title": "Simple Neural Net Backward Pass",
    "section": "",
    "text": "In this post we attempt to connect the math to the code for what Jeremy Howard did in his 03_backprop.ipynb notebook for the 2022 part 2 course."
  },
  {
    "objectID": "posts/2021-11-13-backward-pass.html#gradient-of-loss-with-respect-to-the-activations-ai",
    "href": "posts/2021-11-13-backward-pass.html#gradient-of-loss-with-respect-to-the-activations-ai",
    "title": "Simple Neural Net Backward Pass",
    "section": "Gradient of loss with respect to the Activations \\(a^{(i)}\\)",
    "text": "Gradient of loss with respect to the Activations \\(a^{(i)}\\)\n\\(J\\left(\\{y^{(i)}\\}_{i=1}^{N},\\{a^{(i)}\\}_{i=1}^{N}\\right) = \\frac{1}{N}\\sum_{i=1}^{N}(y^{(i)}-a^{(i)})^{2}\\) is the loss (mean squared error) across the \\(N\\) training examples\nThe backward function of the Mse class computes an estimate of how the loss function changes as the input activations change.\n\nclass Mse():\n    def __call__(self, inp, targ):\n        self.inp,self.targ = inp,targ\n        self.out = mse(inp, targ)\n        return self.out\n    \n    def backward(self):\n        N = self.targ.shape[0]\n        A = self.inp\n        Y = self.targ\n        dJ_dA = (2./N) * (A.squeeze() - Y).unsqueeze(-1)\n        self.inp.g = dJ_dA\n\nThe change in the loss as the \\(i\\)-th activation changes is given by\n\\[\\frac{\\partial J}{\\partial a^{(i)}} = \\frac{2}{N}\\sum_{i=1}^{N} \\frac{\\partial (y^{(i)}-a^{(i)})^{2}}{\\partial a^{(i)}} = \\frac{2}{N}(y^{(i)}-a^{(i)})\\frac{\\partial (y^{(i)}-a^{(i)}) }{\\partial a^{(i)}} = \\frac{2}{N}(a^{(i)}-y^{(i)})\\]\nwhere the last step follows because \\(\\frac{\\partial (y^{(i)}-a^{(i)}) }{\\partial a^{(i)}} = 0-1 =-1\\).\nThe change in the loss as a function of the change in activations from our training examples is captured by the \\(N \\times 1\\) matrix:\n\\[\n\\frac{\\partial J}{\\partial \\mathbf{a}} = \\begin{bmatrix}\n\\frac{\\partial J}{\\partial a^{(1)}} \\\\\n\\frac{\\partial J}{\\partial a^{(2)}} \\\\\n\\vdots \\\\\n\\frac{\\partial J}{\\partial a^{(N)}}\n\\end{bmatrix}\n= \\begin{bmatrix}\n\\frac{2}{N}\\left(a^{(1)}-y^{(1)}\\right) \\\\\n\\frac{2}{N}\\left(a^{(2)}-y^{(2)}\\right) \\\\\n\\vdots \\\\\n\\frac{2}{N}\\left(a^{(N)}-y^{(N)}\\right)\n\\end{bmatrix}\n\\]\nFrom the implementation perspective, the activations \\(\\{a^{(i)}\\}_{i=1}^{N}\\) and the targets \\(\\{y^{(i)}\\}_{i=1}^{N}\\) are passed in and stored during the forward pass (specifically in the dunder __call__ method). In the backward pass these are retrieved and \\(\\frac{\\partial J}{\\partial \\mathbf{a}}\\) is computed and stored for access by the backward function of the prior layer. Hopefully the backward method for the Mse class makes sense now."
  },
  {
    "objectID": "posts/2021-11-13-backward-pass.html#gradient-of-loss-with-respect-to-the-linear-output-zi",
    "href": "posts/2021-11-13-backward-pass.html#gradient-of-loss-with-respect-to-the-linear-output-zi",
    "title": "Simple Neural Net Backward Pass",
    "section": "Gradient of loss with respect to the Linear Output \\(z^{(i)}\\)",
    "text": "Gradient of loss with respect to the Linear Output \\(z^{(i)}\\)\n\nclass Relu():\n    def __call__(self, inp):\n        self.inp = inp\n        self.out = inp.clamp_min(0.)\n        return self.out\n    \n    def backward(self): \n        dJ_dA = self.out.g\n        dA_dZ = (self.inp>0).float() \n        \n        # Note this is an elementwise multiplication         \n        dJ_dZ = dJ_dA * dA_dZ\n        \n        self.inp.g = dJ_dZ\n\nHow does the loss change as the output of the linear unit changes?\n\\[\\frac{\\partial J}{\\partial z^{(i)}} = \\frac{\\partial J}{\\partial a^{(i)}} \\frac{\\partial a^{(i)}}{\\partial z^{(i)}} = \\frac{2}{N}\\left(a^{(i)}-y^{(i)}\\right)\\frac{\\partial a^{(i)}}{\\partial z^{(i)}}\\]\nFor the ReLU activation function we have that, \\[\n\\frac{\\partial a^{(i)}}{\\partial z^{(i)}} =\n\\begin{cases}\n  1 & z^{(i)} \\gt 0 \\\\\n  0 & z^{(i)} \\leq 0\n\\end{cases}\n\\]\nand hence\n\\[\\frac{\\partial \\mathbf{a}}{\\partial \\mathbf{z}} =\n\\begin{bmatrix}\n\\frac{\\partial a^{(1)}}{\\partial z^{(1)}} \\\\\n\\frac{\\partial a^{(2)}}{\\partial z^{(2)}} \\\\\n\\vdots \\\\\n\\frac{\\partial a^{(N)}}{\\partial z^{(N)}}\n\\end{bmatrix}\n\\]\nThus the change in the loss as a function of the change in the output from the linear unit on our training examples is given by the \\(N \\times 1\\) matrix:\n\\[\n\\frac{\\partial J}{\\partial \\mathbf{z}} = \\begin{bmatrix}\n\\frac{\\partial J}{\\partial z^{(1)}} \\\\\n\\frac{\\partial J}{\\partial z^{(2)}} \\\\\n\\vdots \\\\\n\\frac{\\partial J}{\\partial z^{(N)}}\n\\end{bmatrix}\n= \\begin{bmatrix}\n\\frac{\\partial J}{\\partial a^{(1)}}\\frac{\\partial a^{(1)}}{\\partial z^{(1)}} \\\\\n\\frac{\\partial J}{\\partial a^{(2)}}\\frac{\\partial a^{(2)}}{\\partial z^{(2)}} \\\\\n\\vdots \\\\\n\\frac{\\partial J}{\\partial a^{(N)}}\\frac{\\partial a^{(N)}}{\\partial z^{(N)}}\n\\end{bmatrix}\n\\]\nSo \\(\\frac{\\partial J}{\\partial \\mathbf{z}}\\) ends up being an elementwise product between the corresponding entries of \\(\\frac{\\partial J}{\\partial \\mathbf{a}}\\) and \\(\\frac{\\partial \\mathbf{a}}{\\partial \\mathbf{z}}\\).\nFrom the implementation perspective, in the backward pass \\(\\frac{\\partial \\mathbf{a}}{\\partial \\mathbf{z}}\\) will be computed locally and multiplied elementwise with \\(\\frac{\\partial J}{\\partial \\mathbf{a}}\\) (this will have been computed in the backward pass in the Mse class and will be available to access when the backward function of the Relu function is called)."
  },
  {
    "objectID": "posts/2021-11-13-backward-pass.html#gradient-of-loss-with-respect-to-w_j-b-and-x",
    "href": "posts/2021-11-13-backward-pass.html#gradient-of-loss-with-respect-to-w_j-b-and-x",
    "title": "Simple Neural Net Backward Pass",
    "section": "Gradient of loss with respect to \\(w_{j}, b\\) and \\(X\\)",
    "text": "Gradient of loss with respect to \\(w_{j}, b\\) and \\(X\\)\nThe next three subsections will explain the backward function of the Lin class.\n\nclass Lin():\n    def __init__(self, w, b): self.w,self.b = w,b\n\n    def __call__(self, inp):\n        self.inp = inp\n        self.out = lin(inp, self.w, self.b)\n        return self.out\n\n    def backward(self):\n        # See Gradient of loss with respect to w_j         \n        dJ_dZ = self.out.g\n        X = self.inp\n        dJ_dW = X.t() @ dJ_dZ\n        self.w.g = dJ_dW\n        \n        # See Gradient of loss with respect to the bias b\n        dJ_db = dJ_dZ.sum(0)\n        self.b.g = dJ_db\n        \n        # See Gradient of loss with respect to X\n        dJ_dX = dJ_dZ @ self.w.t()\n        self.inp.g = dJ_dX\n\n\nGradient of loss with respect to \\(w_{j}\\)\nHow does the loss react when we wiggle \\(w_{j}\\)? \\[\\frac{\\partial J}{\\partial w_{j}} = \\frac{\\partial J}{\\partial \\mathbf{z}} \\frac{\\partial \\mathbf{z}}{\\partial w_{j}}= \\sum_{i=1}^{N}\\frac{\\partial J}{\\partial z^{(i)}} \\frac{\\partial z^{(i)}}{\\partial w_{j}} = \\frac{\\partial J}{\\partial z^{(1)}}x^{(1)}_{j} + \\frac{\\partial J}{\\partial z^{(2)}}x^{(2)}_{j} + \\ldots + \\frac{\\partial J}{\\partial z^{(N)}}x^{(N)}_{j}\\]\nThus,\n\\[\n\\frac{\\partial J}{\\partial \\mathbf{w}} = \\begin{bmatrix}\n\\frac{\\partial J}{\\partial w_{1}} \\\\\n\\frac{\\partial J}{\\partial w_{2}} \\\\\n\\vdots \\\\\n\\frac{\\partial J}{\\partial w_{d}}\n\\end{bmatrix}\n= \\begin{bmatrix}\nx^{(1)}_{1}\\frac{\\partial J}{\\partial z^{(1)}} + x^{(2)}_{1}\\frac{\\partial J}{\\partial z^{(2)}} + \\ldots + x^{(N)}_{1}\\frac{\\partial J}{\\partial z^{(N)}} \\\\\nx^{(1)}_{2}\\frac{\\partial J}{\\partial z^{(1)}} + x^{(2)}_{2}\\frac{\\partial J}{\\partial z^{(2)}} + \\ldots + x^{(N)}_{2}\\frac{\\partial J}{\\partial z^{(N)}} \\\\\n\\vdots \\\\\nx^{(1)}_{d}\\frac{\\partial J}{\\partial z^{(1)}} + x^{(2)}_{d}\\frac{\\partial J}{\\partial z^{(2)}} + \\ldots + x^{(N)}_{d}\\frac{\\partial J}{\\partial z^{(N)}}\n\\end{bmatrix}\n\\]\nWe can write this more compactly as:\n\\[\n\\frac{\\partial J}{\\partial \\mathbf{w}} = \\begin{bmatrix}\nx^{(1)}_1 & x^{(2)}_1 & \\cdots & x^{(N)}_1\\\\\nx^{(1)}_2 & x^{(2)}_2 & \\cdots & x^{(N)}_2\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\nx^{(1)}_d & x^{(2)}_d & \\cdots & x^{(N)}_d\n\\end{bmatrix}\n\\begin{bmatrix}\n\\frac{\\partial J}{\\partial z^{(1)}} \\\\\n\\frac{\\partial J}{\\partial z^{(2)}} \\\\\n\\vdots \\\\\n\\frac{\\partial J}{\\partial z^{(N)}}\n\\end{bmatrix}\n=\\mathbf{X}^{T}\\frac{\\partial J}{\\partial \\mathbf{z}}\n\\]\nFrom the implementation perspective \\(\\mathbf{X}^{T}\\) is computed locally in the backward function of the Lin class while \\(\\frac{\\partial J}{\\partial \\mathbf{z}}\\) is ready and waiting to be accessed. Recall the latter was computed in the backward function of the Relu class.\n\n\nGradient of loss with respect to the bias \\(b\\)\n\\[\\frac{\\partial J}{\\partial b} = \\frac{\\partial J}{\\partial \\mathbf{z}} \\frac{\\partial \\mathbf{z}}{\\partial b}= \\sum_{i=1}^{N}\\frac{\\partial J}{\\partial z^{(i)}} \\frac{\\partial z^{(i)}}{\\partial b}= \\sum_{i=1}^{N}\\frac{\\partial J}{\\partial z^{(i)}} 1 = \\frac{\\partial J}{\\partial z^{(1)}} + \\frac{\\partial J}{\\partial z^{(2)}} + \\ldots + \\frac{\\partial J}{\\partial z^{(N)}}\\]\nFrom the implementation perspective we need to access \\(\\frac{\\partial J}{\\partial \\mathbf{z}}\\) and sum across the first axis. The local derivative computation of \\(\\frac{\\partial \\mathbf{z}}{\\partial b}\\) is particulary simple (since each \\(\\frac{\\partial z^{(i)}}{\\partial b}\\) is just \\(1\\)).\n\n\nGradient of loss with respect to \\(x^{(i)}_{j}\\)\nLet’s understand how the loss will change as we twiddle the \\(j\\)-th feature of the \\(i\\)-th training example.\n\\[\\frac{\\partial J}{\\partial x^{(i)}_{j}} = \\frac{\\partial J}{\\partial \\mathbf{z}} \\frac{\\partial \\mathbf{z}}{\\partial x^{(i)}_{j}}= \\sum_{k=1}^{N}\\frac{\\partial J}{\\partial z^{(k)}} \\frac{\\partial z^{(k)}}{\\partial x^{(i)}_{j}} = \\frac{\\partial J}{\\partial z^{(i)}}\\frac{\\partial z^{(i)}}{\\partial x^{(i)}_{j}} + \\sum_{k: k \\neq i}^{N} \\frac{\\partial J}{\\partial z^{(k)}}\\frac{\\partial z^{(k)}}{\\partial x^{(i)}_{j}}\\]\nSince \\(\\frac{\\partial z^{(k)}}{\\partial x^{(i)}_{j}} = 0\\) for any \\(k \\neq i\\) we get\n\\[\\frac{\\partial J}{\\partial x^{(i)}_{j}} = \\frac{\\partial J}{\\partial z^{(i)}}w_{j}\\]\nThus the \\(N \\times d\\) matrix of these gradients are,\n\\[\n\\frac{\\partial J}{\\partial \\mathbf{X}} =\n\\begin{bmatrix}\n\\frac{\\partial J}{\\partial x^{(1)}_{1}} & \\frac{\\partial J}{\\partial x^{(1)}_{2}} & \\cdots & \\frac{\\partial J}{\\partial x^{(1)}_{d}} \\\\\n\\frac{\\partial J}{\\partial x^{(2)}_{1}} & \\frac{\\partial J}{\\partial x^{(2)}_{2}} & \\cdots & \\frac{\\partial J}{\\partial x^{(2)}_{d}} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial J}{\\partial x^{(N)}_{1}} & \\frac{\\partial J}{\\partial x^{(N)}_{2}} & \\cdots & \\frac{\\partial J}{\\partial x^{(N)}_{d}}\n\\end{bmatrix}=\n\\begin{bmatrix}\n\\frac{\\partial J}{\\partial z^{(1)}}w_{1} & \\frac{\\partial J}{\\partial z^{(1)}}w_{2} & \\cdots & \\frac{\\partial J}{\\partial z^{(1)}}w_{d} \\\\\n\\frac{\\partial J}{\\partial z^{(2)}}w_{1} & \\frac{\\partial J}{\\partial z^{(2)}}w_{2} & \\cdots & \\frac{\\partial J}{\\partial z^{(2)}}w_{d} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial J}{\\partial z^{(N)}}w_{1} & \\frac{\\partial J}{\\partial z^{(N)}}w_{2} & \\cdots & \\frac{\\partial J}{\\partial z^{(N)}}w_{d}\n\\end{bmatrix}\n\\]\nMore compactly this can be represented as an outer product,\n\\[\n\\frac{\\partial J}{\\partial \\mathbf{X}} =\n\\begin{bmatrix}\n\\frac{\\partial J}{\\partial z^{(1)}} \\\\\n\\frac{\\partial J}{\\partial z^{(2)}} \\\\\n\\vdots \\\\\n\\frac{\\partial J}{\\partial z^{(N)}}\n\\end{bmatrix}\n\\begin{bmatrix}\nw_{1} & w_{2} & \\cdots & w_{d}\n\\end{bmatrix}\n=\\frac{\\partial J}{\\partial \\mathbf{z}}\\mathbf{w}^{T}\n\\]"
  },
  {
    "objectID": "posts/2021-08-14-collaborative-filtering-on-movie-lens-with-cross-entropy-loss.html",
    "href": "posts/2021-08-14-collaborative-filtering-on-movie-lens-with-cross-entropy-loss.html",
    "title": "Collaborative Filtering With Cross Entropy Loss",
    "section": "",
    "text": "Introduction\nWe work through a problem posed in Chapter 8 of [1].\n\nfrom fastai.collab import *\nfrom fastai.tabular.all import *\npath = untar_data(URLs.ML_100k)\n\n\n    \n        \n      \n      100.15% [4931584/4924029 00:00<00:00]\n    \n    \n\n\n\nPath.BASE_PATH = path\npath.ls()\n\n(#23) [Path('u1.base'),Path('u1.test'),Path('u4.test'),Path('allbut.pl'),Path('u.item'),Path('ua.test'),Path('u.occupation'),Path('u3.test'),Path('u5.base'),Path('ub.test')...]\n\n\n\nratings = pd.read_csv(path/'u.data', delimiter='\\t', header=None,\n                     names=['user','movie','rating','timestamp'])\nratings.head()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      user\n      movie\n      rating\n      timestamp\n    \n  \n  \n    \n      0\n      196\n      242\n      3\n      881250949\n    \n    \n      1\n      186\n      302\n      3\n      891717742\n    \n    \n      2\n      22\n      377\n      1\n      878887116\n    \n    \n      3\n      244\n      51\n      2\n      880606923\n    \n    \n      4\n      166\n      346\n      1\n      886397596\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nWe see that the rating can be one of the following values:\n\nratings.rating.unique()\n\narray([3, 1, 2, 4, 5])\n\n\n\nmovies = pd.read_csv(path/'u.item', delimiter='|', encoding='latin-1',\n                     usecols=(0,1), names=('movie','title'), header=None)\nmovies.head()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      movie\n      title\n    \n  \n  \n    \n      0\n      1\n      Toy Story (1995)\n    \n    \n      1\n      2\n      GoldenEye (1995)\n    \n    \n      2\n      3\n      Four Rooms (1995)\n    \n    \n      3\n      4\n      Get Shorty (1995)\n    \n    \n      4\n      5\n      Copycat (1995)\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nratings = ratings.merge(movies)\nratings.head()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      user\n      movie\n      rating\n      timestamp\n      title\n    \n  \n  \n    \n      0\n      196\n      242\n      3\n      881250949\n      Kolya (1996)\n    \n    \n      1\n      63\n      242\n      3\n      875747190\n      Kolya (1996)\n    \n    \n      2\n      226\n      242\n      5\n      883888671\n      Kolya (1996)\n    \n    \n      3\n      154\n      242\n      3\n      879138235\n      Kolya (1996)\n    \n    \n      4\n      306\n      242\n      5\n      876503793\n      Kolya (1996)\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\nDataLoaders\n\ndls = CollabDataLoaders.from_df(ratings, item_name='title', bs=64)\ndls.show_batch()\n\n\n\n  \n    \n      \n      user\n      title\n      rating\n    \n  \n  \n    \n      0\n      328\n      Star Trek VI: The Undiscovered Country (1991)\n      3\n    \n    \n      1\n      251\n      Toy Story (1995)\n      4\n    \n    \n      2\n      456\n      Judgment Night (1993)\n      2\n    \n    \n      3\n      653\n      Raiders of the Lost Ark (1981)\n      5\n    \n    \n      4\n      551\n      Aliens (1986)\n      4\n    \n    \n      5\n      416\n      Edge, The (1997)\n      4\n    \n    \n      6\n      116\n      Good Will Hunting (1997)\n      3\n    \n    \n      7\n      500\n      Chain Reaction (1996)\n      3\n    \n    \n      8\n      393\n      Barb Wire (1996)\n      3\n    \n    \n      9\n      227\n      Father of the Bride Part II (1995)\n      3\n    \n  \n\n\n\n\n\nProbabilistic Matrix Factorization baseline\nUnderneath the covers we have a dot product model (where the dot product is between the user embedding and the item embedding). We use L2 regularization (wd=0.5) and use y_range to make sure the sigmoid_range can get to a rating of \\(5\\). Recall that in the sigmoid function only an activation of \\(\\infty\\) will get us a \\(1\\). This will use the MSE as the loss function.\n\nlearn = collab_learner(dls, n_factors=50, y_range=(0,5.5))\nlearn.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.938077\n      0.954516\n      00:09\n    \n    \n      1\n      0.845044\n      0.872871\n      00:09\n    \n    \n      2\n      0.723613\n      0.843354\n      00:18\n    \n    \n      3\n      0.586253\n      0.829404\n      00:09\n    \n    \n      4\n      0.491200\n      0.829491\n      00:10\n    \n  \n\n\n\nSo \\(0.829\\) is what we want to beat!\n\n\nDeep Learning baseline\nInstead of a Dot Product model we use a model where the user and the movie embeddings are concatenated together and then passed through a neural net. We continue to use MSE as our loss.\n\nlearn2 = collab_learner(dls, n_factors=50, use_nn=True, layers=[100], y_range=(0,5.5))\nlearn2.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.991514\n      0.999929\n      00:12\n    \n    \n      1\n      0.940058\n      0.930773\n      00:13\n    \n    \n      2\n      0.867342\n      0.879026\n      00:12\n    \n    \n      3\n      0.810829\n      0.855366\n      00:12\n    \n    \n      4\n      0.753963\n      0.852470\n      00:12\n    \n  \n\n\n\nThe model is as follows:\n\nlearn2.model\n\nEmbeddingNN(\n  (embeds): ModuleList(\n    (0): Embedding(944, 74)\n    (1): Embedding(1665, 102)\n  )\n  (emb_drop): Dropout(p=0.0, inplace=False)\n  (bn_cont): BatchNorm1d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (layers): Sequential(\n    (0): LinBnDrop(\n      (0): Linear(in_features=176, out_features=100, bias=False)\n      (1): ReLU(inplace=True)\n      (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): LinBnDrop(\n      (0): Linear(in_features=100, out_features=1, bias=True)\n    )\n    (2): SigmoidRange(low=0, high=5.5)\n  )\n)\n\n\n\n\nCross Entropy Loss\nTurn each rating into a class label by subtracting one. Recall we saw that ratings are integers from \\(1\\) to \\(5\\).\n\nratings['class'] = ratings['rating'] - 1\nratings.head(n=5)\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      user\n      movie\n      rating\n      timestamp\n      title\n      class\n    \n  \n  \n    \n      0\n      196\n      242\n      3\n      881250949\n      Kolya (1996)\n      2\n    \n    \n      1\n      63\n      242\n      3\n      875747190\n      Kolya (1996)\n      2\n    \n    \n      2\n      226\n      242\n      5\n      883888671\n      Kolya (1996)\n      4\n    \n    \n      3\n      154\n      242\n      3\n      879138235\n      Kolya (1996)\n      2\n    \n    \n      4\n      306\n      242\n      5\n      876503793\n      Kolya (1996)\n      4\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nratings['class'].unique()\n\narray([2, 4, 3, 1, 0])\n\n\n\ndls3 = CollabDataLoaders.from_df(ratings, item_name='title', rating_name='class', bs=64)\ndls3.show_batch()\n\n\n\n  \n    \n      \n      user\n      title\n      class\n    \n  \n  \n    \n      0\n      581\n      Return of the Jedi (1983)\n      2\n    \n    \n      1\n      159\n      Courage Under Fire (1996)\n      3\n    \n    \n      2\n      521\n      Long Kiss Goodnight, The (1996)\n      3\n    \n    \n      3\n      533\n      Stupids, The (1996)\n      0\n    \n    \n      4\n      77\n      Brazil (1985)\n      3\n    \n    \n      5\n      12\n      Lion King, The (1994)\n      3\n    \n    \n      6\n      605\n      Usual Suspects, The (1995)\n      3\n    \n    \n      7\n      561\n      Richard III (1995)\n      2\n    \n    \n      8\n      246\n      Doors, The (1991)\n      0\n    \n    \n      9\n      658\n      Boot, Das (1981)\n      4\n    \n  \n\n\n\nCreate a class that is able to output as many activations as there are classes.\n\nclass CollabNN(Module):\n    def __init__(self, user_sz, item_sz, n_act=100, n_classes=5):\n        self.user_factors = Embedding(*user_sz)\n        self.item_factors = Embedding(*item_sz)\n        self.layers = nn.Sequential(\n            nn.Linear(user_sz[1]+item_sz[1], n_act),\n            nn.ReLU(),\n            nn.Linear(n_act, n_classes))\n        \n    def forward(self, x):\n        embs = self.user_factors(x[:,0]),self.item_factors(x[:,1])\n        return self.layers(torch.cat(embs, dim=1))\n\n\nembs = get_emb_sz(dls3)\nembs\n\n[(944, 74), (1665, 102)]\n\n\n\nmodel = CollabNN(*embs)\n\nExamine a batch of inputs and outputs\n\nx, y = dls3.one_batch()\nx[:3], y[:3]\n\n(tensor([[ 763, 1211],\n         [ 474, 1006],\n         [ 261,  734]]), tensor([[3],\n         [4],\n         [3]], dtype=torch.int8))\n\n\nGet activations by passing through the model\n\nmodel(x[:3])\n\ntensor([[ 0.1065, -0.0083, -0.1098, -0.1151, -0.0471],\n        [ 0.1053, -0.0053, -0.1108, -0.1145, -0.0490],\n        [ 0.1017, -0.0110, -0.1114, -0.1135, -0.0478]],\n       grad_fn=<AddmmBackward0>)\n\n\nApply softmax to the activations\n\ntorch.softmax(model(x[:3]), dim=-1)\n\ntensor([[0.2296, 0.2047, 0.1849, 0.1839, 0.1969],\n        [0.2293, 0.2053, 0.1848, 0.1841, 0.1965],\n        [0.2289, 0.2045, 0.1850, 0.1846, 0.1971]], grad_fn=<SoftmaxBackward0>)\n\n\nTake the argmax to get the prediction of the class\n\ntorch.softmax(model(x[:3]), dim=-1).argmax(dim=-1)\n\ntensor([0, 0, 0])\n\n\nUnsqueeze to match the shape of the y’s\n\ntorch.softmax(model(x[:3]), dim=-1).argmax(dim=-1).unsqueeze(-1)\n\ntensor([[0],\n        [0],\n        [0]])\n\n\nConvert to float and compute the mse_loss\n\nF.mse_loss(torch.softmax(model(x[:3]), dim=-1).argmax(dim=-1).unsqueeze(-1).float(), y[:3])\n\ntensor(11.3333)\n\n\nCreate our custom mse function that can get a loss from these activations for the different classes. We do this so we can compare the validation loss metric against the previous baselines. Clearly cross entropy loss cannot be compared directly to the mean squared error.\n\ndef ks_mse(inp, targ, axis=-1):\n    sm_acts = torch.softmax(inp, dim=-1)\n    preds = sm_acts.argmax(dim=axis).unsqueeze(-1)\n    return F.mse_loss(preds.float(), targ)\n\nCompare with our custom function\n\nks_mse(model(x[:3]), y[:3])\n\ntensor(11.3333)\n\n\n\nlearn3 = Learner(dls3, model, loss_func=CrossEntropyLossFlat(), metrics=[accuracy, ks_mse])\n\n\nlrs = learn3.lr_find()\n\n\n\n\n\n\n\n\nlrs\n\nSuggestedLRs(valley=0.0012022644514217973)\n\n\n\nlearn3.fit_one_cycle(5, lrs.valley)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      ks_mse\n      time\n    \n  \n  \n    \n      0\n      1.177951\n      1.238908\n      0.444100\n      1.217100\n      00:13\n    \n    \n      1\n      1.186859\n      1.232669\n      0.448450\n      1.215550\n      00:13\n    \n    \n      2\n      1.168161\n      1.228795\n      0.452750\n      1.146900\n      00:12\n    \n    \n      3\n      1.132046\n      1.233022\n      0.451050\n      1.154100\n      00:12\n    \n    \n      4\n      1.096163\n      1.237107\n      0.450800\n      1.162600\n      00:13\n    \n  \n\n\n\nUnfortunately, in this attempt, we came up short and were unable to beat the baseline.\nFinally, let’s use a confusion matrix to visualize where this model is coming up short.\n\nraw_preds, targs, decoded_preds = learn3.get_preds(with_decoded=True)\nraw_preds[:3], decoded_preds[:3], targs[:3]\n\n\n\n\n(tensor([[0.0255, 0.2153, 0.2651, 0.4588, 0.0353],\n         [0.0089, 0.0586, 0.2399, 0.4129, 0.2797],\n         [0.0028, 0.1422, 0.3484, 0.3990, 0.1076]]),\n tensor([3, 3, 3]),\n tensor([[4],\n         [4],\n         [3]], dtype=torch.int8))\n\n\n\nfrom sklearn.metrics import ConfusionMatrixDisplay\nConfusionMatrixDisplay.from_predictions(targs, decoded_preds)\n\n<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay>\n\n\n\n\n\n\nfrom sklearn.metrics import classification_report\nprint(classification_report(targs, decoded_preds, labels=[0, 1, 2, 3, 4]))\n\n              precision    recall  f1-score   support\n\n           0       0.45      0.35      0.39      1216\n           1       0.34      0.19      0.25      2254\n           2       0.43      0.42      0.43      5437\n           3       0.46      0.56      0.51      6931\n           4       0.49      0.47      0.48      4162\n\n    accuracy                           0.45     20000\n   macro avg       0.43      0.40      0.41     20000\nweighted avg       0.44      0.45      0.44     20000\n\n\n\n\n\n\n\n\nReferences\n\n[1] J. Howard and S. Gugger, Deep learning for coders with fastai and PyTorch: AI applications without a PhD, 1st ed. O’Reilly, 2020."
  },
  {
    "objectID": "posts/2021-12-08-chvatal-chapter-1-problem-5.html",
    "href": "posts/2021-12-08-chvatal-chapter-1-problem-5.html",
    "title": "Chvátal Chapter 1 Problem 5",
    "section": "",
    "text": "Problem Statement\nFrom (page 10 Chapter 1) [1].\nProblem 1.7\n\n\n\nProve or disprove: If Problem 1.7 above is unbounded, then there is a subscript \\(k\\) such that the Problem 1.5 below is unbounded.\nProblem 1.5\n\n\n\n\n\nProof\nAssume without loss of generality that the objective function coefficients \\(c_{j}\\) of all decision variables, \\(x_{j}\\), in Problem 1.7 are non-zero.\nWe see that Problem 1.5 defines \\(n\\) sub-problems, one for each decision variable \\(x_k\\) in Problem 1.7. Now, suppose there is no subscript \\(k\\) for which the sub-problems are unbounded. This means we can identify constants \\(M_{1},\\ldots,M_{n}\\) which are the maximum objective function values for each sub-problem in Problem 1.5. So, for example, \\(M_1\\) is the largest value \\(x_{1}\\) can take while still staying feasible.\nNext, let \\(I_{c_{j} \\gt 0}\\) be an indicator function that tells us with a one or a zero whether the coefficient of decision variable \\(x_{j}\\) in the objective function of Problem 1.7 is strictly positive.\nNow we claim that \\(M=\\sum_{j=1}^{n}c_{j}I_{c_{j} \\gt 0}M_{j}\\) is the greatest upper bound on the objective function of Problem 1.7.\nIt is possible that if some objective function coefficient \\(c_{l}\\) is negative that a more accurate upper bound can be obtained by solving a minimization version of Problem 1.5. Solving this minimization would give us the smallest \\(x_{l}\\) for which the problem remains feasible. But since \\(c_{l} \\lt 0\\) having \\(x_{l} >0\\) will only serve to reduce \\(\\sum_{j=1}^{n}c_{j}x_{j}\\).\nWhat this means is that \\(M\\) is the greatest upper bound on the objective function of Problem 1.7.\nHowever, the existence of a finite upper bound contradicts the unboundedness of Problem 1.7 since for an unbounded problem we can always find a feasible solution such that \\(\\sum_{j=1}^{n}c_{j}x_{j} \\gt M\\) for any value of \\(M\\).\n\n\n\n\n\nReferences\n\n[1] V. Chvátal, Linear programming. Macmillan, 1983."
  },
  {
    "objectID": "posts/2021-10-23-fastai-reproducibility-note.html",
    "href": "posts/2021-10-23-fastai-reproducibility-note.html",
    "title": "Reproducible runs with fastai",
    "section": "",
    "text": "Reproducibility can end up being important when trying to isolate the impact of the changes that happen as we tweak models.\n\nfrom fastai.vision.all import *\n\nGrab the pets dataset.\n\npath = untar_data(URLs.PETS)/'images'\ndef is_cat(x): return x[0].isupper()\n\n\n    \n        \n      \n      100.00% [811712512/811706944 00:09<00:00]\n    \n    \n\n\nCreate a data loader passing in a seed. Next create a learner and fine tune the resnet34 model for 1 epoch.\n\ndls = ImageDataLoaders.from_name_func(\n    path, get_image_files(path), valid_pct=0.2, seed=21,\n    label_func=is_cat, item_tfms=Resize(224))\n\nlearn = cnn_learner(dls, resnet34, metrics=error_rate)\nlearn.fine_tune(1)\n\nDownloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n\n\n\n\n\n/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.129521\n      0.022127\n      0.007442\n      01:10\n    \n  \n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.056711\n      0.023975\n      0.010149\n      01:18\n    \n  \n\n\n\nWe end up with an error rate of \\(0.010149\\).\nLet’s do another round where we recreate the dataloaders, the learner and fine tune again for a single epoch. Since we have used the same seed we will get the same final result, right?\n\ndls = ImageDataLoaders.from_name_func(\n    path, get_image_files(path), valid_pct=0.2, seed=21,\n    label_func=is_cat, item_tfms=Resize(224))\n\nlearn = cnn_learner(dls, resnet34, metrics=error_rate)\nlearn.fine_tune(1)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.140996\n      0.024327\n      0.007442\n      01:07\n    \n  \n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.058567\n      0.012324\n      0.004736\n      01:18\n    \n  \n\n\n\nWrong!\nThe train_loss, valid_loss and the error rate at the end of the two rounds are different."
  },
  {
    "objectID": "posts/2021-10-23-fastai-reproducibility-note.html#can-we-omit-the-call-to-set_seed-in-a-subsequent-run",
    "href": "posts/2021-10-23-fastai-reproducibility-note.html#can-we-omit-the-call-to-set_seed-in-a-subsequent-run",
    "title": "Reproducible runs with fastai",
    "section": "Can we omit the call to set_seed in a subsequent run?",
    "text": "Can we omit the call to set_seed in a subsequent run?\n\ndls = ImageDataLoaders.from_name_func(\n    path, get_image_files(path), valid_pct=0.2,\n    label_func=is_cat, item_tfms=Resize(224))\n\nlearn = cnn_learner(dls, resnet34, metrics=error_rate)\nlearn.fine_tune(1)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.161395\n      0.019973\n      0.006766\n      01:42\n    \n  \n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.070191\n      0.034742\n      0.012855\n      02:20\n    \n  \n\n\n\nNice try but no."
  },
  {
    "objectID": "posts/2021-10-23-fastai-reproducibility-note.html#can-we-omit-the-reproducibletrue-in-the-call-to-set_seed",
    "href": "posts/2021-10-23-fastai-reproducibility-note.html#can-we-omit-the-reproducibletrue-in-the-call-to-set_seed",
    "title": "Reproducible runs with fastai",
    "section": "Can we omit the reproducible=True in the call to set_seed?",
    "text": "Can we omit the reproducible=True in the call to set_seed?\n\nset_seed(21)\n\ndls = ImageDataLoaders.from_name_func(\n    path, get_image_files(path), valid_pct=0.2,\n    label_func=is_cat, item_tfms=Resize(224))\n\nlearn = cnn_learner(dls, resnet34, metrics=error_rate)\nlearn.fine_tune(1)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.151476\n      0.018651\n      0.006766\n      01:43\n    \n  \n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.042918\n      0.015299\n      0.006766\n      02:21\n    \n  \n\n\n\n\nset_seed(21)\n\ndls = ImageDataLoaders.from_name_func(\n    path, get_image_files(path), valid_pct=0.2,\n    label_func=is_cat, item_tfms=Resize(224))\n\nlearn = cnn_learner(dls, resnet34, metrics=error_rate)\nlearn.fine_tune(1)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.151476\n      0.018651\n      0.006766\n      01:43\n    \n  \n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.042918\n      0.015299\n      0.006766\n      02:21\n    \n  \n\n\n\nSeems like we can 🤷 but I would keep it since the code of the set_seed function suggests it is being used for cudnn."
  },
  {
    "objectID": "posts/2021-10-23-fastai-reproducibility-note.html#can-we-avoid-recreating-the-dataloaders-from-scratch",
    "href": "posts/2021-10-23-fastai-reproducibility-note.html#can-we-avoid-recreating-the-dataloaders-from-scratch",
    "title": "Reproducible runs with fastai",
    "section": "Can we avoid recreating the dataloaders from scratch?",
    "text": "Can we avoid recreating the dataloaders from scratch?\nSpoiler alert: No!\n\nset_seed(21, reproducible=True)\n\nlearn = cnn_learner(dls, resnet34, metrics=error_rate)\nlearn.fine_tune(1)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.161448\n      0.013740\n      0.004060\n      01:42\n    \n  \n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.048693\n      0.012253\n      0.003383\n      02:20"
  },
  {
    "objectID": "posts/2021-12-11-chvatal-chapter-2-simplex-implementation.html",
    "href": "posts/2021-12-11-chvatal-chapter-2-simplex-implementation.html",
    "title": "Chvátal Chapter 2 Simplex Implementation",
    "section": "",
    "text": "Code\n!pip install rich\nThe data corresponding to this problem and other problem instances can be obtained from the methods in the collapsed code cell below."
  },
  {
    "objectID": "posts/2021-12-11-chvatal-chapter-2-simplex-implementation.html#constants.",
    "href": "posts/2021-12-11-chvatal-chapter-2-simplex-implementation.html#constants.",
    "title": "Chvátal Chapter 2 Simplex Implementation",
    "section": "Constants.",
    "text": "Constants.\n\nm = A.shape[0] # number of constraints\nn = A.shape[1] # number of decision variables\n\nFIRST_ROW = 0\nALL_BUT_LAST_ROW = -1\n\nFIRST_COLUMN = 0\nSECOND_COLUMN = 1\n\nZ_ROW = m"
  },
  {
    "objectID": "posts/2021-12-11-chvatal-chapter-2-simplex-implementation.html#identify-the-variable-to-enter-the-basis",
    "href": "posts/2021-12-11-chvatal-chapter-2-simplex-implementation.html#identify-the-variable-to-enter-the-basis",
    "title": "Chvátal Chapter 2 Simplex Implementation",
    "section": "Identify the variable to enter the basis",
    "text": "Identify the variable to enter the basis\nStrictly speaking the method identifies the column corresponding to the variable that is going to be entering the basis.\n\n\nCode\ndef get_entering_column(chvatal_dict):\n  entering_column = None\n\n  # and second column and after since we are using\n  # the first column to store the RHSs.\n  z_coeffs = chvatal_dict[ Z_ROW , SECOND_COLUMN : ]\n\n  if np.max( z_coeffs ) > 0:\n    # the column that enters from the dictionary\n    # is one with the largest positive coefficient\n    # (since we are maximizing)\n    entering_column = np.argmax( z_coeffs ) + 1\n    # we need to add one since we looked from the\n    # second column onwards\n\n  return entering_column"
  },
  {
    "objectID": "posts/2021-12-11-chvatal-chapter-2-simplex-implementation.html#identify-the-variable-to-leave-the-basis",
    "href": "posts/2021-12-11-chvatal-chapter-2-simplex-implementation.html#identify-the-variable-to-leave-the-basis",
    "title": "Chvátal Chapter 2 Simplex Implementation",
    "section": "Identify the variable to leave the basis",
    "text": "Identify the variable to leave the basis\nThe method identifies the row corresponding to the variable that is going to be leaving the basis.\n\n\nCode\ndef get_leaving_row(chvatal_dict, entering_column):\n  cd = chvatal_dict[ FIRST_ROW : ALL_BUT_LAST_ROW, : ]\n\n  cd_first_column = cd[:, FIRST_COLUMN].copy()\n  cd_entering_column = -cd[:, entering_column].copy()\n  \n  # avoid a divide by zero - identify if any\n  # coefficients in the entering column are zero\n  if sum(cd_entering_column==0):\n    # set the numerator to -1\n    cd_first_column[cd_entering_column==0] = -1\n    # now replace the 0 in the denominators with 1\n    cd_entering_column[cd_entering_column==0] = 1\n    # This ensures that such columns are not considered\n    # when it comes time to decide the leaving variable\n    # Also note that after this step\n    # all entries in cd_entering_column==0 will be False\n\n  leaving_row_candidates = cd_first_column / cd_entering_column\n  # print(leaving_row_candidates)\n\n  # Identify the leaving variable\n  leaving_row = None\n  if np.min( leaving_row_candidates ) < 0:\n    # Replace candidates violating non-negativity by infinity\n    # so these will be ignored when choosing the leaving variable\n    leaving_row_candidates[ leaving_row_candidates < 0 ] = np.inf\n\n  # the row that leaves from the dictionary\n  # is the one that will impose the strictest\n  # constraint on the requirement that the\n  # variable remain non-negative\n  leaving_row = np.argmin( leaving_row_candidates )\n\n  #print(leaving_row)\n  return leaving_row"
  },
  {
    "objectID": "posts/2021-12-11-chvatal-chapter-2-simplex-implementation.html#construct-the-system-for-the-next-iteration",
    "href": "posts/2021-12-11-chvatal-chapter-2-simplex-implementation.html#construct-the-system-for-the-next-iteration",
    "title": "Chvátal Chapter 2 Simplex Implementation",
    "section": "Construct the system for the next iteration",
    "text": "Construct the system for the next iteration\n\n\nCode\ndef get_pivot_row(chvatal_dict, leaving_row, entering_column):\n  # Need to find the coefficient of the non-basic variable\n  # that is going to be entering the basis.\n  pivot = -1 * chvatal_dict[leaving_row, entering_column]\n  \n  # the pivot_array expresses the newly arrived \n  # basic variable in terms of the non-basic variables\n  next_pivot_row = chvatal_dict[leaving_row,:]/pivot\n  \n  # the coefficent of the newly arrived non-basic variable\n  # it is moving from the LHS to the RHS hence the -1\n  next_pivot_row[entering_column] = -1./pivot\n  \n  return pivot, next_pivot_row   \n\ndef get_next_dictionary(chvatal_dict, leaving_row, entering_column,\n                           row_lookup, column_lookup):\n  _, pivot_array = get_pivot_row(chvatal_dict,\n                                 leaving_row,\n                                 entering_column)\n\n  # Update the remaining rows in the dictionary\n  # so they are now expressed in terms of the new arrived\n  # non-basic variable.\n  # m+1 since we also need to update the row for z\n  cd_candidate_rows = []\n  for j in range(m+1):\n    if j == leaving_row:\n      cd_candidate_rows.append( pivot_array )\n    else:\n      # the coefficient of the leaving non-basic variable\n      multiplier = chvatal_dict[j, entering_column]\n      \n      multiplier_times_pivot_array = multiplier * pivot_array\n      updated_row_array = multiplier_times_pivot_array +  chvatal_dict[j, : ]\n      \n      # correct the multiplier for the newly entered non-basic variable\n      updated_row_array[entering_column] = multiplier_times_pivot_array[entering_column]\n      \n      # print(updated_row_array)\n      cd_candidate_rows.append( updated_row_array )\n\n  cd_next_it = np.vstack( cd_candidate_rows )\n\n  # assemble the updated list of basic variables\n  basic_next_it = row_lookup.copy()\n  # update the leaving row with the entering new basic variable\n  basic_next_it[leaving_row] = column_lookup[entering_column]\n\n  # assemble the updated list of nonbasic variables\n  nonbasic_next_it = column_lookup.copy()\n  # update the entering column with the new nonbasic variable\n  nonbasic_next_it[entering_column] = row_lookup[leaving_row]\n\n  #print(cd_next_it)\n\n  return basic_next_it, nonbasic_next_it, cd_next_it"
  },
  {
    "objectID": "posts/2021-12-11-chvatal-chapter-2-simplex-implementation.html#helper-functions",
    "href": "posts/2021-12-11-chvatal-chapter-2-simplex-implementation.html#helper-functions",
    "title": "Chvátal Chapter 2 Simplex Implementation",
    "section": "Helper functions",
    "text": "Helper functions\n\n\nCode\ndef get_row_variable_names(row_lookup):\n  row_names = {}\n  for row_index in row_lookup:\n    row_names[row_index] = variable_id_to_name[row_lookup[row_index]]\n    # print(f'row_index {row_index} variable {variable_to_name[row_lookup[row_index]]}')\n  # Last row is for z \n  row_names[Z_ROW] = 'z'\n  return row_names\n\ndef get_column_variable_names(col_lookup):\n  col_names = {}\n\n  # The first column contains RHSs\n  col_names[0] = 'RHS'\n  \n  for col_index in col_lookup:\n    col_names[col_index] = variable_id_to_name[col_lookup[col_index]]\n    # print(f'column_index {col_index} variable {variable_to_name[col_lookup[col_index]]}')\n  return col_names\n\ndef get_chvatal_df(cd, row_lookup, col_lookup):\n  row_names = get_row_variable_names(row_lookup)\n  col_names = get_column_variable_names(col_lookup)\n  return pd.DataFrame(cd,\n             columns=[col_names[k] for k in sorted(col_names)],\n             index = [row_names[k] for k in sorted(row_names)])\n\n    # get the keys of nonbasic_next_iteration in the sorted order of values\n    # values here are nonbasic variables. Each key is a column index \n    # ordered_columns = sorted(col_lookup, \n    #                       key=col_lookup.__getitem__)"
  },
  {
    "objectID": "posts/2021-12-11-chvatal-chapter-2-simplex-implementation.html#the-implementation",
    "href": "posts/2021-12-11-chvatal-chapter-2-simplex-implementation.html#the-implementation",
    "title": "Chvátal Chapter 2 Simplex Implementation",
    "section": "The implementation",
    "text": "The implementation\n\nnext_iteration = True\n\n# iteration counter\nit = 0\n\nvariable_id_to_name = dict(zip(range(n+m),[f'x_{i+1}' for i in range(n+m)]))\n\n# variable ids that will take on zero values\nnonbasic = [i for i in range(n)]\n\n# variable ids that will take on nonzero values\nbasic = [i for i in range(n,n+m)]\n\nrow_lookup_it = {}\n# row_lookup_it[i] is a dictionary telling us the id of the \n# decision variable corresponding to any row in the Chvátal \n# dictionary for iteration i\nrow_lookup_it[it] = dict( zip(range(m), basic) )\n\ncol_lookup_it = {}\n# col_lookup_it[i] is a dictionary telling us the id of the \n# decision variable corresponding to any column (strictly greater than 0) \n# in the Chvátal dictionary for iteration i\n# We ignore column 0 since we use it to store the right hand sides.\ncol_lookup_it[it] = dict( zip(range(1,n+1), nonbasic) )\n\nz_star = 0\n\ncd_it = {}\ncd_it[it] = np.vstack( [ np.hstack([b, -A]), np.vstack( [ [z_star] , c ]).T ] )\n\ncd_df = get_chvatal_df(cd_it[it],\n                       row_lookup_it[it],\n                       col_lookup_it[it])\nprint(f'Iteration {it}\\n Chvátal dictionary \\n {cd_df}')\n\nif np.min( cd_it[it][ FIRST_ROW : ALL_BUT_LAST_ROW, FIRST_COLUMN ] ) < 0:\n  # This means that some basic variable takes on a negative\n  # value in the starting solution.\n  print('Stopping. Infeasible starting solution.')\nelse:\n  while next_iteration:\n    entering_column = get_entering_column(cd_it[it])\n\n    if entering_column is None:\n      next_iteration = False\n    else:\n      # get the identifier for the variable that corresponds to\n      # the entering column\n      entering_var_id = col_lookup_it[it][entering_column]\n      entering_var_name = variable_id_to_name[entering_var_id]\n      print(f' Entering variable {entering_var_name}')\n\n      leaving_row = get_leaving_row(cd_it[it], entering_column)\n\n      # get the identifier for the variable that corresponds to\n      # the leaving row\n      leaving_var_id = row_lookup_it[it][leaving_row]\n      leaving_var_name = variable_id_to_name[leaving_var_id]\n      print(f' Leaving variable {leaving_var_name}\\n')\n\n      basic, nonbasic, cd = get_next_dictionary(cd_it[it],\n                                                leaving_row,\n                                                entering_column,\n                                                row_lookup_it[it],\n                                                col_lookup_it[it])\n      \n      # Update\n      it += 1\n\n      row_lookup_it[it] = basic\n      col_lookup_it[it] = nonbasic\n      cd_it[it] = cd\n\n      # Pretty print the updated dictionary\n      cd_df = get_chvatal_df(cd,\n                            basic,\n                            nonbasic)\n      print(f'Iteration {it}\\n Chvátal dictionary \\n {cd_df}')\n\nIteration 0\n Chvátal dictionary \n      RHS  x_1  x_2  x_3\nx_4    5   -2   -3   -1\nx_5   11   -4   -1   -2\nx_6    8   -3   -4   -2\nz      0    5    4    3\n\n\n\n Entering variable x_1\n\n\n\n Leaving variable x_4\n\n\n\n\nIteration 1\n Chvátal dictionary \n       RHS  x_4  x_2  x_3\nx_1   2.5 -0.5 -1.5 -0.5\nx_5   1.0  2.0  5.0  0.0\nx_6   0.5  1.5  0.5 -0.5\nz    12.5 -2.5 -3.5  0.5\n\n\n\n Entering variable x_3\n\n\n\n Leaving variable x_6\n\n\n\n\nIteration 2\n Chvátal dictionary \n       RHS  x_4  x_2  x_6\nx_1   2.0 -2.0 -2.0  1.0\nx_5   1.0  2.0  5.0 -0.0\nx_3   1.0  3.0  1.0 -2.0\nz    13.0 -1.0 -3.0 -1.0"
  },
  {
    "objectID": "posts/2021-08-12-multi-label-classification.html",
    "href": "posts/2021-08-12-multi-label-classification.html",
    "title": "Multi-Label Classification",
    "section": "",
    "text": "In a previous post we saw our Bear classifier confidently predict that an image of a Maine Coone was a Teddy bear. Roughly speaking the problem, in a multi-class setting, is that the exponential (in the softmax) pushes the class with the highest activation to receive a score close to \\(1\\). So while our classifier had never seen a Maine Coone when training, at inference time the image of the Maine Coone happened to push the activations for the Teddy Bear class higher than those for the other classes (Black Bear and Grizzly).\nIn this post we use multi-label classification to solve this problem following Chapter 6 of [1] and the note in [2]. A multi-label classification problem (as opposed to multi-class classification) is one where there can be multiple labels, instead of there being a single correct class, for every object.\nWe want our multi-label classifier to predict the different labels present in the object or no label if it is not confident about the presence of any of the labels. For this we switch to using Binary Cross Entropy as our loss function (and will also come up with a threshold probability for our classifier to use for inference).\n\n\n\nfrom fastai.vision.all import *\n\nSuppose the following are the activations, on a single training example, for the different labels possible in a multi-label classification problem. Pretend that we can have as many as three labels for each image.\n\nactivations = torch.randn(1,3)*3\nactivations\n\ntensor([[ 0.7260, -3.3489, -2.3491]])\n\n\nNext suppose the correct labels in this case are the first and third label. A one-hot encoded representation of the labels for this single training example is as follows.\n\ntargets = tensor([[1,0,1]])\ntargets\n\ntensor([[1, 0, 1]])\n\n\nThe first step is to take the sigmoid activations for each label to convert each activation into a probability score.\n\nsigmoid_activations = activations.sigmoid()\nsigmoid_activations\n\ntensor([[0.6739, 0.0339, 0.0871]])\n\n\nThe next step is to compute the binary cross entropy loss using\n\nbce_loss = -torch.where(targets==1, sigmoid_activations, 1-sigmoid_activations).log()\nbce_loss\n\ntensor([[0.3946, 0.0345, 2.4403]])\n\n\nWhen a label is present, in the target, we take the sigmoid activation corresponding to that label. Otherwise, when the label is absent, we take one minus the that sigmoid activation. By doing this we are asking for the confidence that the classifier places on the particular label being absent.\nThus the binary cross entropy function summarizes the “correctness” of the classifer across the presence and absence of each possible label in a training example. Contrast this with the use of cross entropy loss in multi-class classification where we only care about the probability being predicted for the single correct class of the training example.\n\nresults = torch.reshape(torch.cat([activations, sigmoid_activations, targets, bce_loss], 0),(4,3))\npd.DataFrame(results, index=['activations', 'sigmoid','target','loss'])\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n    \n  \n  \n    \n      activations\n      0.725968\n      -3.348915\n      -2.349130\n    \n    \n      sigmoid\n      0.673920\n      0.033931\n      0.087135\n    \n    \n      target\n      1.000000\n      0.000000\n      1.000000\n    \n    \n      loss\n      0.394644\n      0.034520\n      2.440297\n    \n  \n\n\n\n\nEach row in the dataframe above shows us the step needed to go from an activation to the loss associated with each possible label of a single training example. We can get the losses directly by\n\nF.binary_cross_entropy_with_logits(activations, targets.float(), reduction=\"none\")\n\ntensor([[0.3946, 0.0345, 2.4403]])\n\n\nOr by\n\nnn.BCEWithLogitsLoss(reduction='none')(activations, targets.float())\n\ntensor([[0.3946, 0.0345, 2.4403]])\n\n\nOf course we will want to take the mean of the loss across the labels using any of the two methods below\n\nF.binary_cross_entropy_with_logits(activations, targets.float()), nn.BCEWithLogitsLoss()(activations, targets.float())\n\n(tensor(0.9565), tensor(0.9565))\n\n\n\n\n\nIn the preceding the labels were hard. In other words, we knew with certainty the labels that are present for any training example. What if we only knew the probability with which a label would be present in a training example?\nFeel free to skip to the next section as this is somewhat of a digression.\nIn the example below we have four training examples and only a single label. The activations for the training examples are as below:\n\nactivations = tensor([[0.0122],[0.2294],[-0.1563],[-0.1483]])\nactivations.shape\n\ntorch.Size([4, 1])\n\n\nApply sigmoid to get the predictions for whether the label is present.\n\nsigmoid_activations = activations.sigmoid()\nsigmoid_activations\n\ntensor([[0.5030],\n        [0.5571],\n        [0.4610],\n        [0.4630]])\n\n\nGet the predictions for whether the label is absent (i.e., some other label is present).\n\none_minus_sigmoid_activations = (1. - sigmoid_activations)\none_minus_sigmoid_activations\n\ntensor([[0.4970],\n        [0.4429],\n        [0.5390],\n        [0.5370]])\n\n\nThe targets for each example is as follows. This says that in the first training example label 0 is present with probability 0.5, similarly the probability for label 0 being present in the second example is 0.29 and so on.\n\ntargets = tensor([0.5, 0.29, 0.36, 0.03])\ntargets.shape\n\ntorch.Size([4])\n\n\nCompute the binary cross entropy loss for each training example.\n\nbce_loss = -1 * ( targets*sigmoid_activations.flatten().log() + (1. - targets)*( one_minus_sigmoid_activations.flatten().log() ) )\nbce_loss\n\ntensor([0.6932, 0.7479, 0.6743, 0.6262])\n\n\nCompute the average binary cross entropy loss across training examples.\n\nbce_loss.mean()\n\ntensor(0.6854)\n\n\nThe above can just be done as a one-liner using BCEWithLogitsLossFlat\n\nBCEWithLogitsLossFlat(reduction='mean')(activations, targets)\n\nTensorBase(0.6854)\n\n\nNote that we cannot just use nn.BCEWithLogitsLoss because the input tensor isn’t flattened and we will get an error.\nValueError: Target size (torch.Size([4])) must be the same as input size (torch.Size([4, 1]))\nNow suppose we have two examples and three soft labels per example.\n\nactivations = tensor([[0.0122, 0.001, 0.5],[0.2294, 0.3, 0.75]])\nactivations.shape\n\ntorch.Size([2, 3])\n\n\nThe soft labels are:\n\ntargets = tensor([0.9, 0.1, 0.2],[0.25, 0.5, 0.33 ])\ntargets.shape\n\ntorch.Size([2, 3])\n\n\nWe can compute the binary cross entropy loss in the cumbersome way as below:\n\nbce_loss = -1 * ( targets * activations.sigmoid().log() + (1. - targets)*( 1.0 - activations.sigmoid() ).log() )\nbce_loss, bce_loss.sum(), bce_loss.mean()\n\n(tensor([[0.6883, 0.6935, 0.8741],\n         [0.7571, 0.7044, 0.8894]]), tensor(4.6067), tensor(0.7678))\n\n\nOr just use the one-liner to get the average loss.\n\nnn.BCEWithLogitsLoss(reduction='mean')(activations, targets)\n\ntensor(0.7678)\n\n\nI find it useful to think of each individual training example as being replicated as many times as there are possible labels. Then 2 training examples each with 3 possible labels is better thought of as 6 training examples. Hence the average loss, in this setting, requires us to divide by \\(2*3 = 6\\).\nNote that we could reasonably think that the loss for each label would be summed up into a single loss per training example and we would then divide that by \\(2\\) but do me a favor and don’t walk down that path!"
  },
  {
    "objectID": "posts/2021-08-12-multi-label-classification.html#set-up-data-block",
    "href": "posts/2021-08-12-multi-label-classification.html#set-up-data-block",
    "title": "Multi-Label Classification",
    "section": "Set up Data Block",
    "text": "Set up Data Block\nWe will need a multicategory block and a function that takes a single label and converts it into a list.\n\ndef get_label_list(some_label): return [some_label]\n\ndblock = DataBlock(\n    blocks=(ImageBlock, MultiCategoryBlock),\n    get_items=get_image_files,\n    splitter=RandomSplitter(valid_pct=0.2,seed=0),\n    get_y= Pipeline([parent_label, get_label_list]),\n    item_tfms=RandomResizedCrop(228, min_scale=0.5), \n    batch_tfms=aug_transforms())\n\ndls = dblock.dataloaders(path)\n\n\ndls.show_batch(max_n=8, nrows=2)"
  },
  {
    "objectID": "posts/2021-08-12-multi-label-classification.html#train-model",
    "href": "posts/2021-08-12-multi-label-classification.html#train-model",
    "title": "Multi-Label Classification",
    "section": "Train Model",
    "text": "Train Model\nFor the model we will use the fastai’s BCEWithLogitsLossFlat to compute the binary cross entropy loss. Another item to note is the use of accuracy_multi as the metric. Accuracy multi is defined as\ndef accuracy_multi(inp, targ, thresh=0.5, sigmoid=True):\n    \"Compute accuracy when `inp` and `targ` are the same size.\"\n    inp,targ = flatten_check(inp,targ)\n    if sigmoid: inp = inp.sigmoid()\n    return ((inp>thresh)==targ.bool()).float().mean()\nThis shows that accuracy_multi uses the threshold to determine whether a label is present and then takes a mean across the labels for every example in the inputs. The training process does not care about the threshold it only cares about minimizing the loss function.\n\nlearn = cnn_learner(dls, resnet18, metrics=[partial(accuracy_multi, thresh=0.95),\\\n                                            APScoreMulti()], \n                    loss_func=BCEWithLogitsLossFlat())\nlearn.fine_tune(4)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy_multi\n      average_precision_score\n      time\n    \n  \n  \n    \n      0\n      1.037557\n      0.579388\n      0.783333\n      0.868948\n      00:07\n    \n  \n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy_multi\n      average_precision_score\n      time\n    \n  \n  \n    \n      0\n      0.497570\n      0.354649\n      0.883333\n      0.960842\n      00:09\n    \n    \n      1\n      0.386510\n      0.170127\n      0.961111\n      0.987437\n      00:09\n    \n    \n      2\n      0.306835\n      0.135320\n      0.966667\n      0.989491\n      00:09\n    \n    \n      3\n      0.263357\n      0.124831\n      0.966667\n      0.989491\n      00:09\n    \n  \n\n\n\nLet’s look at the results.\n\nlearn.show_results()"
  },
  {
    "objectID": "posts/2021-08-12-multi-label-classification.html#sadness-revisited",
    "href": "posts/2021-08-12-multi-label-classification.html#sadness-revisited",
    "title": "Multi-Label Classification",
    "section": "Sadness revisited",
    "text": "Sadness revisited\nLet’s test with our Maine Coon image.\n\ntest_mainecoon_image_location = '../test_images/mc.jpg'\nim = Image.open(test_mainecoon_image_location)\nim.to_thumb(128,128)\n\n\n\n\n\nlearn.predict(test_mainecoon_image_location)\n\n\n\n\n((#2) ['black','teddy'],\n tensor([ True, False,  True]),\n tensor([0.5806, 0.0633, 0.8982]))\n\n\nWhat happened? Our multi-label classifier still chooses to predict some of the labels.\nLooking at the predicted probabilities we see that none has a score of greater than 0.95 so this means that the threshold we passed in for accuracy_multi is not being used during inference.\nIf we look at the source of learn.predict we see that it calls out to learn.get_preds(…, with_decoded=True). Per the documentation of get_predict with_decoded=True will also return the decoded predictions using the decodes function of the loss function (if it exists).\nLooking at the definition of BCEWithLogitsLossFlat we see the presence of the decodes method.\nclass BCEWithLogitsLossFlat(BaseLoss):\n    \"Same as `nn.BCEWithLogitsLoss`, but flattens input and target.\"\n    @use_kwargs_dict(keep=True, weight=None, reduction='mean', pos_weight=None)\n    def __init__(self, *args, axis=-1, floatify=True, thresh=0.5, **kwargs):\n        if kwargs.get('pos_weight', None) is not None and kwargs.get('flatten', None) is True:\n            raise ValueError(\"`flatten` must be False when using `pos_weight` to avoid a RuntimeError due to shape mismatch\")\n        if kwargs.get('pos_weight', None) is not None: kwargs['flatten'] = False\n        super().__init__(nn.BCEWithLogitsLoss, *args, axis=axis, floatify=floatify, is_2d=False, **kwargs)\n        self.thresh = thresh\n\n    def decodes(self, x):    return x>self.thresh\n    def activation(self, x): return torch.sigmoid(x)\nThus, all we need to do is pass the threshold into BCEWithLogitsLossFlat.\n\nlearn.loss_func = BCEWithLogitsLossFlat(thresh=0.95)\nlearn.predict(test_mainecoon_image_location)\n\n\n\n\n((#0) [], tensor([False, False, False]), tensor([0.5806, 0.0633, 0.8982]))\n\n\nMuch better. Now none of the labels are predicted.\nTo get the best threshold we first get all the predictions on the validation set.\n\npreds,targs = learn.get_preds()\n\n\n\n\nSince the predictions from get_preds has the sigmoid applied already we will pass sigmoid=False when assessing the accuracy_multi at different threshold values .\n\nxs = torch.linspace(0.05,0.995,100)\naccs = [accuracy_multi(preds, targs, thresh=i, sigmoid=False) for i in xs]\nplt.plot(xs,accs);\n\n\n\n\nLooks like our choice of threshold is fine. One more thing to note is that for multi-label problems it is useful to look at the average precision score using APScoreMulti. This is because on harder datasets it will provide a decidely less rosier view of classifier performance than accuracy_multi.\nAs a future step it would be nice to do threshold selection for each label following [3]."
  },
  {
    "objectID": "posts/2021-08-07-cross-entropy-loss-pytorch.html",
    "href": "posts/2021-08-07-cross-entropy-loss-pytorch.html",
    "title": "A note on Cross Entropy loss with PyTorch",
    "section": "",
    "text": "Suppose we have \\(N\\) training examples and we have a multi-class problem such that each training example belongs to one and only one out of \\(K\\) possible classes. Let \\(C(i) \\in \\{1,\\ldots, K\\}\\) be the correct class for the \\(i\\)-th training example and \\(o^{[C(i)]}_{i}\\) is the probability assigned by a classifier to the correct class for the \\(i\\)-th training example. We want this classifier to maximize: \\[\\prod_{i=1}^{N} o^{[C(i)]}_{i}\\]\nIf the classifier assigns a probability of \\(1\\) to the correct class for \\(N-1\\) training examples and a probability of \\(0\\) for the \\(N\\)-th example then the entire product shown above becomes zero. So to maximize this product of probabilities, the classifier has to assign a high probability to the correct class for each and every training example.\nNow, maximizing the product is equivalent to maximizing \\[ln(\\prod_{i=1}^{N}o^{[C(i)]}_{i}) = \\sum_{i=1}^{N}ln(o^{[C(i)]}_{i})\\]\nThis is the same as minimizing the sum of the negative log likelihoods \\[-\\sum_{i=1}^{N}ln(o^{[C(i)]}_{i})\\]\nThe above can now serve as a loss function for an optimization routine.\nRecall that Cross Entropy = \\(-\\sum_{k=1}^{K}y^{[k]}ln(o^{[k]})\\) where \\(y\\) is the reference distribution over \\(K\\) classes while our predictions over the \\(K\\) classes is given by \\(o\\). Observe that this summation will collapse to being a single term when, in the reference distribution \\(y\\), only one of the classes has a probability of \\(1\\).\nThus \\(-\\sum_{i=1}^{N}ln(o^{[C(i)]}_{i})\\) can be interpreted as the sum of cross entropy losses across all examples.\n\nfrom fastai.vision.all import *\n\nPretend the following are the activations of each class of a multiclass classification problem. So we have 6 examples and in each row we have the activation for each class the example could belong to.\n\nactivations = torch.randn((6,2))*2\nactivations\n\ntensor([[-1.6453,  1.8893],\n        [ 1.9800,  1.7681],\n        [ 2.8183,  4.6643],\n        [-0.3635, -0.0614],\n        [ 0.4064, -0.4668],\n        [-3.3801,  3.2484]])\n\n\nSuppose the correct class of each example is as follows\n\ntargets = tensor([0,1,0,1,1,0])\ntargets\n\ntensor([0, 1, 0, 1, 1, 0])\n\n\nTake the softmax of the activations\n\nsm_acts = torch.softmax(activations, dim=1)\nsm_acts\n\ntensor([[0.0283, 0.9717],\n        [0.5528, 0.4472],\n        [0.1363, 0.8637],\n        [0.4250, 0.5750],\n        [0.7054, 0.2946],\n        [0.0013, 0.9987]])\n\n\nExtract the probabilities predicted for the correct class.\n\nidx = range(6)\nlist(idx)\n\n[0, 1, 2, 3, 4, 5]\n\n\n\np_correct_class = sm_acts[idx, targets]\np_correct_class\n\ntensor([0.0283, 0.4472, 0.1363, 0.5750, 0.2946, 0.0013])\n\n\nTake the log of the softmax activations\n\ntorch.log(sm_acts)\n\ntensor([[-3.5634e+00, -2.8753e-02],\n        [-5.9281e-01, -8.0469e-01],\n        [-1.9925e+00, -1.4659e-01],\n        [-8.5559e-01, -5.5344e-01],\n        [-3.4895e-01, -1.2222e+00],\n        [-6.6298e+00, -1.3213e-03]])\n\n\nComputing the softmax of the activations and then taking the log is equivalent to applying PyTorch’s log_softmax function directly to the original activations. We want to do the latter because it will faster and more accurate.\n\ntorch.log_softmax(activations, dim=1)\n\ntensor([[-3.5634e+00, -2.8753e-02],\n        [-5.9281e-01, -8.0469e-01],\n        [-1.9925e+00, -1.4659e-01],\n        [-8.5559e-01, -5.5344e-01],\n        [-3.4895e-01, -1.2222e+00],\n        [-6.6298e+00, -1.3213e-03]])\n\n\nLet’s compute the mean of cross entropy losses across the training examples:\n\n-1*torch.log(p_correct_class), (-1*torch.log(p_correct_class)).mean()\n\n(tensor([3.5634, 0.8047, 1.9925, 0.5534, 1.2222, 6.6298]), tensor(2.4610))\n\n\nWe can just use Pytorch to compute this directly\n\nnn.CrossEntropyLoss(reduction='none')(activations, targets), nn.CrossEntropyLoss()(activations, targets)\n\n(tensor([3.5634, 0.8047, 1.9925, 0.5534, 1.2222, 6.6298]), tensor(2.4610))\n\n\nor by using:\n\nF.cross_entropy(activations, targets, reduction='none'), F.cross_entropy(activations, targets)\n\n(tensor([3.5634, 0.8047, 1.9925, 0.5534, 1.2222, 6.6298]), tensor(2.4610))"
  },
  {
    "objectID": "posts/2021-08-07-cross-entropy-loss-pytorch.html#partial-derivative-of-oj-with-respect-to-zi",
    "href": "posts/2021-08-07-cross-entropy-loss-pytorch.html#partial-derivative-of-oj-with-respect-to-zi",
    "title": "A note on Cross Entropy loss with PyTorch",
    "section": "Partial derivative of \\(o^{[j]}\\) with respect to \\(z^{[i]}\\)",
    "text": "Partial derivative of \\(o^{[j]}\\) with respect to \\(z^{[i]}\\)\n\\[\n\\frac{\\partial}{\\partial z^{[i]}} o^{[j]}  = \\frac{\\partial}{\\partial z^{[i]}} \\frac{e^{z^{[j]}}}{\\sum_l e^{z^{[l]}}}\n= e^{z^{[j]}} \\frac{\\partial}{\\partial z^{[i]}} \\Bigg(\\sum_l e^{z^{[l]}} \\Bigg)^{-1} \\\\\n\\qquad = -e^{z^{[j]}} \\Bigg(\\sum_l e^{z^{[l]}} \\Bigg)^{-2} e^{z^{[i]}}\n= -o^{[j]} \\cdot o^{[i]}\n\\]"
  },
  {
    "objectID": "posts/2021-08-07-cross-entropy-loss-pytorch.html#partial-derivative-of-oi-with-respect-to-zi",
    "href": "posts/2021-08-07-cross-entropy-loss-pytorch.html#partial-derivative-of-oi-with-respect-to-zi",
    "title": "A note on Cross Entropy loss with PyTorch",
    "section": "Partial derivative of \\(o^{[i]}\\) with respect to \\(z^{[i]}\\)",
    "text": "Partial derivative of \\(o^{[i]}\\) with respect to \\(z^{[i]}\\)\n\\[\n\\frac{\\partial}{\\partial z^{[i]}} o^{[i]}\n= \\frac{\\partial}{\\partial z^{[i]}} \\frac{e^{z^{[i]}}}{\\sum_l e^{z^{[l]}}}\n= \\frac{e^{z^{[i]}}}{\\sum_{l} e^{z^{[l]}}} + e^{z^{[i]}} \\frac{\\partial}{\\partial z^{[i]}} \\Bigg(\\sum_l e^{z^{[l]}} \\Bigg)^{-1}\\\\\n\\quad \\qquad \\qquad \\qquad = o^{[i]}-e^{z^{[i]}} \\Bigg(\\sum_l e^{z^{[l]}} \\Bigg)^{-2} e^{z^{[i]}}\n= o^{[i]} - o^{[i]} \\cdot o^{[i]}\n= o^{[i]} \\cdot (1 - o^{[i]})\n\\]\nLet’s compute the gradient of the cross-entropy loss with respect to the activation of the \\(i\\)-the class:\n\n\n\nPer the Sylvain says section (page 203 Chapter 5) of [2], ” The gradient is proportional to the difference between the prediction and the target.… Because the gradient is linear we won’t see sudden jumps or exponential increases in gradients, which should lead to smoother training of models.”"
  },
  {
    "objectID": "posts/2022-05-27-exercises-with-convex-sets.html",
    "href": "posts/2022-05-27-exercises-with-convex-sets.html",
    "title": "Exercises with Convex Sets",
    "section": "",
    "text": "Consider two points \\(x_1\\) and \\(x_2\\) in \\(S\\). Is \\(y = \\theta x_1 + (1-\\theta)x_2\\) in the set (for \\(\\theta \\in \\mathbf{R})\\)?\n\\[Fy = F( \\theta x_1 + (1-\\theta)x_2)\\] \\[\\quad = \\theta Fx_1 + (1-\\theta)Fx_2\\] and since \\(x_1, x_2 \\in S\\) we have \\[Fy = \\theta g + (1-\\theta)g = g\\]\nThis means \\(y \\in S\\) and that \\(S\\) is affine."
  },
  {
    "objectID": "posts/2022-05-27-exercises-with-convex-sets.html#check-whether-s-alpha-in-mathbfr3-alpha_1-alpha_2-e-t-alpha_3-e-2t-leq-1.1-text-for-t-geq-1-is-affine-convex-andor-a-polyhedron.",
    "href": "posts/2022-05-27-exercises-with-convex-sets.html#check-whether-s-alpha-in-mathbfr3-alpha_1-alpha_2-e-t-alpha_3-e-2t-leq-1.1-text-for-t-geq-1-is-affine-convex-andor-a-polyhedron.",
    "title": "Exercises with Convex Sets",
    "section": "Check whether \\(S = \\{ \\alpha \\in \\mathbf{R}^{3} | \\alpha_1 +\\alpha_2 e^{-t} + \\alpha_3 e^{-2t} \\leq 1.1 \\text{ for } t \\geq 1\\}\\) is affine, convex and/or a polyhedron.",
    "text": "Check whether \\(S = \\{ \\alpha \\in \\mathbf{R}^{3} | \\alpha_1 +\\alpha_2 e^{-t} + \\alpha_3 e^{-2t} \\leq 1.1 \\text{ for } t \\geq 1\\}\\) is affine, convex and/or a polyhedron.\n\\(S\\) cannot be a polyhedron since a polyhedron is defined by finitely many linear inequalities (the condition \\(t \\geq 1\\) is definitely not on finitely many \\(t\\)).\nSuppose \\(\\alpha, \\beta \\in S\\) then is \\(\\gamma = \\theta \\alpha + (1-\\theta) \\beta\\) in \\(S\\)?\n\\[\\gamma_1 +\\gamma_2 e^{-t} + \\gamma_3 e^{-2t} = \\theta \\alpha_1 + (1-\\theta) \\beta_1 + (\\theta \\alpha_2 + (1-\\theta) \\beta_2) e^{-t} + (\\theta \\alpha_3 + (1-\\theta) \\beta_3) e^{-2t}\\] \\[= \\theta (\\alpha_1 +\\alpha_2 e^{-t} + \\alpha_3 e^{-2t}) + (1-\\theta)(\\beta_1 +\\beta_2 e^{-t} + \\beta_3 e^{-2t}) \\leq 1.1\\]\n\nthe last inequality holds if \\(\\theta \\in [0,1]\\) as $ (_1 +_2 e^{-t} + _3 e^{-2t}) $ and \\((1-\\theta)(\\beta_1 +\\beta_2 e^{-t} + \\beta_3 e^{-2t}) \\leq (1-\\theta) * 1.1\\).\nThe last inequality won’t hold for \\(\\theta \\in \\mathbf{R}\\) as $ (_1 +_2 e^{-t} + _3 e^{-2t}) $ if \\(\\theta < 0\\) (a similar observation holds for \\((1-\\theta)\\)) so \\(S\\) is not affine.\n\nThus, \\(\\gamma \\in S\\) if \\(\\theta \\in [0,1]\\) so \\(S\\) is convex."
  },
  {
    "objectID": "posts/2022-05-27-exercises-with-convex-sets.html#if-two-sets-can-be-separated-by-a-hyperplane-then-they-are-convex.-true-or-false",
    "href": "posts/2022-05-27-exercises-with-convex-sets.html#if-two-sets-can-be-separated-by-a-hyperplane-then-they-are-convex.-true-or-false",
    "title": "Exercises with Convex Sets",
    "section": "If two sets can be separated by a hyperplane, then they are convex. True or False?",
    "text": "If two sets can be separated by a hyperplane, then they are convex. True or False?\nFalse. (Can do a proof by picture)"
  },
  {
    "objectID": "posts/2022-05-27-exercises-with-convex-sets.html#a-point-x_0-on-the-boundary-of-a-convex-set-uniquely-defines-a-supporting-hyperplane-for-the-set-at-that-point.-true-or-false.",
    "href": "posts/2022-05-27-exercises-with-convex-sets.html#a-point-x_0-on-the-boundary-of-a-convex-set-uniquely-defines-a-supporting-hyperplane-for-the-set-at-that-point.-true-or-false.",
    "title": "Exercises with Convex Sets",
    "section": "A point \\(x_0\\) on the boundary of a convex set uniquely defines a supporting hyperplane for the set at that point. True or False.",
    "text": "A point \\(x_0\\) on the boundary of a convex set uniquely defines a supporting hyperplane for the set at that point. True or False.\nFalse. If \\(x_0\\) is, for example, the corner of a square in \\(\\mathbf{R}^2\\), then there are infinitely many supporting hyperplanes at that point."
  },
  {
    "objectID": "posts/2022-05-27-exercises-with-convex-sets.html#consider-the-cone-k-x_1x_2-0-leq-x_1-leq-x_2-subseteq-mathbfr2.",
    "href": "posts/2022-05-27-exercises-with-convex-sets.html#consider-the-cone-k-x_1x_2-0-leq-x_1-leq-x_2-subseteq-mathbfr2.",
    "title": "Exercises with Convex Sets",
    "section": "Consider the cone \\(K = \\{(x_1,x_2) | 0 \\leq x_1 \\leq x_2\\} \\subseteq \\mathbf{R}^2\\).",
    "text": "Consider the cone \\(K = \\{(x_1,x_2) | 0 \\leq x_1 \\leq x_2\\} \\subseteq \\mathbf{R}^2\\).\n\nIs \\((1,3) \\preceq_{K} (3,4)\\) True or False? False. \\((1,3) \\preceq_{K} (3,4) \\Longleftrightarrow (3,4) - (1,3) \\in K\\). So the question is if \\((2,1) \\in K\\)? Clearly not since the \\(x_1 > x_2\\).\nIs \\((-1,2) \\succeq_{K^{\\ast}} 0\\) True or False? True. Now \\((-1,2) \\succeq_{K^{\\ast}} 0 \\Longleftrightarrow (-1,2)^{T}x \\geq 0, \\forall x \\succeq_{K} 0\\). But $(-1,2)^{T}x = -x_1 + x_2 >=0 $ for any \\(x \\succeq_{K} 0\\) (since \\(x_1 \\leq x_2\\)) thus this claim is True."
  },
  {
    "objectID": "posts/2022-05-27-exercises-with-convex-sets.html#what-is-the-distance-between-the-two-parallel-hyperplanes-h_1-x-in-mathbfrn-atxb_1-and-h_2-x-in-mathbfrn-atxb_2",
    "href": "posts/2022-05-27-exercises-with-convex-sets.html#what-is-the-distance-between-the-two-parallel-hyperplanes-h_1-x-in-mathbfrn-atxb_1-and-h_2-x-in-mathbfrn-atxb_2",
    "title": "Exercises with Convex Sets",
    "section": "What is the distance between the two parallel hyperplanes \\(H_1 = \\{x \\in \\mathbf{R}^{n} | a^Tx=b_1\\}\\) and \\(H_2 =\\{x \\in \\mathbf{R}^{n} | a^Tx=b_2\\}\\)?",
    "text": "What is the distance between the two parallel hyperplanes \\(H_1 = \\{x \\in \\mathbf{R}^{n} | a^Tx=b_1\\}\\) and \\(H_2 =\\{x \\in \\mathbf{R}^{n} | a^Tx=b_2\\}\\)?\nLet’s turn to 2d for inspiration.\n\n\n\nThe line with end points \\((b_2/a_1,0)\\) and \\((0,b_2/a_2)\\) is on the hyperplane \\(\\{x \\in \\mathbf{R}^{2} | a^Tx=b_2\\}\\) while that with end points \\((b_1/a_1,0)\\) and \\((0,b_1/a_2)\\) is on the hyperplane \\(\\{x \\in \\mathbf{R}^{2} | a^Tx=b_2\\}\\).\nWe have been asked to find \\(d\\).\nThe vector \\(x = (x_1,x_2)\\) and \\(\\hat{x} = (\\hat{x}_1,\\hat{x}_2)\\) are collinear with the vector \\(a = (a_1,a_2)\\). Thus, the cosine of the angle between the vectors \\(x\\) with \\(a\\) and \\(\\hat{x}\\) with \\(a\\) should equal \\(1\\) (since \\(\\cos 0 =1\\)).\n\\(\\frac{a_1x_1 + a_2x_2}{||a||_2||x||_2} = 1\\) and \\(\\frac{a_1\\hat{x}_1 + a_2\\hat{x}_2}{||a||_2||\\hat{x}||_2} = 1\\) or that \\(\\frac{b_2}{||a||_2||x||_2} = 1\\) and \\(\\frac{b_1}{||a||_2||\\hat{x}||_2} = 1\\) (since \\(x\\) and \\(\\hat{x}\\) lie on \\(H_1\\) and \\(H_2\\) respectively).\nThus, we get that \\(||x||_2 = \\frac{b_2}{||a||_2}\\) and \\(||\\hat{x}||_2 = \\frac{b_1}{||a||_2}\\) (these measure the lengths of the vector \\(x\\) and \\(\\hat{x}\\) respectively).\nHence, \\(d = ||\\hat{x}||_2 - ||x||_2 = \\frac{b_1 - b_2}{||a||_2}\\)\nAnother way to think about this is that for \\(\\theta \\in [0,1]\\) a point of the form \\((\\frac{b_2}{a_1}\\theta, \\frac{b_2}{a_2}(1-\\theta))\\) lies on \\(H_1\\). So,in particular, for \\(\\theta = \\frac{a_1^2}{||a||^2_2}\\) we have that \\((x_1,x_2) = (\\frac{b_2}{a_1}\\frac{a_1^2}{||a||^2_2}, \\frac{b_2}{a_2}\\frac{a_2^2}{||a||^2_2})\\) lies on \\(H_1\\). Thus, \\(x = (\\frac{b_2}{||a||^2_2}a_1, \\frac{b_2}{||a||^2_2}a_2)\\)\nSimilarly, \\(\\hat{x} = (\\frac{b_1}{||a||^2_2}a_1, \\frac{b_1}{||a||^2_2}a_2)\\)\nSo d = \\(||x-\\hat{x}||_2 = \\sqrt{ \\frac{(b_2-b_1)^2}{||a||^4_2}a^2_1 + \\frac{(b_2-b_1)^2}{||a||^4_2}a^2_2} = \\sqrt{\\frac{(b_2-b_1)^2}{||a||^2_2}} = \\frac{|b_2-b_1|}{||a||_2}\\)"
  },
  {
    "objectID": "posts/2022-05-27-exercises-with-convex-sets.html#let-a-and-b-be-distinct-points-in-mathbfrn-and-consider-the-set-of-points-that-are-closer-in-euclidean-norm-to-a-than-b-i.e.-mathcalc-x-x-a_2-leq-x-b_2-.-show-that-mathcalc-is-a-halfspace-namely-that-it-can-be-described-by-a-set-of-the-form-x-ctx-leq-d-for-c-neq-0.",
    "href": "posts/2022-05-27-exercises-with-convex-sets.html#let-a-and-b-be-distinct-points-in-mathbfrn-and-consider-the-set-of-points-that-are-closer-in-euclidean-norm-to-a-than-b-i.e.-mathcalc-x-x-a_2-leq-x-b_2-.-show-that-mathcalc-is-a-halfspace-namely-that-it-can-be-described-by-a-set-of-the-form-x-ctx-leq-d-for-c-neq-0.",
    "title": "Exercises with Convex Sets",
    "section": "Let \\(a\\) and \\(b\\) be distinct points in \\(\\mathbf{R}^{n}\\) and consider the set of points that are closer (in Euclidean norm) to \\(a\\) than \\(b\\) i.e., \\(\\mathcal{C} = \\{x | ||x-a||_2 \\leq ||x-b||_2 \\}\\). Show that \\(\\mathcal{C}\\) is a halfspace (namely that it can be described by a set of the form \\(\\{x | c^Tx \\leq d\\}\\) for \\(c \\neq 0\\)).",
    "text": "Let \\(a\\) and \\(b\\) be distinct points in \\(\\mathbf{R}^{n}\\) and consider the set of points that are closer (in Euclidean norm) to \\(a\\) than \\(b\\) i.e., \\(\\mathcal{C} = \\{x | ||x-a||_2 \\leq ||x-b||_2 \\}\\). Show that \\(\\mathcal{C}\\) is a halfspace (namely that it can be described by a set of the form \\(\\{x | c^Tx \\leq d\\}\\) for \\(c \\neq 0\\)).\nNorms are strictly non-negative so, \\[||x-a||_2 \\leq ||x-b||_2 \\Longleftrightarrow ||x-a||^{2}_2 \\leq ||x-b||^{2}_2 \\]\nThus, \\((x-a)^T(x-a) \\leq (x-b)^T(x-b)\\) or \\(x^Tx -2a^Tx + a^Ta \\leq x^Tx -2b^Tx + b^Tb\\).\nSo \\(2(b^T - a^T)x \\leq b^Tb - a^Ta\\) or \\(2(b-a)^Tx \\leq b^Tb - a^Ta\\) which satisfies the definition of a half space of the form \\(\\{x | c^Tx \\leq d\\}\\) with \\(c = (b-a)^T\\) and \\(d=\\frac{b^Tb - a^Ta}{2}\\)."
  },
  {
    "objectID": "posts/2022-05-27-exercises-with-convex-sets.html#which-of-the-following-sets-is-convex",
    "href": "posts/2022-05-27-exercises-with-convex-sets.html#which-of-the-following-sets-is-convex",
    "title": "Exercises with Convex Sets",
    "section": "Which of the following sets is convex?",
    "text": "Which of the following sets is convex?\n\nA slab, i.e., a set of the form \\(\\{x \\in \\mathbf{R}^{n} | \\alpha \\leq a^Tx\\leq \\beta \\}\\). Yes, convex since this is an intersection of two half spaces (it is also a polyhedron). Recall a half space is convex and an intersection of convex sets is convex.\nA rectangle, i.e., a set of the form \\(\\{x \\in \\mathbf{R}^{n} | \\alpha_i \\leq x_i\\leq \\beta_i \\}\\). A rectangle is sometimes called a hyperrectangle when \\(n > 2\\). Yes, convex since it is an intersection of finitely many half spaces (it’s also a polyhedron).\nA wedge, i.e., a set of the form \\(\\{x \\in \\mathbf{R}^{n} | a_1^Tx\\leq b_1, a_2^Tx\\leq b_2 \\}\\). Yes, convex since an intersection of two half spaces.\nThe set of points closer to a given point than a given set i.e., \\(\\{x | ||x - x_0||_2 \\leq ||x-y||_2 \\text{ for all } y\\in S\\}\\) where \\(S \\subseteq \\mathbf{R}^{n}\\). We can see that \\(S = \\cap_{y\\in S} S(y)\\) where \\(S(y) = \\{x | ||x - x_0)||_2 \\leq ||x-y||_2\\}\\) (for some specific choice of \\(y \\in S\\)). Each \\(S(y)\\) is convex and hence \\(S\\) is convex since it is the intersection of convex sets.\nThe set of points closer to one set than another, i.e., \\(\\mathcal{C} = \\{x | dist(x,S) \\leq dist(x,T)\\}\\), where \\(S,T \\subseteq \\mathbf{R}^{n}\\), and \\(dist(x,S) = \\text{inf}\\{||x-z||_2 | z \\in S\\}\\). Suppose \\(x,y \\in \\mathcal{C}\\) then for \\(\\theta \\in [0,1]\\) is \\(c=\\theta x + (1-\\theta) y\\) in \\(\\mathcal{C}\\)?\n\nSince \\(x,y \\in \\mathcal{C}\\) we have \\(\\text{inf}\\{||x-z||_2 | z \\in S\\} \\leq \\text{inf}\\{||x-z||_2 | z \\in T\\}\\) and \\(\\text{inf}\\{||y-z||_2 | z \\in S\\} \\leq \\text{inf}\\{||y-z||_2 | z \\in T\\}\\). Then, \\(\\text{inf}\\{\\theta||x-z||_2 | z \\in S\\} + \\text{inf}\\{(1-\\theta)||y-z||_2 | z \\in S\\} \\leq \\text{inf}\\{\\theta||x-z||_2| z \\in T\\} + \\text{inf}\\{(1-\\theta)||y-z||_2| z \\in T\\}\\)\nIn order to show \\(\\text{inf}\\{||c-z||_2 | z \\in S\\} \\leq \\text{inf}\\{||c-z||_2 | z \\in T\\}\\) we could potentially argue that \\(\\text{inf}\\{||c-z||_2 | z \\in S\\} \\leq \\text{inf}\\{\\theta||x-z||_2 | z \\in S\\} + \\text{inf}\\{(1-\\theta)||y-z||_2 | z \\in S\\}\\) (Triangle inequality?)\nHowever we likely won’t be able to show \\(\\text{inf}\\{||c-z||_2 | z \\in T\\} \\leq \\text{inf}\\{\\theta||x-z||_2| z \\in T\\} + \\text{inf}\\{(1-\\theta)||y-z||_2| z \\in T\\}\\) (it may go the other direction).\nSo I feel \\(\\mathcal{C}\\) is not convex."
  },
  {
    "objectID": "posts/2022-05-27-exercises-with-convex-sets.html#let-x-be-a-real-valued-random-variable-with-mathbfprobxa_ip_i-i1ldotsn-where-a_1-a_2-ldots-a_n.-of-course-p-in-mathbfrn-lies-in-the-standard-probability-simplex-p-p-mathbf1tp1-p-succeq-0.-which-if-the-following-conditions-are-convex-in-p.-that-is-for-which-of-the-following-conditions-is-the-set-of-p-in-p-that-satisfy-the-condition-convex.",
    "href": "posts/2022-05-27-exercises-with-convex-sets.html#let-x-be-a-real-valued-random-variable-with-mathbfprobxa_ip_i-i1ldotsn-where-a_1-a_2-ldots-a_n.-of-course-p-in-mathbfrn-lies-in-the-standard-probability-simplex-p-p-mathbf1tp1-p-succeq-0.-which-if-the-following-conditions-are-convex-in-p.-that-is-for-which-of-the-following-conditions-is-the-set-of-p-in-p-that-satisfy-the-condition-convex.",
    "title": "Exercises with Convex Sets",
    "section": "Let \\(x\\) be a real-valued random variable with \\(\\mathbf{prob}(x=a_{i})=p_i, i=1,\\ldots,n\\), where \\(a_1 < a_2 < \\ldots < a_n\\). Of course \\(p \\in \\mathbf{R}^n\\) lies in the standard probability simplex \\(P = \\{p | \\mathbf{1}^Tp=1, p \\succeq 0\\}\\). Which if the following conditions are convex in \\(p\\). (That is, for which of the following conditions is the set of \\(p \\in P\\) that satisfy the condition convex?).",
    "text": "Let \\(x\\) be a real-valued random variable with \\(\\mathbf{prob}(x=a_{i})=p_i, i=1,\\ldots,n\\), where \\(a_1 < a_2 < \\ldots < a_n\\). Of course \\(p \\in \\mathbf{R}^n\\) lies in the standard probability simplex \\(P = \\{p | \\mathbf{1}^Tp=1, p \\succeq 0\\}\\). Which if the following conditions are convex in \\(p\\). (That is, for which of the following conditions is the set of \\(p \\in P\\) that satisfy the condition convex?).\nNow \\(\\mathbf{E}f(x) = \\sum_{i=1}^{n}p_i f(a_i) = w^Tp\\) where \\(w^{T} =(f(a_1),\\ldots,f(a_n))\\). Essentially the \\(w\\)’s are constants.\n\n\\(\\alpha \\leq \\mathbf{E}f(x) \\leq \\beta\\) - This is a slab so it is convex.\n\\(\\mathbf{prob}(x > \\alpha) \\leq \\beta\\) - Now \\(\\mathbf{prob}(x > \\alpha) = \\sum_{i:a_{i} > \\alpha}p_{i}\\). We can define an indicator function that takes the value \\(0\\) when \\(i\\) is such that \\(a_i \\leq \\alpha\\) and \\(1\\) otherwise. The expecation of that indicator function is the same as the desired probability. So \\(\\mathbf{prob}(x > \\alpha) = \\sum_{i} p_{i} f(a_i)\\) where \\(f(a_i) = I(a_i > \\alpha)\\) so this is a half space and hence convex.\n\\(\\mathbf{E}|x^3| \\leq \\alpha \\mathbf{E}|x|\\) - This is nothing but \\(\\sum_{i=1}^{n} (|a_i^3| - \\alpha|a_i|)p_i \\leq 0\\) which is a half space hence it is convex.\n\\(\\mathbf{E}|x^2| \\leq \\alpha\\) - This is nothing but \\(\\sum_{i=1}^{n} |a_i^2|p_i \\leq 0\\) which is a half space hence it is convex."
  },
  {
    "objectID": "posts/2022-05-27-exercises-with-convex-sets.html#what-is-the-dual-cone-of",
    "href": "posts/2022-05-27-exercises-with-convex-sets.html#what-is-the-dual-cone-of",
    "title": "Exercises with Convex Sets",
    "section": "What is the dual cone of:",
    "text": "What is the dual cone of:\n\n\\(K = \\{0\\} \\subseteq \\mathbf{R}^2\\)? The dual cone of a cone \\(K\\) is \\(K^{\\ast} = \\{y | y^Tx \\geq 0 \\forall x \\in K\\}\\). So for this case we will have \\(K^{\\ast} = \\{y | y^Tx \\geq 0 \\text{ for all } x \\in \\{0\\}\\}\\). Hence \\(K^{\\ast} = \\mathbf{R}^2\\).\n\\(K = \\mathbf{R}^2\\)? \\(K^{\\ast} = \\{y | y^Tx \\geq 0 \\text{ for all } x \\in \\mathbf{R}^2\\} = \\{0\\}\\)\n\\(K = \\{ (x,y)| \\text{  } |x| \\leq y \\}\\)? A 2d picture really helps here, as we are essentially looking a region of vectors which has a non negative inner product with \\(K\\). \\(K^{\\ast} = \\{ (x,y)| \\text{  } |x| \\leq y \\}\\)\n\\(K = \\{ (x,y) | x + y = 0 \\}\\)? Draw a picture of this line in 2d. Then convince yourself that \\(K^{\\ast} = \\{ (x,y) | x - y = 0 \\}\\)"
  },
  {
    "objectID": "posts/2022-05-27-exercises-with-convex-sets.html#which-conditions-in-terms-of-the-ordinary-inequalities-on-matrix-coefficents-must-hold-true-for-the-elements-of-the-positive-semidefinite-cone-mathbfsn_-for-n123",
    "href": "posts/2022-05-27-exercises-with-convex-sets.html#which-conditions-in-terms-of-the-ordinary-inequalities-on-matrix-coefficents-must-hold-true-for-the-elements-of-the-positive-semidefinite-cone-mathbfsn_-for-n123",
    "title": "Exercises with Convex Sets",
    "section": "Which conditions, in terms of the ordinary inequalities on matrix coefficents, must hold true for the elements of the positive semidefinite cone \\(\\mathbf{S}^{n}_+\\) for \\(n=1,2,3\\):",
    "text": "Which conditions, in terms of the ordinary inequalities on matrix coefficents, must hold true for the elements of the positive semidefinite cone \\(\\mathbf{S}^{n}_+\\) for \\(n=1,2,3\\):\nRecall that: 1. \\(X \\in \\mathbf{S}^{n}_+ \\Longleftrightarrow z^TXz \\geq 0\\) for all \\(z\\).\n\nPer [1], a symmetric \\(n x n\\) matrix is positive semidefinite if and only if all principal minors (determinants of symmetric submatrices) of the the matrix are nonnegative. In contrast, for a symmetric \\(n x n\\) matrix to be positive definite we only need all of the leading principal minors (upper left determinants) to be positive (Sylvester’s criterion).\n\n\n\\(n=1,\\quad [x_1]\\): Thus we must have \\(z^2x \\geq 0\\) for all \\(z\\). Since \\(z^2 \\geq 0\\) we must have \\(x_1 \\geq 0\\).\n\\(n=2\\), \\[\\begin{equation*}\nX =\n\\begin{bmatrix}\nx_1 & x_2 \\\\\nx_2 & x_3\n\\end{bmatrix}\n\\end{equation*}\\] We want \\[\\begin{equation*}\n\\begin{bmatrix}\nz_1 & z_2\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 & x_2 \\\\\nx_2 & x_3\n\\end{bmatrix}\n\\begin{bmatrix}\nz_1 \\\\\nz_2\n\\end{bmatrix}\n=x_1z_1^2 + 2x_2z_1z_2 + x_3z_2^2 \\geq 0\n\\end{equation*}\\] So for \\(X\\) to be positive semidefinite we must have \\(x_1 \\geq 0\\), \\(x_3 \\geq 0\\) and \\(x_1x_3 -x^2_2 \\geq 0\\).\n\\(n=3\\), \\[\\begin{equation*}\nX=\n\\begin{bmatrix}\nx_1 & x_2 & x_3 \\\\\nx_2 & x_4 & x_5 \\\\\nx_3 & x_5 & x_6\n\\end{bmatrix}\n\\end{equation*}\\] So for \\(X\\) to be positive semidefinite we must have \\(x_1 \\geq 0\\), \\(x_4 \\geq 0\\), \\(x_6 \\geq 0\\), \\(x_4x_6 -x^2_5 \\geq 0\\), \\(x_1x_6 -x^2_3 \\geq 0\\), \\(x_1x_4 -x^2_2 \\geq 0\\) and \\(x_6x_4x_1 - x_6x_2^2 - x_4x_3^2 -x_5^2x_1 + 2x_5x_3x_2 \\geq 0\\)"
  },
  {
    "objectID": "posts/2022-05-27-exercises-with-convex-sets.html#show-the-set-c-xy-in-mathbfr2_-xy-geq-alpha-is-convex.",
    "href": "posts/2022-05-27-exercises-with-convex-sets.html#show-the-set-c-xy-in-mathbfr2_-xy-geq-alpha-is-convex.",
    "title": "Exercises with Convex Sets",
    "section": "Show the set \\(C = \\{(x,y) \\in \\mathbf{R}^2_{++} | xy \\geq \\alpha \\}\\) is convex.",
    "text": "Show the set \\(C = \\{(x,y) \\in \\mathbf{R}^2_{++} | xy \\geq \\alpha \\}\\) is convex.\nConsider two points \\((x_1,y_1) \\in C\\) and \\((x_2,y_2) \\in C\\). So we have that \\(y_1 \\geq \\frac{\\alpha}{x_1}\\) and \\(y_2 \\geq \\frac{\\alpha}{x_2}\\) (since both \\((x_1,y_1), (x_2,y_2) \\in \\mathbf{R}^2_{++}\\) we can safely divide by \\(x_1\\) and \\(x_2\\)). Thus, for \\(\\theta \\in [0,1]\\) we have\n\\[\\theta y_1 + (1-\\theta)y_2 \\geq \\alpha\\left[ \\frac{\\theta}{x_1} + \\frac{1-\\theta}{x_2} \\right]\\]\nMultiplying both sides by \\(\\theta x_1 + (1-\\theta)x_2\\) we get that\n\\((\\theta x_1 + (1-\\theta)x_2)(\\theta y_1 + (1-\\theta)y_2) \\geq \\alpha\\left[ \\theta^2 + \\theta(1-\\theta)\\frac{x_1}{x_2} + \\theta(1-\\theta)\\frac{x_2}{x_1} + (1-\\theta)^2 \\right]\\) \\[ \\geq \\alpha\\left[ 1 + 2\\theta^2 -2\\theta + \\theta(1-\\theta)\\frac{x_1^2 + x_2^2}{x_1x_2} \\right]\\] \\[ \\geq \\alpha\\left[ 1 - 2\\theta(1 -\\theta) + \\theta(1-\\theta)\\frac{x_1^2 + x_2^2}{x_1x_2} \\right]\\] \\[ \\geq \\alpha\\left[ 1 - \\theta(1 -\\theta)\\left(2- \\frac{x_1^2 + x_2^2}{x_1x_2}\\right) \\right]\\] \\[ \\geq \\alpha\\left[ 1 + \\theta(1 -\\theta)\\frac{(x_1 - x_2)^2}{x_1x_2} \\right]\\]\nSince \\(1 + \\theta(1 -\\theta)\\frac{(x_1 - x_2)^2}{x_1x_2} \\geq 1\\) we have shown that \\((\\theta x_1 + (1-\\theta)x_2)(\\theta y_1 + (1-\\theta)y_2) \\geq \\alpha\\).\nThus \\((\\theta x_1 + (1-\\theta)x_2, \\theta y_1 + (1-\\theta)y_2) \\in C\\) and hence \\(C\\) is convex.\nNote that \\(C = \\{(x,y) \\in \\mathbf{R}^2_{+} | xy \\geq 0 \\}\\) is convex since in that case \\(C = \\{(x,y) \\in \\mathbf{R}^2_{+}\\}\\) which is convex."
  },
  {
    "objectID": "posts/2022-05-26-losses.html",
    "href": "posts/2022-05-26-losses.html",
    "title": "Losses",
    "section": "",
    "text": "Some notes while reading Barratt, Angeris, and Boyd (2021)."
  },
  {
    "objectID": "posts/2022-05-26-losses.html#equality-loss",
    "href": "posts/2022-05-26-losses.html#equality-loss",
    "title": "Losses",
    "section": "Equality Loss",
    "text": "Equality Loss\nConstrain \\(x = f_{\\text{des}}\\) so let \\(l(x, f_{des}) = +\\infty\\) for \\(x \\neq f_{des}\\) and \\(0\\) when \\(x = f_{des}\\). Here \\(l\\) represents our loss function.\n\nConvex Optimization Version\n\\[\\min \\frac{1}{\\lambda}\\sum_{i=1}^{m}( \\hat{f}_{i}-f^{i})^{2} = \\frac{1}{\\lambda}||\\hat{f}-f||^{2}_{2}\\] \\[\\text{subject to:}\\quad \\hat{f}_{i} = f^{i}_{\\text{des}},\\quad i=1,\\ldots,m\\]\n\n\nProximal Version\n\\(\\mathbf{prox}_{\\lambda l}(f_{\\text{des}}) = argmin_{x} \\left( l(x, f_{des}) + \\frac{1}{2\\lambda}||x-f_{\\text{des}}||^{2}_{2} \\right)\\)\nSo we have to set \\(x_{i} = f^{i}_{\\text{des}}\\)for each \\(i\\) in order to avoid the loss of \\(\\infty\\) which we incur for any other choice.\nThus, \\(\\mathbf{prox}_{\\lambda l}(f_{\\text{des}})_{i} = f^{i}_{\\text{des}}\\) is the \\(i\\)-th component of the minimizer \\(x\\).\n\n# Number of \"marginals\" we want to match.\nm = 10\n\n# Expected values of these functions under the induced distribution.\n# There is one for each marginal that we want to match.\nf = np.random.randn(m) \n\n# The desired or target values we want to match by applying weights\n#  to f.\nfdes = np.random.randn(m)\n\nlam = 1\n\nThe following demonstrates that the \\(\\hat{f}\\) obtained via Convex optimization is the same as the minimizer \\(x\\) obtained using the Proximal route.\n\nfhat = cp.Variable(m)\ncp.Problem(cp.Minimize(1 / lam * cp.sum_squares(fhat - f)),\n            [fhat == fdes]).solve()\n\nequality = rsw.EqualityLoss(fdes)   \nnp.testing.assert_allclose(fhat.value, equality.prox(f, lam))"
  },
  {
    "objectID": "posts/2022-05-26-losses.html#inequality-loss",
    "href": "posts/2022-05-26-losses.html#inequality-loss",
    "title": "Losses",
    "section": "Inequality Loss",
    "text": "Inequality Loss\nConstrain \\(x\\) to a range around \\(f_{\\text{des}}\\) so let \\(l(x, f_{des}) = 0\\) for \\(x \\in [f_{des}+\\text{lower}, f_{des}+\\text{upper}]\\) and \\(+\\infty\\) otherwise. Here the lower and upper refer to some acceptable range around each entry in \\(f_{des}.\\)\n\nConvex Optimization Version\n\\[\\min \\frac{1}{\\lambda}||\\hat{f}-f||^{2}_{2}\\] \\[\\text{subject to:}\\quad f^{i}_{\\text{des}} + \\text{lower}^{i} \\leq \\hat{f}_{i} \\leq f^{i}_{\\text{des}} + \\text{upper}^{i},\\quad i=1,\\ldots,m\\]\nSo ideally we want to set each \\(\\hat{f}_{i}\\) to \\(f_{i}\\) to minimize the objective function. But what if \\(f_{i} > f^{i}_{\\text{des}} + \\text{upper}^{i}\\)? In this case, accepting such an \\(\\hat{f}_{i}\\) leads to infeasibility and to avoid this we will clip \\(\\hat{f}_{i}\\) to \\(f^{i}_{\\text{des}} + \\text{upper}^{i}\\). We cannot clip any lower because then we would be incurring additional loss (over and above that resulting from setting to \\(f^{i}_{\\text{des}} + \\text{upper}^{i}\\)) in the objective function unnecessarily.\nSimilary, we clip \\(\\hat{f}_{i}\\) to \\(f^{i}_{\\text{des}} + \\text{lower}^{i}\\) if we find that \\(f_{i} < f^{i}_{\\text{des}} + \\text{lower}^{i}\\).\n\n\nProximal Version\n\\(\\mathbf{prox}_{\\lambda l}(f_{\\text{des}}) = argmin_{x} \\left( l(x, f_{des}) + \\frac{1}{2\\lambda}||x-f_{\\text{des}}||^{2}_{2} \\right)\\)\nThus, \\(\\mathbf{prox}_{\\lambda l}(f_{\\text{des}})_{i} = \\begin{cases} f^{i}_{\\text{des}} + \\text{lower}^{i}, \\text{if } f_{i} \\leq f^{i}_{\\text{des}} + \\text{lower}^{i}\\\\ f^{i}_{\\text{des}} + \\text{upper}^{i}, \\text{if } f_{i} \\geq f^{i}_{\\text{des}} + \\text{upper}^{i}\\\\ f_{i}, \\text{ otherwise}\\end{cases}\\)\nSo we see that the Proximal version when passed the input (vector) \\(f\\) and the Convex optimization version will both end up with the same minimizer.\n\nlower = np.array([-.3])\nupper = np.array([.3])\n\nfhat = cp.Variable(m)\ncp.Problem(cp.Minimize(1 / lam * cp.sum_squares(fhat - f)),\n            [lower <= fhat - fdes, fhat - fdes <= upper]).solve()\n\ninequality = rsw.InequalityLoss(fdes, lower, upper)           \nnp.testing.assert_allclose(fhat.value, inequality.prox(f, lam))"
  },
  {
    "objectID": "posts/2022-05-26-losses.html#boolean-loss",
    "href": "posts/2022-05-26-losses.html#boolean-loss",
    "title": "Losses",
    "section": "Boolean Loss",
    "text": "Boolean Loss\n\\(l(x) = \\begin{cases}0 \\quad x \\in \\{0, 1/k\\}^{n} \\\\ \\infty \\quad \\text{Otherwise}\\end{cases}\\)\nWe want \\(x_{i} = 1/k\\) for \\(k < n\\) samples (and \\(0\\) for the others). Choosing any other value results an infinite loss (think infeasibility).\n\\[\\mathbf{prox}_{\\lambda l}(f_{\\text{des}}) = argmin_{x}( l(x) + \\frac{1}{2\\lambda}||x - f_{des}||^{2}_{2} )\\]\nThe proximal operator of \\(l\\) is the projection of \\(f_{des}\\) onto the (nonconvex) set \\(\\{x \\in \\{0, 1/k\\}^{n} | \\mathbf{1}^{T}x=1\\}\\). If each component of \\(x\\) is either \\(0\\) or \\(1/k\\) then the constraint \\(\\mathbf{1}^{T}x=1\\) means that exactly \\(k\\) of them are set to \\(1/k\\) and the remaining are set to \\(0\\) (as was desired).\nThe solution here is to set the largest \\(k\\) entries of \\(f_{des}\\) to \\(1/k\\) and the remaining to \\(0\\).\n\nboolean = rsw.BooleanRegularizer(3)\nnp.testing.assert_allclose(np.array([1/3, 0, 0, 1/3, 1/3]), boolean.prox(np.array([5, 1, 0, 2, 4]), lam))"
  },
  {
    "objectID": "posts/2022-05-26-losses.html#weighted-least-squares-loss",
    "href": "posts/2022-05-26-losses.html#weighted-least-squares-loss",
    "title": "Losses",
    "section": "(Weighted) Least Squares Loss",
    "text": "(Weighted) Least Squares Loss\n\\[\\min \\frac{1}{2}\\sum_{i=1}^{m}d^{2}_{i}(\\hat{f}_{i} - f^{i}_{des})^2 + \\frac{1}{2\\lambda}||\\hat{f} - f_{des}||^{2}_{2}\\]\nTaking the derivative with respect to \\(\\hat{f}_{i}\\) and setting equal to zero we get:\n\\[d^{2}_{i}(\\hat{f}_{i} - f^{i}_{des}) + \\frac{1}{\\lambda}(\\hat{f}_{i} - f^{i}_{des}) = 0\\]\n\\[\\hat{f}_{i}( d^{2}_{i} + \\frac{1}{\\lambda} ) = \\frac{f^{i}_{des}}{\\lambda} + d^{2}_{i}f^{i}_{des}\\]\nFinally we get that \\(\\hat{f}_{i} = \\frac{ d^{2}_{i}f^{i}_{des} + \\frac{f^{i}_{des}}{\\lambda} }{ ( d^{2}_{i} + \\frac{1}{\\lambda} ) }\\)\nThe \\(d\\)’s are called the diagonals in the code and are used to weight each sample (row) in the data. Imagine a matrix with zeros everywhere but for the diagonal entries.\n\nd = np.random.uniform(0, 1, size=m)\n\nfhat = cp.Variable(m)\ncp.Problem(cp.Minimize(1 / 2 * cp.sum_squares(cp.multiply(d, fhat - fdes)) +\n                        1 / (2 * lam) * cp.sum_squares(fhat - f))).solve()\n\nlstsq = rsw.LeastSquaresLoss(fdes, d)                        \nnp.testing.assert_allclose(fhat.value, lstsq.prox(f, lam))"
  },
  {
    "objectID": "posts/2022-05-26-losses.html#entropy-loss",
    "href": "posts/2022-05-26-losses.html#entropy-loss",
    "title": "Losses",
    "section": "Entropy Loss",
    "text": "Entropy Loss\n\\[\\min \\sum_{i=1}^{m}\\hat{f}_{i}\\ln\\hat{f}_{i} + \\frac{1}{2\\lambda}||\\hat{f} - f||^{2}_{2}\\]\nTaking the derivative with respect to \\(\\hat{f}_{i}\\) and setting equal to zero we get:\n\\[1 + \\ln\\hat{f}_{i} + \\frac{1}{\\lambda}(\\hat{f}_{i} - f_{i}) = 0\\]\nRearranging terms: \\[ \\ln\\hat{f}_{i} + \\frac{\\hat{f}_{i}}{\\lambda} = \\frac{f_{i}}{\\lambda} - 1 \\]\nExponentiating both sides and then dividing both sides by \\(\\lambda\\): \\[\\frac{\\hat{f}_{i}}{\\lambda}e^{\\frac{\\hat{f}_{i}}{\\lambda}} = \\frac{1}{\\lambda}e^{\\frac{f_{i}}{\\lambda} - 1 }\\]\nApplying the Lambert W function on both sides: \\[W(\\frac{\\hat{f}_{i}}{\\lambda}e^{\\frac{\\hat{f}_{i}}{\\lambda}}) = W(\\frac{1}{\\lambda}e^{\\frac{f_{i}}{\\lambda} - 1 })\\]\nFinally, \\(\\hat{f}_{i} = \\lambda W(\\frac{1}{\\lambda}e^{\\frac{f_{i}}{\\lambda} - 1 })\\).\nJust for fun, suppose that each instance of \\(\\lambda\\) and \\(f_{i}\\) in the above result were replaced by \\(0.5\\lambda\\) and \\(f_{i} + 0.5\\lambda \\ln f^{i}_{des}\\) respectively then:\n\\(\\hat{f}_{i} = 0.5\\lambda W(\\frac{1}{0.5\\lambda}e^{\\frac{f_{i} + 0.5\\lambda \\ln f^{i}_{des}}{0.5\\lambda} - 1 }) = 0.5\\lambda W(\\frac{f^{i}_{des}}{0.5\\lambda}e^{\\frac{f_{i}}{0.5\\lambda} - 1 })\\)\n\nf = np.random.uniform(0, 1, size=m)\nf /= f.sum()\n\nfhat = cp.Variable(m)\ncp.Problem(cp.Minimize(cp.sum(-cp.entr(fhat)) +\n                        1 / (2 * lam) * cp.sum_squares(fhat - f))).solve()\nnp.testing.assert_allclose(\n    fhat.value, rsw.losses._entropy_prox(f, lam), atol=1e-5)\n\n\nEntropy Regularizer\nMinimize \\(r(w)\\) where\n\\[r(w) = \\begin{cases} \\sum_{i=1}^{n} w_{i}\\ln w_{i}, \\quad (1/(\\kappa n))\\mathbf{1} \\leq w \\leq (\\kappa/n )\\mathbf{1} \\\\ \\infty \\quad \\text{Otherwise}\\end{cases}\\]\nHere \\(\\kappa > 1\\) is a hyper-parameter (also called limit in the code). Observe that there is a constraint on \\(w\\) that says they must lie within \\([1/(\\kappa n), \\kappa/n]\\) otherwise the loss is infinite (i.e., we have an infeasible solution). If we interpret \\(w_i\\) as a weight then the constraint says that no item can be down-weighted by less than \\(1/\\kappa\\) or up-weighted by more than \\(\\kappa\\).\nThe final solution is \\(\\mathbf{prox}_{\\lambda r}(w_{\\text{des}})_{i} = \\begin{cases} 1/(\\kappa n), \\text{if } \\hat{w}_{i} \\leq 1/(\\kappa n)\\\\ \\kappa/n, \\text{if } \\hat{w}_{i} \\geq \\kappa/n\\\\ \\hat{w}_{i}, \\text{ otherwise}\\end{cases}\\) where \\(\\hat{w} = \\lambda W(\\frac{1}{\\lambda}e^{\\frac{w^{des}_{i}}{\\lambda} - 1 })\\) from the Entropy Loss section (\\(w^{des}\\) is some desired weight vector, provided as input, which we want \\(\\hat{w}\\) to be close to)."
  },
  {
    "objectID": "posts/2022-05-26-losses.html#kl-loss",
    "href": "posts/2022-05-26-losses.html#kl-loss",
    "title": "Losses",
    "section": "KL Loss",
    "text": "KL Loss\n\\[\\min \\frac{1}{2}\\left[ \\sum_{i=1}^{m}\\hat{f}_{i}\\ln\\hat{f}_{i} - \\sum_{i=1}^{m}\\hat{f}_{i}\\ln f^{i}_{des}  \\right] + \\frac{1}{2\\lambda}||\\hat{f} - f||^{2}_{2}\\]\nTaking the derivative with respect to \\(\\hat{f}_{i}\\) and setting equal to zero we get:\n\\[\\frac{1}{2}\\left[ 1 + \\ln\\hat{f}_{i} - \\ln f^{i}_{des} \\right] + \\frac{1}{\\lambda}(\\hat{f}_{i} - f_{i}) = 0\\]\nMultiplying both sides by \\(2\\) and then rearrange terms to obtain: \\[\\ln\\hat{f}_{i} + \\frac{2\\hat{f}_{i}}{\\lambda} = \\frac{2f_{i}}{\\lambda} -1 + \\ln f^{i}_{des}\\]\nNext, exponentiate both sides to get: \\[\\hat{f}_{i}e^{\\frac{\\hat{f}_{i}}{0.5\\lambda}} = f^{i}_{des}e^{\\frac{f_{i}}{0.5\\lambda} -1}\\]\nDivide both sides by \\(0.5\\lambda\\) and then apply the Lambert W function to get:\n\\[W(\\frac{\\hat{f}_{i}}{0.5\\lambda}e^{\\frac{\\hat{f}_{i}}{0.5\\lambda}}) = W(\\frac{f^{i}_{des}}{0.5\\lambda}e^{\\frac{f_{i}}{0.5\\lambda} -1})\\]\nThus, \\(\\hat{f}_{i} = 0.5\\lambda W(\\frac{f^{i}_{des}}{0.5\\lambda}e^{\\frac{f_{i}}{0.5\\lambda} -1}).\\)\nHopefully, this reminds you of the last expression in the Entropy Loss section.\n\nf = np.random.uniform(0, 1, size=m)\nf /= f.sum()\n\nfdes = np.random.uniform(0, 1, size=m)\nfdes /= fdes.sum()\n\nkl = rsw.KLLoss(fdes, scale=.5)\nfhat = cp.Variable(m, nonneg=True)\ncp.Problem(cp.Minimize(.5 * (cp.sum(-cp.entr(fhat) - cp.multiply(fhat, np.log(fdes)))) +\n                        1 / (2 * lam) * cp.sum_squares(fhat - f))).solve()\nnp.testing.assert_allclose(fhat.value, kl.prox(f, lam), atol=1e-5)"
  },
  {
    "objectID": "posts/2022-03-11-mcelreath-chapter-2.html",
    "href": "posts/2022-03-11-mcelreath-chapter-2.html",
    "title": "McElreath Chapter 2",
    "section": "",
    "text": "Exercise problems from Chapter 2, Small Worlds and Large Worlds, of [1].\n##2M1\nRecall the globe tossing model from the chapter. Compute and plot the grid approximate posterior distribution for each of the following sets of observations. In each case, assume a uniform prior for p.\n##2M3\nSuppose there are two globes, one for Earth and one for Mars. The Earth globe is 70% covered in water and the Mars globe is 100% land. One of these globes was tossed in the air and produced a “land” observation. Each globe was equally likely to be tossed. Compute the posterior probability that the globe was the Earth, conditional on seeing “land”.\n\\(Pr(Earth|land) = \\frac{Pr(land|Earth)Pr(Earth)}{Pr(land|Earth)Pr(Earth) + Pr(land|Mars)Pr(Mars)} = \\frac{(1-0.7)*0.5}{(1-0.7)*0.5 + 1.0*0.5} = 0.23\\)\n##2M4 Suppose you have a deck with only three cards. Each card has two sides, and each side is either black or white. One card has two black sides, the second card has one black, one white side and the third card has two white sides. Now these cards are shuffled in a bag, one is drawn and placed flat on a table. A black side is showing up. What is the color of the other side?\nLet \\(C_{1}\\) denote the card with 2 black sides, \\(C_{2}\\) be the card with one black and one white side and \\(C_{3}\\) be the card with two white sides.\nNow, \\(Pr(\\text{Side facing down is Black} | \\text{Side facing up is Black}) = \\frac{Pr(\\text{Side facing down is Black AND Side facing up is Black})}{Pr(\\text{Side facing up is Black})}\\)\nThe numerator can be computed as: \\(Pr(\\text{Side facing down is Black AND Side facing up is Black}) = Pr(C_{1})Pr(\\text{Side facing up is Black}|C_{1}) = \\frac{1}{3}\\)\nThe \\(1/3\\) is because each card is equally likely to be drawn.\nThe denominator can be computed as: \\(Pr(\\text{Side facing up is Black}) = \\sum_{i=1}^{3}Pr(C_{i})Pr(\\text{Side facing up is Black}|C_{i}) = \\frac{1}{3}(1.0 + 0.5 + 0) = \\frac{1}{2}\\)\nThus, \\(Pr(\\text{Side facing down is Black} | \\text{Side facing up is Black}) = \\frac{1/3}{1/2} = \\frac{2}{3}\\)\nAnother way to think about this is that there are three ways for the face up card to be black. Two ways if \\(C_{1}\\) is drawn and one way if \\(C_{2}\\) is drawn. And in only two of those ways is the side facing down also black.\n##2M5 Now suppose there are four cards: B/B, B/W, W/W and another B/B. Again suppose a card is drawn from the bag and a black side appears face up. Again calculate the probability that the other side is black.\nGiven that drawing each card is equally likely then there are \\(5\\) ways for the face up card to be black (\\(2\\) ways each from the first and the last card and one way from the second card). Out of these there are \\(4\\) ways for the face down card to be black.\nHence the probability that the side facing down is black given that the side facing up is black is \\(4/5\\).\n##2M6 Imagine that black ink is heavy, and so cards with black sides are heavier that cards with white sides. Assume again there are three cards: B/B, B/W, and W/W and assume that for every way to pull B/B there are two ways to pull B/W and 3 ways to pull the W/W card. Again, a card is pulled and a black side appears face up. What is the probability that the other side is black?\nSo there are \\(2\\) ways to get the side facing up as black from B/B. There are also \\(2\\) ways to get the side facing up as black from B/W (since we were told that there are two ways of drawing B/W for every way to draw B/B). Of these \\(4\\) ways there are \\(2\\) ways in which the face down card is also black.\nHence the probability that the side facing down is black given that the side facing up is black is \\(2/4\\) or \\(0.5\\).\n##2M7\nAssume again the original card problem (2M4). This time, before looking at the other side, we draw another card from the bag and lay it face up on the table. The face that is shown on the new card is white. What is the probability that the first card is black on the other side?\nThere are 8 ways of drawing two cards one after the other such that the side facing up is black for the first card and is white for the second card. Of these there are 6 ways for the side facing down of the first card to be black. Thus the probability is \\(6/8\\) or \\(0.75\\)."
  },
  {
    "objectID": "posts/2022-03-11-mcelreath-chapter-2.html#h1",
    "href": "posts/2022-03-11-mcelreath-chapter-2.html#h1",
    "title": "McElreath Chapter 2",
    "section": "2H1",
    "text": "2H1\nSuppose there are two species of panda bear. Both are equally common, they look exactly the same and there’s no genetic test to distinguish between them yet. They only differ in family size: Species A gives birth to twins 10% of the time while Species B births twins 20% of the time (otherwise it’s a single infant). Imagine you have a female panda that just gave birth to twins. What is the probability that her next birth will also be twins?\nLet \\(T_1\\) and \\(T_2\\) denote the event of having twins on the first and second birth respectively.\n\\(Pr(T_2|T_1) = \\frac{Pr(T_2,T_1)}{Pr(T_1)} = \\frac{Pr(T_2,T_1 | \\text{Species A})Pr(\\text{Species A}) + Pr(T_2,T_1 | \\text{Species B})Pr(\\text{Species B}) }{ Pr(T_1 | \\text{Species A})Pr(\\text{Species A}) + Pr(T_1 | \\text{Species B})Pr(\\text{Species B}) }\\)\nSince the species are equally common this means \\(Pr(\\text{Species A}) = Pr(\\text{Species B}) = 0.5\\) and we can rewrite the above as:\n\\(Pr(T_2|T_1) = \\frac{Pr(T_2,T_1 | \\text{Species A}) + Pr(T_2,T_1 | \\text{Species B}) }{ Pr(T_1 | \\text{Species A}) + Pr(T_1 | \\text{Species B}) } = \\frac{0.1*0.1 + 0.2*0.2}{0.1+0.2} = \\frac{0.05}{0.3} = \\frac{1}{6} \\approx 0.17\\)"
  },
  {
    "objectID": "posts/2022-03-11-mcelreath-chapter-2.html#h2",
    "href": "posts/2022-03-11-mcelreath-chapter-2.html#h2",
    "title": "McElreath Chapter 2",
    "section": "2H2",
    "text": "2H2\nNow compute the probability that the panda we have is from species A, given that we observed one twin birth.\n\\(Pr(\\text{Species A} | T_1) = \\frac{Pr(\\text{Species A} , T_1)}{ Pr(T_1) } = \\frac{ Pr(T_1 | \\text{Species A})Pr(\\text{Species A}) }{ Pr(T_1 | \\text{Species A})Pr(\\text{Species A}) + Pr(T_1 | \\text{Species B})Pr(\\text{Species B}) } = \\frac{0.1*0.5}{0.1*0.5+0.2*0.5}=\\frac{1}{3}\\)"
  },
  {
    "objectID": "posts/2022-03-11-mcelreath-chapter-2.html#h3",
    "href": "posts/2022-03-11-mcelreath-chapter-2.html#h3",
    "title": "McElreath Chapter 2",
    "section": "2H3",
    "text": "2H3\nSuppose the same panda mother has a second birth, this time to a singleton infant. What is the probability that this pandas is from species A?\nLet \\(S_2\\) denote the event of a singleton infant in the second birth.\n\\(𝑃𝑟(\\text{ Species A }|𝑇_1,S_2) = \\frac{ Pr(T_1,S_2 | \\text{Species A})Pr(\\text{Species A}) }{ Pr(T_1,S_2 | \\text{Species A})Pr(\\text{Species A}) + Pr(T_1,S_2 | \\text{Species B})Pr(\\text{Species B}) } = \\frac{0.1*0.9*0.5}{0.1*0.9*0.5+0.2*0.8*0.5}=\\frac{9}{25} = 0.36\\)\nThe nicer way to think about this is that the posterior from 2H2 gives us the updated priors for Species A and Species B (as \\(1/3\\) and \\(2/3\\) respectively).\nUsing these updated priors we get\n\\(𝑃𝑟(\\text{ Species A }|S_2) = \\frac{ Pr(S_2 | \\text{Species A})Pr(\\text{Species A}) }{ Pr(S_2 | \\text{Species A})Pr(\\text{Species A}) + Pr(S_2 | \\text{Species B})Pr(\\text{Species B}) } = \\frac{0.9*(1/3)}{0.9*(1/3)+0.8*(2/3)}=\\frac{9}{25} = 0.36\\)"
  },
  {
    "objectID": "posts/2022-03-11-mcelreath-chapter-2.html#h4",
    "href": "posts/2022-03-11-mcelreath-chapter-2.html#h4",
    "title": "McElreath Chapter 2",
    "section": "2H4",
    "text": "2H4\nSuppose now, there is a new genetic test that can identify the species of our mother panda. The test is imperfect though:\n\nThe probability it correctly identifies a species A panda is 0.8.\nThe probability it correctly identifies a species B panda is 0.65.\n\nThe test for our mother panda is positive for species A. Ignoring the information from the births, what is the probability that our panda is species A? Next redo your calculation, now using the data on both births (twins followed by singleton).\nLet \\(A\\) and \\(B\\) denote the event of testing positive for Species A and Species B respectively.\n\\(Pr(\\text{Species A} | A) = \\frac{Pr(A | \\text{Species A} )Pr(\\text{Species A})}{Pr(A | \\text{Species A} )Pr(\\text{Species A}) + Pr(A | \\text{Species B} )Pr(\\text{Species B})} = \\frac{0.8*0.5}{0.8*0.5+0.35*0.5} = 0.6956\\)\nNow using the data on two births and the preceding as a prior we get:\n\\(𝑃𝑟(\\text{ Species A }|A,𝑇_1,S_2) = \\frac{ Pr(T_1,S_2 | \\text{Species A})Pr(\\text{Species A}|A) }{ Pr(T_1,S_2 | \\text{Species A})Pr(\\text{Species A}|A) + Pr(T_1,S_2 | \\text{Species B})Pr(\\text{Species B}|A) } = \\frac{0.1*0.9*0.6956}{0.1*0.9*0.6956+0.2*0.8*0.3044}= 0.56\\)\nYet another way to attack this problem is to use the posterior 2H3 as our prior for Species A and Species B respectively,\n\\(Pr(\\text{Species A} | A,𝑇_1,S_2) = \\frac{Pr(A | \\text{Species A} )Pr(\\text{Species A}|𝑇_1,S_2)}{Pr(A | \\text{Species A} )Pr(\\text{Species A}|𝑇_1,S_2) + Pr(A | \\text{Species B} )Pr(\\text{Species B}|𝑇_1,S_2)} = \\frac{0.8*0.36}{0.8*0.36+0.35*(1-0.36)} = 0.56\\)"
  },
  {
    "objectID": "posts/2021-07-31-bear-classification-and-gradio.html",
    "href": "posts/2021-07-31-bear-classification-and-gradio.html",
    "title": "Bear Classification and Gradio",
    "section": "",
    "text": "I am following along with Chapter 2 of [1].\n\n\n\nInstall the jmd_imagescraper library using !pip3 install jmd_imagescraper\nI set FASTAI_HOME in my .bashrc so that datasets downloaded using Fastai are stored under a different location than the default. I do this because the default location, /home/kaushik/.fastai, is space constrained. Feel free to omit this step.\n\nexport FASTAI_HOME=/data/kaushik/.fastai\nexport PATH=$FASTAI_HOME:$PATH"
  },
  {
    "objectID": "posts/2021-07-31-bear-classification-and-gradio.html#inspect-a-few-items-from-the-validation-set",
    "href": "posts/2021-07-31-bear-classification-and-gradio.html#inspect-a-few-items-from-the-validation-set",
    "title": "Bear Classification and Gradio",
    "section": "Inspect a few items from the validation set",
    "text": "Inspect a few items from the validation set\n\n#collapse-output\ndls = bears.dataloaders(path)\ndls.valid.show_batch(max_n=4,nrows=1)"
  },
  {
    "objectID": "posts/2021-07-31-bear-classification-and-gradio.html#inspect-effect-of-data-augmentations",
    "href": "posts/2021-07-31-bear-classification-and-gradio.html#inspect-effect-of-data-augmentations",
    "title": "Bear Classification and Gradio",
    "section": "Inspect effect of Data Augmentations",
    "text": "Inspect effect of Data Augmentations\n\n#collapse-output\nbears = bears.new(item_tfms=Resize(128), batch_tfms=aug_transforms(mult=2))\ndls = bears.dataloaders(path)\ndls.train.show_batch(max_n=9, nrows=3, unique=True)"
  },
  {
    "objectID": "posts/2021-07-31-bear-classification-and-gradio.html#clean-the-data",
    "href": "posts/2021-07-31-bear-classification-and-gradio.html#clean-the-data",
    "title": "Bear Classification and Gradio",
    "section": "Clean the data",
    "text": "Clean the data\n\nfrom fastai.vision.widgets import *\n\nThe following will give us a UI that allows to mark images that are mislabeled (for relabeling) or completely wrong (for deletion).\n\ncleaner = ImageClassifierCleaner(learn)\ncleaner\n\n\n\n\n\n\n\n\n\n\nSo for each label we do the following:\n\nFirst we choose the images to delete or relabel.\nSecond we run the following:\n\nfor idx in cleaner.delete() : cleaner.fns[idx].unlink()\nfor idx, cat in cleaner.change(): shutil.move(str(cleaner.fns[idx]), path/cat)\nItems that need to be deleted or whose label needs to be changed\n\ncleaner.delete(), cleaner.change()\n\n((#4) [5,9,21,27],\n (#11) [(0, 'black'),(1, 'black'),(2, 'black'),(3, 'black'),(11, 'black'),(14, 'black'),(15, 'black'),(16, 'black'),(17, 'black'),(19, 'black')...])\n\n\n\nfor idx in cleaner.delete() : cleaner.fns[idx].unlink()\nfor idx, cat in cleaner.change(): shutil.move(str(cleaner.fns[idx]), path/cat)"
  },
  {
    "objectID": "posts/2021-07-31-bear-classification-and-gradio.html#sadness",
    "href": "posts/2021-07-31-bear-classification-and-gradio.html#sadness",
    "title": "Bear Classification and Gradio",
    "section": "Sadness",
    "text": "Sadness\nSo far so good, test images of bears seem to be recognized perfectly. So now there is nowhere to go but downhill.\n\ntest_mainecoon_image_location = '../test_images/mc.jpg'\nim = Image.open(test_mainecoon_image_location)\nim.to_thumb(128,128)\n\n\n\n\n\npred, pred_idx, probs = get_prediction(learn_inf, test_mainecoon_image_location)\nf'Prediction: {pred}; Probability: {probs[pred_idx]:.04f}'\n\n\n\n\n'Prediction: teddy; Probability: 0.8470'\n\n\nSo our classifier is very confident that the Maine Coon is a Teddy!\nWe work through how to tackle this issue by the use of multi-label classification in this post."
  },
  {
    "objectID": "posts/2022-05-24-lambert-w.html",
    "href": "posts/2022-05-24-lambert-w.html",
    "title": "Lambert W",
    "section": "",
    "text": "From [1]:\nThe Lambert W Function is denoted \\(W(x)\\) and is the inverse of the function \\(f(x) = xe^{x}\\).\nThus, \\(W(x) = f^{-1}(x)\\).\nFor \\(f(x)\\) we have that:\nThe last two bullet points mean that the domain and range of \\(W(x)\\) are \\([-\\frac{1}{e},\\infty)\\) and \\([-1,\\infty)\\) respectively.\nFor \\(W(x)\\) we also have that:"
  },
  {
    "objectID": "posts/2022-05-24-lambert-w.html#solve-xx-2",
    "href": "posts/2022-05-24-lambert-w.html#solve-xx-2",
    "title": "Lambert W",
    "section": "Solve \\(x^{x} = 2\\)",
    "text": "Solve \\(x^{x} = 2\\)\nTake the log of both sides to get \\(x\\ln(x) = \\ln(2)\\) or that \\(e^{\\ln(x)}\\ln(x) = \\ln(2)\\). Rearranging we get \\(\\ln(x)e^{\\ln(x)} = \\ln(2)\\).\nApplying the Lambert W function on both sides produces \\(W(\\ln(x)e^{\\ln(x)}) = W(\\ln(2))\\) or that \\(\\ln(x) = W(\\ln(2))\\).\nFinally, we get that \\(x = e^{W(\\ln(2))}\\).\n\nfrom scipy.special import lambertw\nimport numpy as np\n\n\nx = np.exp( lambertw( np.log(2) ) )\nx, np.allclose( np.power(x,x) , 2 )\n\n((1.5596104694623694+0j), True)\n\n\nThus, the solution of \\(x^{x} = 2\\) is \\(x = 1.55961\\)"
  },
  {
    "objectID": "posts/2022-05-24-lambert-w.html#solve-x2ex-2",
    "href": "posts/2022-05-24-lambert-w.html#solve-x2ex-2",
    "title": "Lambert W",
    "section": "Solve \\(x^{2}e^{x} = 2\\)",
    "text": "Solve \\(x^{2}e^{x} = 2\\)\nTake the square root in both sides to get \\(xe^{x/2} = \\sqrt 2\\). Divide both sides by \\(2\\) to get \\(0.5xe^{0.5x} = \\frac{1}{\\sqrt 2}\\).\nApplying the Lambert W function on both sides produces \\(W(0.5xe^{0.5x}) = W(\\frac{1}{\\sqrt 2})\\) or \\(0.5x = W(\\frac{1}{\\sqrt 2})\\).\nFinally, we get \\(x = 2W(\\frac{1}{\\sqrt 2})\\).\n\nx = 2 * lambertw(1./np.sqrt(2))\nx, np.allclose( x*x*np.exp(x), 2 )\n\n((0.9012010317296661+0j), True)"
  },
  {
    "objectID": "posts/2022-05-24-lambert-w.html#solve-x-ex-2",
    "href": "posts/2022-05-24-lambert-w.html#solve-x-ex-2",
    "title": "Lambert W",
    "section": "Solve \\(x + e^{x} = 2\\)",
    "text": "Solve \\(x + e^{x} = 2\\)\nExponentiating both sides we get \\(e^{x}e^{e^{x}} = e^{2}\\). Applying the Lambert W function on both sides produces \\(W( e^{x}e^{e^{x}} ) = W(e^{2})\\) or \\(e^{x} = W(e^{2})\\)\nThus, \\(x = \\ln(W(e^{2}))\\)\n\nx = np.log( lambertw( np.exp(2) ) )\nx, np.allclose( x + np.exp(x) , 2 )\n\n((0.4428544010023887+0j), True)"
  },
  {
    "objectID": "posts/2022-05-24-lambert-w.html#find-the-minimizer-of-x-lnfracxu-frac12lambdax-v2-suppose-lambda-0.",
    "href": "posts/2022-05-24-lambert-w.html#find-the-minimizer-of-x-lnfracxu-frac12lambdax-v2-suppose-lambda-0.",
    "title": "Lambert W",
    "section": "Find the minimizer of \\(x \\ln(\\frac{x}{u}) + \\frac{1}{2\\lambda}(x-v)^{2}\\) (suppose \\(\\lambda >0\\)).",
    "text": "Find the minimizer of \\(x \\ln(\\frac{x}{u}) + \\frac{1}{2\\lambda}(x-v)^{2}\\) (suppose \\(\\lambda >0\\)).\nThis is a sum of convex functions so it is convex and a minimizer exists.\nTaking the derivative and setting equal to zero we get: \\[1 + \\ln x - \\ln u + \\frac{x}{\\lambda} - \\frac{v}{\\lambda} = 0\\]\nHence, \\(\\ln x + \\frac{x}{\\lambda} = \\ln u + \\frac{v}{\\lambda} - 1\\).\nExponentiating both sides we get:\n\\(e^{\\ln x + \\frac{x}{\\lambda}} = e^{\\ln u + \\frac{v}{\\lambda} - 1}\\) or \\(xe^{\\frac{x}{\\lambda}} = ue^{\\frac{v}{\\lambda} - 1}\\).\nDividing both sides by \\(\\lambda\\) we have \\(\\frac{x}{\\lambda}e^{\\frac{x}{\\lambda}} = \\frac{u}{\\lambda}e^{\\frac{v}{\\lambda} - 1}\\).\nApplying the Lambert W function on both sides gives: \\[ W(\\frac{x}{\\lambda}e^{\\frac{x}{\\lambda}}) = W(\\frac{u}{\\lambda}e^{\\frac{v}{\\lambda} - 1})\\] or\n\\(\\frac{x}{\\lambda} = W(\\frac{u}{\\lambda}e^{\\frac{v}{\\lambda} - 1})\\).\nThus, \\[x = \\lambda W(\\frac{u}{\\lambda}e^{\\frac{v}{\\lambda} - 1})\\]"
  },
  {
    "objectID": "posts/2021-08-27-quadratic-curve-fit-with-gradient-descent.html",
    "href": "posts/2021-08-27-quadratic-curve-fit-with-gradient-descent.html",
    "title": "Quadratic Curve Fit with Gradient Descent",
    "section": "",
    "text": "Someone walks up to you and hands over a simple data set of x and y coordinates. You eyeball it and think, “hmm seems like we can fit a quadratic function to this”. This post walks through how this can be done using gradient descent (and follows the treatment in Chapter 4 of [1]).\n\nfrom fastai.imports import *\nfrom fastai.torch_imports import *\nfrom fastai.torch_core import *\n\n\nnum_points = 20\nx = torch.arange(0,num_points).float()\nx\n\ntensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13.,\n        14., 15., 16., 17., 18., 19.])\n\n\n\nd = 0.75\ne = 9.5\nnoise = 1 + torch.randn(num_points)*3\ny =  d*(x-e)**2 + noise\ny, -2*d*e, d*(e**2)\n\n(tensor([68.6996, 53.2421, 43.1072, 35.0240, 18.7474, 20.2123,  4.7285,  7.6421,\n          0.6463, -0.4163, -3.0223,  5.8641,  4.2702, 12.0058, 16.8083, 20.2149,\n         32.4927, 41.8890, 59.8105, 63.5625]),\n -14.25,\n 67.6875)\n\n\nSince \\(d(x-e)^{2} = d*(x^{2} -2ex + e^{2}) = dx^{2} - 2dex + d*e^{2}\\) we expect to see the parameters \\(d\\), \\(-2*d*e\\) and \\(\\approx 1+d*e^{2}\\) being uncovered. The last term is because we are adding a \\(1\\) in the noise.\nThe data then looks like the following\n\nplt.scatter(x,y)\n\n<matplotlib.collections.PathCollection>"
  },
  {
    "objectID": "posts/2021-08-27-quadratic-curve-fit-with-gradient-descent.html#initialize-the-parameters",
    "href": "posts/2021-08-27-quadratic-curve-fit-with-gradient-descent.html#initialize-the-parameters",
    "title": "Quadratic Curve Fit with Gradient Descent",
    "section": "Initialize the parameters",
    "text": "Initialize the parameters\nWe need three parameters to describe a quadratic \\(ax^{2}+bx +c\\)\n\nparams = torch.randn(3).requires_grad_()"
  },
  {
    "objectID": "posts/2021-08-27-quadratic-curve-fit-with-gradient-descent.html#calculate-the-predictions",
    "href": "posts/2021-08-27-quadratic-curve-fit-with-gradient-descent.html#calculate-the-predictions",
    "title": "Quadratic Curve Fit with Gradient Descent",
    "section": "Calculate the predictions",
    "text": "Calculate the predictions\nWe create a function where we can plug in our input features (\\(x\\) coordinate values in this case) and get a prediction for the \\(y\\). The function below will do just that.\n\ndef f(x, params):\n    a,b,c=params\n    return a*(x**2) + b*x + c\n\nThe predictions, yhat, can then be obtained using\n\nyhat = f(x, params)\nyhat\n\ntensor([-1.0763e-01, -9.4094e-01, -4.0345e+00, -9.3884e+00, -1.7002e+01,\n        -2.6877e+01, -3.9011e+01, -5.3406e+01, -7.0062e+01, -8.8977e+01,\n        -1.1015e+02, -1.3359e+02, -1.5928e+02, -1.8724e+02, -2.1746e+02,\n        -2.4994e+02, -2.8467e+02, -3.2167e+02, -3.6093e+02, -4.0245e+02],\n       grad_fn=<AddBackward0>)\n\n\nCompare predictions to the data\n\nplt.scatter(x, y)\nplt.scatter(x, to_np(yhat), color = 'red')\n\n<matplotlib.collections.PathCollection>"
  },
  {
    "objectID": "posts/2021-08-27-quadratic-curve-fit-with-gradient-descent.html#calculate-the-loss",
    "href": "posts/2021-08-27-quadratic-curve-fit-with-gradient-descent.html#calculate-the-loss",
    "title": "Quadratic Curve Fit with Gradient Descent",
    "section": "Calculate the loss",
    "text": "Calculate the loss\nWe will use mean squared error for this.\n\ndef mse(yhat, y):\n    return ((yhat-y)**2).mean()\n\n\nloss = mse(yhat, y)\nloss\n\ntensor(44301.6484, grad_fn=<MeanBackward0>)"
  },
  {
    "objectID": "posts/2021-08-27-quadratic-curve-fit-with-gradient-descent.html#calculate-the-gradients",
    "href": "posts/2021-08-27-quadratic-curve-fit-with-gradient-descent.html#calculate-the-gradients",
    "title": "Quadratic Curve Fit with Gradient Descent",
    "section": "Calculate the gradients",
    "text": "Calculate the gradients\n\nloss.backward()\n\n\nparams.grad\n\ntensor([-70097.9297,  -4488.5391,   -324.2715])"
  },
  {
    "objectID": "posts/2021-08-27-quadratic-curve-fit-with-gradient-descent.html#take-a-gradient-descent-step",
    "href": "posts/2021-08-27-quadratic-curve-fit-with-gradient-descent.html#take-a-gradient-descent-step",
    "title": "Quadratic Curve Fit with Gradient Descent",
    "section": "Take a gradient descent step",
    "text": "Take a gradient descent step\nSet a learning rate, lr, and then do a step like so\n\nlr = 1e-5\nparams.data -= lr*params.grad.data\nparams.grad = None\n\nGet a new prediction and plot again.\n\nplt.scatter(x, y)\nplt.scatter(x, to_np(f(x,params)), color = 'red')\n\n<matplotlib.collections.PathCollection>"
  },
  {
    "objectID": "posts/2021-10-25-image-embeddings.html",
    "href": "posts/2021-10-25-image-embeddings.html",
    "title": "Image Embeddings with fastai",
    "section": "",
    "text": "We start off by following Chapter 18 of [1].\n\n\nCode\nfrom fastai.vision.all import *\n\n\nLoad the pets dataset. We have a cat image if the filename starts with an uppercase letter otherwise it is a dog image. Thus, an output of False (or 0) corresponds to a dog image while True (or 1) corresponds to a cat image.\n\npath = untar_data(URLs.PETS)/'images'\ndef is_cat(x): return x[0].isupper()\n\nset_seed(42, reproducible=True)\ndls = ImageDataLoaders.from_name_func(\n    path, get_image_files(path), valid_pct=0.2, seed=21,\n    label_func=is_cat, item_tfms=Resize(224))\n\nlearn = cnn_learner(dls, resnet34, metrics=error_rate)\n\n\n    \n        \n      \n      100.00% [811712512/811706944 00:11<00:00]\n    \n    \n\n\nDownloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n\n\n\n\n\n/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n\n\n\nlearn.fine_tune(1)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.139079\n      0.015485\n      0.004060\n      01:43\n    \n  \n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.055484\n      0.008766\n      0.002030\n      02:21\n    \n  \n\n\n\nDownload a sample image for which we will extract activations from the model.\n\n#collapse-output\n!cd sample_data/ && wget https://wallup.net/wp-content/uploads/2016/01/18662-cat-Nikon-camera-animals-biting.jpg\n\n--2021-10-25 18:30:27--  https://wallup.net/wp-content/uploads/2016/01/18662-cat-Nikon-camera-animals-biting.jpg\nResolving wallup.net (wallup.net)... 104.21.67.12, 172.67.167.152, 2606:4700:3030::6815:430c, ...\nConnecting to wallup.net (wallup.net)|104.21.67.12|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: unspecified [image/jpg]\nSaving to: ‘18662-cat-Nikon-camera-animals-biting.jpg’\n\n18662-cat-Nikon-cam     [   <=>              ] 661.12K  1.48MB/s    in 0.4s    \n\n2021-10-25 18:30:28 (1.48 MB/s) - ‘18662-cat-Nikon-camera-animals-biting.jpg’ saved [676983]\n\n\n\n\nimg = PILImage.create('sample_data/18662-cat-Nikon-camera-animals-biting.jpg')\nx, = first(dls.test_dl([img]))\n\n\nx.shape\n\ntorch.Size([1, 3, 224, 224])\n\n\n\nclass Hook():\n    def hook_func(self, m, i, o): self.stored = o.detach().clone()\n\n\nhook_output = Hook()\nhook = learn.model[0].register_forward_hook(hook_output.hook_func)\n\nwith torch.no_grad(): output = learn.model.eval()(x)\n\nact = hook_output.stored[0]\n\nact.shape, output.shape, learn.model[1][-1].weight.shape\n\n(torch.Size([512, 7, 7]), torch.Size([1, 2]), torch.Size([2, 512]))\n\n\nThe output from the final linear layer is as follows:\n\noutput\n\ntensor([[-11.0284,  10.0199]], device='cuda:0')\n\n\nPass these through a softmax to get the predictions.\n\nF.softmax(output, dim=-1)\n\ntensor([[7.2253e-10, 1.0000e+00]], device='cuda:0')\n\n\nOur model is very confident that this is a cat image. Recall that \\(0\\) encodes the class dog while \\(1\\) encodes cat.\n\ncam_map = torch.einsum('ck,kij->cij', learn.model[1][-1].weight, act)\ncam_map.shape\n\ntorch.Size([2, 7, 7])\n\n\n\n#collapse-output\ncam_map\n\ntensor([[[ 2.4612,  2.2954,  2.6431,  2.6772,  2.2160,  0.7992,  0.1278],\n         [ 2.4558,  2.4668,  2.6862,  2.7808,  3.5802,  3.8468,  2.2116],\n         [ 2.4177,  1.4307,  0.6136,  1.9688,  3.1065,  4.2844,  4.0371],\n         [ 3.1196,  2.8230,  1.0616,  1.4165,  4.1816,  7.1037,  7.4767],\n         [ 1.6073,  2.0172,  1.8519,  3.1966,  5.6278,  6.5893,  5.9116],\n         [ 3.6521,  5.5285,  4.9569,  5.3924,  5.6237,  5.3038,  4.2787],\n         [ 4.7321,  5.9305,  4.7503,  4.2259,  4.8412,  3.8734,  1.8111]],\n\n        [[-1.7039, -0.6877, -0.9466, -1.1840, -1.4696, -1.5489, -1.1791],\n         [-2.1570, -2.2049, -2.1784, -2.7038, -1.0871, -0.4607, -1.5684],\n         [-4.5787, -4.8054, -2.9991, -2.3972,  1.1643,  3.3241,  1.6656],\n         [-2.7080, -2.8545, -1.7021, -1.4661,  0.9079,  3.0881,  2.8714],\n         [-0.1489,  1.2784,  0.8865,  0.2769,  0.3615,  0.9657,  0.2920],\n         [ 1.1236,  2.5738,  0.8960,  0.1405,  1.0975,  0.5311,  0.6032],\n         [ 2.1957,  2.4272,  0.7434, -1.2605,  0.4661, -0.0410,  0.7805]]],\n       device='cuda:0', grad_fn=<ViewBackward>)\n\n\nFrom the book, “note that we need to decode the input x since it’s been normalized by the DataLoader, and we need to cast to TensorImage since at the time this book is written PyTorch does not maintain types when indexing”.\n\n#collapse-output\ndecoded = dls.train.decode((x,))\ndecoded\n\n(TensorImage([[[[219, 220, 221,  ...,   8,   8,   7],\n           [220, 220, 221,  ...,   8,  10,  10],\n           [222, 221, 221,  ...,   9,  10,  10],\n           ...,\n           [ 14,  29,  23,  ...,  60,  54,  60],\n           [  4,  20,  36,  ...,  42,  44,  49],\n           [  4,   7,  29,  ...,  43,  44,  43]],\n \n          [[215, 216, 217,  ...,   7,   6,   6],\n           [216, 216, 217,  ...,   6,   7,   7],\n           [218, 217, 217,  ...,   6,   7,   7],\n           ...,\n           [ 11,  24,  14,  ...,  37,  30,  35],\n           [  2,  14,  27,  ...,  26,  27,  29],\n           [  1,   4,  22,  ...,  28,  29,  27]],\n \n          [[214, 215, 215,  ...,   2,   1,   1],\n           [214, 213, 214,  ...,   1,   0,   0],\n           [215, 214, 214,  ...,   1,   0,   0],\n           ...,\n           [  4,  12,   7,  ...,  14,  12,  15],\n           [  0,   6,  17,  ...,   6,  11,  11],\n           [  0,   2,  15,  ...,  10,  10,   8]]]]),)\n\n\n\ndecoded[0].shape, decoded[0][0].shape\n\n(torch.Size([1, 3, 224, 224]), torch.Size([3, 224, 224]))\n\n\n\ndef plot_image(the_map, the_alpha=0.6):\n  x_dec = TensorImage(decoded[0][0])\n  _,ax = plt.subplots()\n  x_dec.show(ctx=ax)\n  ax.imshow(the_map.detach().cpu(), alpha=the_alpha, extent=(0,224,224,0),\n            interpolation='bilinear', cmap='magma');\n\nFirst look at the areas of the picture that influence the network to decide that something may be a dog image. Bright yellow correspond to high activations while areas in purple correspond to low activations\n\nplot_image(cam_map[0])\n\n\n\n\nThe network appears to be focussing on the mouth.\nNext look at the areas of the picture that influence the network to decide that something is a cat image.\n\nplot_image(cam_map[1])\n\n\n\n\nThe network appears to be focussing on the entire face in the decision to label this as a cat image.\nCompare this to a random activations map where there is no singular area of focus.\n\n#collapse-output\nrand_cmap = torch.randn((7,7), device='cuda:0')\nrand_cmap\n\ntensor([[-0.7637, -1.3432,  1.1882, -0.3386, -0.6900, -0.6698, -0.5695],\n        [ 1.1075, -1.1094, -0.2857,  1.3275,  0.8616,  1.9981,  0.8572],\n        [-0.3191, -0.8256,  0.6985,  0.1862,  0.1230, -1.1539, -0.0230],\n        [-0.7334, -0.8848,  0.1476, -0.8934,  0.6844,  0.4076, -0.7339],\n        [ 1.0249,  0.6300, -0.1655,  0.6776,  0.8610, -0.9098,  0.2662],\n        [-0.1487,  0.7496,  1.2978,  0.9887, -0.6177,  0.2148, -0.4154],\n        [-0.6950, -0.6311, -0.7638,  0.0544, -1.0707, -0.4667, -1.0079]],\n       device='cuda:0')\n\n\n\nplot_image(rand_cmap)\n\n\n\n\nLet’s create some fake maps just to get a sense for how this overlay of the map over the cat image works. The alpha determines the extent to which the map dominates the original image. Use alpha = 1 to just display the map by itself (here the cat image is not seen at all).\n\nmy_map = tensor([[0., 0., 1., 0., 0., 0., 0.],\n        [ 1., 1., 1., 1., 1., 1., 1.],\n        [ 0., 0., 1., 0., 0., 0., 0.],\n        [ 0., 0., 1., 0., 0., 0., 0.],\n        [ 0., 0., 1., 0., 0., 0., 0.],\n        [ 1., 1., 1., 1., 1., 1., 1.],\n        [ 0., 0., 0., 0., 0., 0., 0.]],\n       device='cuda:0')\n\n\nplot_image(my_map, the_alpha=1.)\n\n\n\n\nSo the row and column of ones show up as yellow. Next reduce the value of alpha so the cat image can also be seen.\n\nplot_image(my_map, the_alpha= 0.6)\n\n\n\n\nIn the following map we put weight mainly on the left half.\n\nmy_map = tensor([[0., 0., 0.,  0.,  0., 0., 0.],\n        [ 1.,  1., 1., 1., 0.,  0.,  0.],\n        [ 2.,  2., 2., 2., 0.,  0.,  0.],\n        [ 3.,  3., 3., 3., 0.,  0.,  0.],\n        [ 2.,  2., 2., 2., 0.,  0.,  0.],\n        [ 1.,  1., 1., 1., 0.,  0.,  0.],\n        [ 0.,  0., 0., 0., 0.,  0.,  0.]],\n       device='cuda:0')\n\nplot_image(my_map, the_alpha= 0.6)\n\n\n\n\nFinally remove the hook so it doesn’t leak any memory.\n\nhook.remove()\n\n\n\n\n\nReferences\n\n[1] J. Howard and S. Gugger, Deep learning for coders with fastai and PyTorch: AI applications without a PhD, 1st ed. O’Reilly, 2020."
  },
  {
    "objectID": "posts/2022-03-19-mcelreath-chapter-3.html",
    "href": "posts/2022-03-19-mcelreath-chapter-3.html",
    "title": "McElreath Chapter 3",
    "section": "",
    "text": "Exercise problems from Chapter 3, Sampling the Imaginary, of [1]."
  },
  {
    "objectID": "posts/2022-03-19-mcelreath-chapter-3.html#e1",
    "href": "posts/2022-03-19-mcelreath-chapter-3.html#e1",
    "title": "McElreath Chapter 3",
    "section": "3E1",
    "text": "3E1\nFrom the samples, how much posterior probability lies below \\(p=0.2\\)?\n\nnp.sum( samples < 0.2 ) /1e4 \n\n0.0008"
  },
  {
    "objectID": "posts/2022-03-19-mcelreath-chapter-3.html#e2",
    "href": "posts/2022-03-19-mcelreath-chapter-3.html#e2",
    "title": "McElreath Chapter 3",
    "section": "3E2",
    "text": "3E2\nFrom the samples, how much posterior probability lies above \\(p=0.8\\)?\n\nnp.sum( samples > 0.8 ) /1e4 \n\n0.1243"
  },
  {
    "objectID": "posts/2022-03-19-mcelreath-chapter-3.html#e3",
    "href": "posts/2022-03-19-mcelreath-chapter-3.html#e3",
    "title": "McElreath Chapter 3",
    "section": "3E3",
    "text": "3E3\nFrom the samples, how much posterior probability lies between \\(p=0.2\\) and \\(p=0.8\\)?\n\nnp.sum( (samples > 0.2) & (samples < 0.8) ) /1e4 \n\n0.8749"
  },
  {
    "objectID": "posts/2022-03-19-mcelreath-chapter-3.html#e4",
    "href": "posts/2022-03-19-mcelreath-chapter-3.html#e4",
    "title": "McElreath Chapter 3",
    "section": "3E4",
    "text": "3E4\nFrom the samples, \\(20\\%\\) of the posterior probability lies below which value of \\(p\\)?\n\nnp.quantile( samples , 0.2 )\n\n0.5175175175175175"
  },
  {
    "objectID": "posts/2022-03-19-mcelreath-chapter-3.html#e5",
    "href": "posts/2022-03-19-mcelreath-chapter-3.html#e5",
    "title": "McElreath Chapter 3",
    "section": "3E5",
    "text": "3E5\nFrom the samples, \\(20\\%\\) of the posterior probability lies above which value of \\(p\\)?\n\nnp.quantile( samples , 0.8 )\n\n0.7627627627627628"
  },
  {
    "objectID": "posts/2022-03-19-mcelreath-chapter-3.html#e6",
    "href": "posts/2022-03-19-mcelreath-chapter-3.html#e6",
    "title": "McElreath Chapter 3",
    "section": "3E6",
    "text": "3E6\nWhich values of \\(p\\) contain the narrowest interval equal to \\(66\\%\\) of the posterior probability?\n\naz.hdi( samples , hdi_prob=0.66 )\n\narray([0.53053053, 0.8028028 ])"
  },
  {
    "objectID": "posts/2022-03-19-mcelreath-chapter-3.html#e7",
    "href": "posts/2022-03-19-mcelreath-chapter-3.html#e7",
    "title": "McElreath Chapter 3",
    "section": "3E7",
    "text": "3E7\nWhich values of \\(p\\) contain \\(66\\%\\) of the posterior probability, assuming equal posterior probability both below and above the interval?\n\nnp.quantile( samples , (0.17, 0.66 + 0.17) )\n\narray([0.5005005 , 0.77777778])"
  },
  {
    "objectID": "posts/2022-03-19-mcelreath-chapter-3.html#m1",
    "href": "posts/2022-03-19-mcelreath-chapter-3.html#m1",
    "title": "McElreath Chapter 3",
    "section": "3M1",
    "text": "3M1\nSuppose the globe tossing data had turned out to be 8 water in 15 tosses. Construct the posterior distribution, using grid approximation. Use the same flat prior as before.\n\np_grid, posterior = grid_approx(1e3, 8, 15)"
  },
  {
    "objectID": "posts/2022-03-19-mcelreath-chapter-3.html#m2",
    "href": "posts/2022-03-19-mcelreath-chapter-3.html#m2",
    "title": "McElreath Chapter 3",
    "section": "3M2",
    "text": "3M2\nDraw 10,000 samples from the posterior above. Then use the samples to calculate the 90% HPDI for p.\n\nsamples = np.random.choice( p_grid , p=posterior , size=int(1e4) , \n                           replace=True )\naz.hdi( samples , hdi_prob=0.9 )\n\narray([0.34034034, 0.72572573])"
  },
  {
    "objectID": "posts/2022-03-19-mcelreath-chapter-3.html#m3",
    "href": "posts/2022-03-19-mcelreath-chapter-3.html#m3",
    "title": "McElreath Chapter 3",
    "section": "3M3",
    "text": "3M3\nConstruct a posterior predictive check for this model and data. This means simulate the distribution of samples, averaging over the posterior uncertainty in p. What is the probability of observing 8 water in 15 tosses?\n\n# Conduct 1 experiment where we toss the globe 15 times \n# and the fraction of the water on the globe is 0.5 \n# and record the number of waters.\nnp.random.binomial( n=15 , p=0.5 , size=1 )\n\narray([9])\n\n\n\n# Conduct 2 experiment where we toss the globe 15 times \n# and the fraction of the water on the globe is 0.2 and 0.5 in the first and \n# second experiment respectively.\n# Record the number of waters.\nnp.random.binomial( n=15 , p= [0.2, 0.5] )\n\narray([3, 7])\n\n\n\nppd = np.random.binomial( n=15 , p=samples )\nppd.shape\n\n(10000,)\n\n\n\nfrom collections import Counter\n\n\ncount_dict = Counter(ppd)\n# We recorded 8 waters as the outcome in 1,460 out of the 10,000 experiments\ncount_dict\n\nCounter({0: 7,\n         1: 35,\n         2: 119,\n         3: 260,\n         4: 525,\n         5: 833,\n         6: 1199,\n         7: 1412,\n         8: 1460,\n         9: 1337,\n         10: 1169,\n         11: 791,\n         12: 522,\n         13: 241,\n         14: 73,\n         15: 17})\n\n\n\npossible_num_waters = sorted( count_dict.keys()  )\nplt.bar( possible_num_waters , \n        [count_dict[waters] for waters in possible_num_waters ] )\nplt.xlabel('number of water samples')\nplt.ylabel('count')\n\nText(0, 0.5, 'count')\n\n\n\n\n\n\n# There is 14.6% probability of observing 8 waters in 15 tosses\nnp.sum( ppd == 8) / ppd.shape[0]\n\n0.146"
  },
  {
    "objectID": "posts/2022-03-19-mcelreath-chapter-3.html#m4",
    "href": "posts/2022-03-19-mcelreath-chapter-3.html#m4",
    "title": "McElreath Chapter 3",
    "section": "3M4",
    "text": "3M4\nUsing the posterior distribution constructed from the new (8/15) data, now calculate the probability of observing 6 water in 9 tosses.\n\nppd = np.random.binomial( n=9 , p=samples )\nnp.sum( ppd == 6) / ppd.shape[0]\n\n0.1789"
  },
  {
    "objectID": "posts/2022-03-19-mcelreath-chapter-3.html#m5",
    "href": "posts/2022-03-19-mcelreath-chapter-3.html#m5",
    "title": "McElreath Chapter 3",
    "section": "3M5",
    "text": "3M5\nStart over at 3M1, but now use a prior that is zero below p=0.5 and a constant above p=0.5. This corresponds to prior information that a majority of the Earth’s surface is water. Repeat each problem above and compare the inferences. What difference does the better prior make? If it helps, compare inferences (using both priors) to the true value p=0.7.\n\nnp.heaviside( np.array([0, 0.25, 0.5, 0.75, 1]) - 0.5, 0.5) * 2\n\narray([0., 0., 1., 2., 2.])\n\n\n\nnp.array([0, 0.25, 0.5, 0.75, 1]) >= 0.5\n\narray([False, False,  True,  True,  True])\n\n\n\n( np.array([0, 0.25, 0.5, 0.75, 1]) >= 0.5 ).astype(int)\n\narray([0, 0, 1, 1, 1])\n\n\n\n( np.array([0, 0.25, 0.5, 0.75, 1]) >= 0.5 ).astype(int) * 2.\n\narray([0., 0., 2., 2., 2.])\n\n\n\ndef grid_approx2(num_grid_points, successes, tosses):\n  p_grid = np.linspace( 0 , 1 , int(num_grid_points) )\n  prior = ( p_grid >= 0.5 ).astype(int) * 2. # truncated prior\n  likelihood = stats.binom.pmf( k=successes , n=tosses , p=p_grid )\n  unstd_posterior = prior * likelihood\n  posterior = unstd_posterior / unstd_posterior.sum()\n  return p_grid, posterior\n\n\np_grid, posterior = grid_approx2(1e3, 8, 15) #3M1'\n\n# 3M2'\nsamples = np.random.choice( p_grid , p=posterior , size=int(1e4) \n                           , replace=True )\naz.hdi( samples , hdi_prob=0.9 )\n\narray([0.5005005 , 0.71271271])\n\n\n\n# 3M3'\nppd = np.random.binomial( n=15 , p=samples )\n\ncount_dict = Counter(ppd)\ncount_dict\n\nCounter({1: 2,\n         2: 6,\n         3: 45,\n         4: 127,\n         5: 341,\n         6: 664,\n         7: 1107,\n         8: 1623,\n         9: 1739,\n         10: 1613,\n         11: 1359,\n         12: 825,\n         13: 383,\n         14: 141,\n         15: 25})\n\n\n\npossible_num_waters = sorted( count_dict.keys()  )\nplt.bar( possible_num_waters , \n        [count_dict[waters] for waters in possible_num_waters ] )\nplt.xlabel('number of water samples')\nplt.ylabel('count')\n\nText(0, 0.5, 'count')\n\n\n\n\n\n\n# reasonable amount of mass on 10 waters\n# 0.7 is the true fraction of water covering the globe\nprint(f'Likelihood {(np.sum( ppd == int(15*0.7) ) / ppd.shape[0]):0.4f} of observing 10 waters in 15 tosses')\n\nLikelihood 0.1613 of observing 10 waters in 15 tosses\n\n\n\nprint(f'Likelihood {(np.sum( ppd == 8) / ppd.shape[0]):0.4f} of observing 8 waters in 15 tosses')\n\nLikelihood 0.1623 of observing 8 waters in 15 tosses"
  },
  {
    "objectID": "posts/2022-03-19-mcelreath-chapter-3.html#m6",
    "href": "posts/2022-03-19-mcelreath-chapter-3.html#m6",
    "title": "McElreath Chapter 3",
    "section": "3M6",
    "text": "3M6\nSuppose you want to estimate the Earth’s proportion of water very precisely. Specifically, you want the 99% percentile interval of the posterior distribution of p to be only 0.05 wide. This means the distance between the upper and lower bound of the interval should be 0.05. How many times will you have to toss the globe to do this?\n\np_water = np.linspace(0,1,10) # ground truth of the fraction of water on Earth\n\nfor p_true in p_water:\n  converged=False\n  N_tosses=1\n  while not converged:\n    # if we observe int(N_tosses*p_true) waters in N_tosses\n    # get the posterior distribution of the fraction of water\n    p_grid, posterior = grid_approx( 1e3, int(N_tosses*p_true), N_tosses )\n    # get samples from the posterior\n    samples = np.random.choice(p_grid, p=posterior, size=int(1e4), replace=True)\n    # compute the 99% interval\n    interval = np.quantile( samples , (0.05, 0.995) )\n    # get the width of the interval\n    width = interval[1] - interval[0]\n    converged = ( width <= 0.05 )\n    # print(p_true, N_tosses, width)\n    N_tosses += 1\n  print(f'{p_true:0.2f}', N_tosses)\n\n0.00 104\n0.11 729\n0.22 1217\n0.33 1523\n0.44 1722\n0.56 1698\n0.67 1509\n0.78 1190\n0.89 668\n1.00 59"
  },
  {
    "objectID": "posts/2022-03-19-mcelreath-chapter-3.html#hard",
    "href": "posts/2022-03-19-mcelreath-chapter-3.html#hard",
    "title": "McElreath Chapter 3",
    "section": "Hard",
    "text": "Hard\nThe Hard problems here all use the data below. These data indicate the gender (male=1, female=0) of officially reported first and second born children in 100 two-child families.\n\nbirth1 = np.array([1,0,0,0,1,1,0,1,0,1,0,0,1,1,0,1,1,0,0,0,1,0,0,0,1,0,\n0,0,0,1,1,1,0,1,0,1,1,1,0,1,0,1,1,0,1,0,0,1,1,0,1,0,0,0,0,0,0,0,\n1,1,0,1,0,0,1,0,0,0,1,0,0,1,1,1,1,0,1,0,1,1,1,1,1,0,0,1,0,1,1,0,\n1,0,1,1,1,0,1,1,1,1])\nbirth2 = np.array([0,1,0,1,0,1,1,1,0,0,1,1,1,1,1,0,0,1,1,1,0,0,1,1,1,0,\n1,1,1,0,1,1,1,0,1,0,0,1,1,1,1,0,0,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,\n1,1,1,0,1,1,0,1,1,0,1,1,1,0,0,0,0,0,0,1,0,0,0,1,1,0,0,1,0,0,1,1,\n0,0,0,1,1,1,0,0,0,0])\n\n\nnp.sum(birth1) + np.sum(birth2)\n\n111\n\n\n\nbirth1.shape[0] + birth2.shape[0]\n\n200"
  },
  {
    "objectID": "posts/2022-03-19-mcelreath-chapter-3.html#h1",
    "href": "posts/2022-03-19-mcelreath-chapter-3.html#h1",
    "title": "McElreath Chapter 3",
    "section": "3H1",
    "text": "3H1\nUsing grid approximation, compute the posterior distribution for the probability of a birth being a boy. Assume a uniform prior probability. Which parameter value maximizes the posterior probability?\n\np_grid, posterior = grid_approx(1000, np.sum(birth1) + np.sum(birth2), \n                                birth1.shape[0] + birth2.shape[0])\np_grid[ np.argmax(posterior) ]\n\n0.5545545545545546\n\n\n##3H2\nUsing the sample function, draw 10,000 random parameter values from the posterior distribution you calculated above. Use these samples to estimate the 50%, 89%, and 97% highest posterior density intervals.\n\nsamples = np.random.choice(p_grid, p=posterior, size=int(1e4), replace=True)\n\n\nprint(f'50% HPDI {az.hdi(samples, hdi_prob=0.5)}')\nprint(f'89% HPDI {az.hdi(samples, hdi_prob=0.89)}')\nprint(f'97% HPDI {az.hdi(samples, hdi_prob=0.97)}')\n\n50% HPDI [0.52552553 0.57357357]\n89% HPDI [0.5005005  0.61161161]\n97% HPDI [0.48248248 0.63263263]"
  },
  {
    "objectID": "posts/2022-03-19-mcelreath-chapter-3.html#h3",
    "href": "posts/2022-03-19-mcelreath-chapter-3.html#h3",
    "title": "McElreath Chapter 3",
    "section": "3H3",
    "text": "3H3\nUse rbinom to simulate 10,000 replicates of 200 births. You should end up with 10,000 numbers, each one a count of boys out of 200 births. Compare the distribution of predicted numbers of boys to the actual count in the data (111 boys out of 200 births). There are many good ways to visualize the simulations, but the dens command (part of the rethinking package) is probably the easiest way in this case. Does it look like the model fits the data well? That is, does the distribution of predictions include the actual observation as a central, likely outcome?\n\nsampled_births = np.random.binomial(n=200, p=samples)\nsampled_births[:10]\n\narray([106, 116, 117, 134, 115, 105, 124, 106, 109, 122])\n\n\nIt looks like the model fits the data well and the distribution of predictions includes the actual observation of 111 boys as a central, likely outcome.\n\nplt.hist(sampled_births)\nplt.title(\"Posterior predictive distribution of #boys per 200 births\")\nplt.xlabel(\"#Boys\")\nplt.ylabel(\"Frequency\");"
  },
  {
    "objectID": "posts/2022-03-19-mcelreath-chapter-3.html#h4",
    "href": "posts/2022-03-19-mcelreath-chapter-3.html#h4",
    "title": "McElreath Chapter 3",
    "section": "3H4",
    "text": "3H4\nNow compare 10,000 counts of boys from 100 simulated first borns only to the number of boys in the first births, birth1. How does the model look in this light?\n\nfirst_births = np.random.binomial(n=100, p=samples)\nplt.hist(first_births)\nplt.title(\"Posterior predictive distribution of #boys per 100 births\")\nplt.xlabel(\"#Boys\")\nplt.ylabel(\"Frequency\");\nplt.axvline(np.sum(birth1), c=\"r\", label=\"Observed births\")\n\n<matplotlib.lines.Line2D>\n\n\n\n\n\n\nprint(f'number of boys in birth1 is {np.sum(birth1)}')\n\nnumber of boys in birth1 is 51\n\n\nIn first_births 51 is not in the center of the predicted number of boys.\n##3H5\nThe model assumes that sex of first and second births are independent. To check this assumption, focus now on second births that followed female first borns. Compare 10,000 simulated counts of boys to only those second births that followed girls. To do this correctly, you need to count the number of first borns who were girls and simulate that many births, 10,000 times. Compare the counts of boys in your simulations to the actual observed count of boys following girls. How does the model look in this light? Any guesses what is going on in these data?\n\n#number of first borns who were girls\ngirls_first_born = birth1.shape[0] - np.sum(birth1)\ngirls_first_born\n\n49\n\n\n\n# Conduct 10,000 experiments where in each experiment \n# we simulate births in the 49 families which had girl first borns\n# we record the outcome of the number of boys\nsecond_births_after_gfb = np.random.binomial( n=girls_first_born, p=samples )\nsecond_births_after_gfb[:10]\n\narray([24, 35, 28, 25, 32, 24, 25, 28, 30, 24])\n\n\n\nplt.hist(second_births_after_gfb)\nplt.title(\"Posterior predictive distribution of #boys after girl births\")\nplt.xlabel(\"#Boys\")\nplt.ylabel(\"Frequency\");\nplt.axvline(np.sum(birth2[birth1==0]), c=\"r\", label=\"Observed births\")\n\n<matplotlib.lines.Line2D>\n\n\n\n\n\nMore boy births were observed in comparison to what our model predicts to be the likely outcomes. This means that the independence assumption we made in our small world model of each birth being of either sex does not hold for this dataset."
  },
  {
    "objectID": "posts/2022-04-10-mcelreath-chapter-4.html",
    "href": "posts/2022-04-10-mcelreath-chapter-4.html",
    "title": "McElreath Chapter 4",
    "section": "",
    "text": "Code\nimport arviz as az\nimport pandas as pd\nimport numpy as np\nimport pymc3 as pm\n\nfrom matplotlib import pylab as plt\nfrom scipy import stats\n\nfrom patsy import dmatrix\n\naz.style.use(\"arviz-darkgrid\")\naz.rcParams[\"stats.hdi_prob\"] = 0.89"
  },
  {
    "objectID": "posts/2022-04-10-mcelreath-chapter-4.html#m1",
    "href": "posts/2022-04-10-mcelreath-chapter-4.html#m1",
    "title": "McElreath Chapter 4",
    "section": "4M1",
    "text": "4M1\nFor the model definition below, simulate observed \\(y\\) values from the prior (not the posterior).\n\\[y \\sim \\text{Normal}(\\mu, \\sigma) \\] \\[\\mu \\sim \\text{Normal}(0, 10) \\] \\[\\sigma \\sim \\text{Exponential}(1) \\]\n\nn_samples = 1000\nsample_mu = stats.norm.rvs( loc=0, scale=10, size=n_samples )\nsample_std = stats.expon.rvs( scale=1 , size=n_samples )\nsample_y = stats.norm.rvs( loc=sample_mu, scale=sample_std, size=n_samples )\n\n\naz.plot_kde(sample_y)\nplt.title('Distribution of Prior Predictive values')\n\nText(0.5, 1.0, 'Distribution of Prior Predictive values')\n\n\n\n\n\n\nplt.hist(sample_y)\n\n(array([ 10.,  35., 116., 202., 232., 218., 118.,  47.,  17.,   5.]),\n array([-30.14262363, -23.65619686, -17.1697701 , -10.68334333,\n         -4.19691657,   2.2895102 ,   8.77593697,  15.26236373,\n         21.7487905 ,  28.23521727,  34.72164403]),\n <a list of 10 Patch objects>)"
  },
  {
    "objectID": "posts/2022-04-10-mcelreath-chapter-4.html#m4",
    "href": "posts/2022-04-10-mcelreath-chapter-4.html#m4",
    "title": "McElreath Chapter 4",
    "section": "4M4",
    "text": "4M4\nA sample of students is measured for height each year for 3 years. After the third year, you want to fit a linear regression predicting height using year as a predictor. Write down the mathematical model definition for this regression, using any variable names and priors you choose. Be prepared to defend your choice of priors.\nLet \\(h_{ij}\\) be the height of the \\(i\\)-th student in year \\(j\\) (where \\(j \\in \\{1,2,3\\}\\)).\n\\[h_{ij} \\sim N(\\mu_{ij},\\sigma)\\] \\[\\mu_{ij} = \\alpha + \\beta(x_{ij}-\\bar{x})\\] \\[\\alpha \\sim N(100,10)\\] \\[\\beta \\sim \\text{Normal}(0,10)\\] \\[\\sigma \\sim \\text{Exponential}(1)\\]\n\nLet \\(x_{ij}\\) be the year corresponding to student \\(i\\) for year \\(j\\) in the dataset.\n\nIn particular, \\(x_{i1}=1, x_{i2}=2\\) and \\(x_{i3}=3\\).\n\nLet \\(\\bar{x}\\) be the average year in the dataset.\n\nAs each student was measured for height each year, \\(\\bar{x} = \\frac{1+2+3}{3}=2\\).\n\nThe mean height of the students is taking to be 100 cm.\nThe Exponential is used as the prior distribution for \\(\\sigma\\) (the standard deviation) to constrain it to be positive.\n\nNext, let’s simulate a dataset consisting of 40 children:\n\nn_samples = 40\nsample_alpha = stats.norm.rvs( loc=100, scale=10, size=n_samples )\nsample_beta = stats.norm.rvs( loc=0, scale=10, size=n_samples )\nsample_std = stats.expon.rvs( scale=1 , size=n_samples )\n\nyears = np.array([1,2,3])\nxbar = np.mean( years )\n\n# sample_y = stats.norm.rvs( loc=sample_mu, scale=sample_std, size=n_samples )\n\n\n# Each row is a student, column 1 is for year-1, column 2 is for year-2 and\n# column 3 is for year-3\nsample_mu = ( sample_alpha.reshape(n_samples, 1) + \n             sample_beta.reshape(n_samples, 1) * (years - xbar).reshape(1, 3) )\n\n\n# Each row is a student, column 1 is for year-1, column 2 is for year-2 and\n# column 3 is for year-3\nsample_height = np.zeros((n_samples, 3))\nfor i in range(n_samples):\n  for j in years:\n    mu_ij = sample_alpha[i] + sample_beta[i]*(j - xbar)\n    sample_height[i, j-1] = stats.norm.rvs( mu_ij , sample_std[i] )\n\n\nfor i in range(n_samples):\n  plt.plot( years , sample_height[i] , color='blue' )\nplt.xticks( years )\nplt.xlabel( 'year' )\nplt.ylabel( 'height (cm)' )\n\nText(0, 0.5, 'height (cm)')\n\n\n\n\n\nEach line tracks the simulated height of each student across the 3 years. Clearly there are some nonsensical relationships here where the height for some students actually goes down across the years."
  },
  {
    "objectID": "posts/2022-04-10-mcelreath-chapter-4.html#m5",
    "href": "posts/2022-04-10-mcelreath-chapter-4.html#m5",
    "title": "McElreath Chapter 4",
    "section": "4M5",
    "text": "4M5\nNow suppose I remind you that every student got taller each year. Does this information lead you to change your choice of priors? How?\nYes, we can impose a Log-Normal prior on the slope so that it is constrained to be positive. Per [1], if \\(Z \\sim \\text{Normal}(0,1)\\) and if \\(\\mu\\) and \\(\\sigma > 0\\) are two real numbers then \\(\\beta = e^{\\mu + \\sigma Z}\\) is log-normally distributed with parameters \\(\\mu\\) and \\(\\sigma\\). Also \\(E[\\beta] = e^{\\mu + \\frac{1}{2}\\sigma^{2}}\\). * So if \\(\\mu=1\\) and \\(\\sigma=0.1\\) then \\(E[\\beta] \\approx 2.73 \\text{cm/year}\\)\n\nn_samples = 40\nsample_alpha = stats.norm.rvs( loc=100, scale=10, size=n_samples )\nsample_beta = np.random.lognormal( mean=1, sigma=0.1, size=n_samples)\nsample_std = stats.expon.rvs( scale=1 , size=n_samples )\n\nyears = np.array([1,2,3])\nxbar = np.mean( years )\n\nsample_height = np.zeros((n_samples, 3))\nfor i in range(n_samples):\n  for j in years:\n    mu_ij = sample_alpha[i] + sample_beta[i]*(j - xbar)\n    sample_height[i, j-1] = stats.norm.rvs( mu_ij , sample_std[i] )\n\n\nfor i in range(n_samples):\n  plt.plot( years , sample_height[i] , color='blue' )\nplt.xticks( years )\nplt.xlabel( 'year' )\nplt.ylabel( 'height (cm)' )\n\nText(0, 0.5, 'height (cm)')\n\n\n\n\n\nThis looks much better with height increasing across the years. However there are still some weird observations where the height zig-zags."
  },
  {
    "objectID": "posts/2022-04-10-mcelreath-chapter-4.html#m6",
    "href": "posts/2022-04-10-mcelreath-chapter-4.html#m6",
    "title": "McElreath Chapter 4",
    "section": "4M6",
    "text": "4M6\nNow suppose I tell you that the variance among heights for students of the same age is never more than 64cm. How does this lead you to revise your priors?\nThis would lead me to set \\(\\sigma \\sim \\text{Uniform}(0,\\sqrt{64})\\)"
  },
  {
    "objectID": "posts/2022-04-10-mcelreath-chapter-4.html#m7",
    "href": "posts/2022-04-10-mcelreath-chapter-4.html#m7",
    "title": "McElreath Chapter 4",
    "section": "4M7",
    "text": "4M7\nRefit model m4.3 from the chapter, but omit the mean weight xbar this time. Compare the new model’s posterior to that of the original model. In particular, look at the covariance among the parameters. What is different? Then compare the posterior predictions of both models.\n\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      height\n      weight\n      age\n      male\n    \n  \n  \n    \n      0\n      151.765\n      47.825606\n      63.0\n      1\n    \n    \n      1\n      139.700\n      36.485807\n      63.0\n      0\n    \n    \n      2\n      136.525\n      31.864838\n      65.0\n      0\n    \n    \n      3\n      156.845\n      53.041914\n      41.0\n      1\n    \n    \n      4\n      145.415\n      41.276872\n      51.0\n      0\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nxbar = d2.weight.mean()\n\nwith pm.Model() as m4_3:\n    a = pm.Normal(\"a\", mu=178, sd=20)\n    b = pm.Lognormal(\"b\", mu=0, sd=1)\n    sigma = pm.Uniform(\"sigma\", 0, 50)\n    mu = a + b * (d2.weight - xbar)\n    height = pm.Normal(\"height\", mu=mu, sd=sigma, observed=d2.height)\n    trace_4_3 = pm.sample(1000, tune=1000, return_inferencedata=False)\n\nwith pm.Model() as m4M7:\n  alpha = pm.Normal( 'alpha', mu=178, sd=20 )\n  beta = pm.Lognormal( 'beta', mu=0, sd=1 )\n  sigma = pm.Uniform( 'sigma' , lower=0, upper=50 )\n  mu = alpha + beta*d2.weight\n  height = pm.Normal( 'height' , mu=mu, sd=sigma, observed=d2.height )\n  trace_4M7 = pm.sample(1000, tune=1000, return_inferencedata=False)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nSequential sampling (2 chains in 1 job)\nNUTS: [sigma, b, a]\n\n\n\n\n\n\n\n    \n      \n      100.00% [2000/2000 00:10<00:00 Sampling chain 0, 0 divergences]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      100.00% [2000/2000 00:08<00:00 Sampling chain 1, 0 divergences]\n    \n    \n\n\nSampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 19 seconds.\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nSequential sampling (2 chains in 1 job)\nNUTS: [sigma, beta, alpha]\n\n\n\n\n\n\n\n    \n      \n      100.00% [2000/2000 00:13<00:00 Sampling chain 0, 0 divergences]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      100.00% [2000/2000 00:13<00:00 Sampling chain 1, 0 divergences]\n    \n    \n\n\nSampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 27 seconds.\nThe acceptance probability does not match the target. It is 0.8897350788344733, but should be close to 0.8. Try to increase the number of tuning steps.\n\n\n\naz.summary(trace_4M7, kind=\"stats\")\n\nGot error No model on context stack. trying to find log_likelihood in translation.\n/usr/local/lib/python3.7/dist-packages/arviz/data/io_pymc3_3x.py:102: FutureWarning: Using `from_pymc3` without the model will be deprecated in a future release. Not using the model will return less accurate and less useful results. Make sure you use the model argument or call from_pymc3 within a model context.\n  FutureWarning,\n\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      mean\n      sd\n      hdi_5.5%\n      hdi_94.5%\n    \n  \n  \n    \n      alpha\n      114.491\n      1.858\n      111.612\n      117.473\n    \n    \n      beta\n      0.892\n      0.041\n      0.826\n      0.955\n    \n    \n      sigma\n      5.093\n      0.202\n      4.750\n      5.383\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\naz.summary(trace_4_3, kind=\"stats\")\n\nGot error No model on context stack. trying to find log_likelihood in translation.\n/usr/local/lib/python3.7/dist-packages/arviz/data/io_pymc3_3x.py:102: FutureWarning: Using `from_pymc3` without the model will be deprecated in a future release. Not using the model will return less accurate and less useful results. Make sure you use the model argument or call from_pymc3 within a model context.\n  FutureWarning,\n\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      mean\n      sd\n      hdi_5.5%\n      hdi_94.5%\n    \n  \n  \n    \n      a\n      154.604\n      0.281\n      154.128\n      155.004\n    \n    \n      b\n      0.903\n      0.043\n      0.839\n      0.975\n    \n    \n      sigma\n      5.104\n      0.202\n      4.774\n      5.416\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nModel 4_3 intercept (a) is larger than in M47 (alpha) but the slope is roughly the same.\n\nPlot the posterior inference against the data\n\nSuperimpose the posterior mean values over the height and weight data\n\nplt.scatter(d2['weight'], d2['height'])\nplt.plot(d2['weight'], ( trace_4M7['alpha'].mean() + \n                        trace_4M7['beta'].mean()*d2['weight'] ),\n         color='orange' )\nplt.xlabel(d2.columns[1])\nplt.ylabel(d2.columns[0])\n\nText(0, 0.5, 'height')\n\n\n\n\n\n\n\nAdd the uncertainty around the mean\nFirst add the uncertainty around the average height.\n\ntrace_iq = trace_4M7 #trace in question\nposterior_samples = np.random.randint(len(trace_iq), size=10000)\nweight_seq = np.arange(0, 71)\n\n\ncred_intervals = np.array(\n    [ az.hdi( trace_iq['alpha'][posterior_samples] + \n             trace_iq['beta'][posterior_samples]*wt ) for wt in weight_seq ]\n)\n\n\ncred_intervals.shape\n\n(71, 2)\n\n\n\nplt.scatter(d2['weight'], d2['height'])\nplt.plot(weight_seq, ( trace_4M7['alpha'].mean() + \n                        trace_4M7['beta'].mean()*weight_seq ),\n         color='orange' )\nplt.fill_between(\n    weight_seq, cred_intervals[:, 0], cred_intervals[:, 1], alpha=0.4, \n    label=r\"Uncertainty in $\\mu$\",\n)\nplt.xlabel(d2.columns[1])\nplt.ylabel(d2.columns[0])\nplt.axvline(np.mean(d2.weight), ls=\"--\", c=\"r\", label=\"Mean weight\")\nplt.legend()\n\n<matplotlib.legend.Legend>\n\n\n\n\n\n\n\n\nAdd the prediction intervals\nNext add the uncertainty around the actual heights making use of the standard deviation.\n\ntrace_iq['alpha'][posterior_samples].shape\n\n(10000,)\n\n\n\n# define a function to compute mu for all posterior samples at given value of \n# weight\ndef compute_mu(w):\n    return ( trace_iq['alpha'][posterior_samples] + \n            trace_iq['beta'][posterior_samples] * w )\n\npi_67 = np.array(\n    [\n        az.hdi(np.random.normal(loc=compute_mu(x), \n                                scale=trace_iq['sigma'][posterior_samples]), \n                                hdi_prob=0.67)\n        for x in weight_seq\n    ]\n)\n\npi_89 = np.array(\n    [\n        az.hdi(np.random.normal(loc=compute_mu(x), \n                                scale=trace_iq['sigma'][posterior_samples]))\n        for x in weight_seq\n    ]\n)\n\npi_97 = np.array(\n    [\n        az.hdi(np.random.normal(loc=compute_mu(x), \n                                scale=trace_iq['sigma'][posterior_samples]), \n                                hdi_prob=0.97)\n        for x in weight_seq\n    ]\n)\n\n\nplt.scatter(d2['weight'], d2['height'])\nplt.plot(weight_seq, ( trace_4M7['alpha'].mean() + \n                        trace_4M7['beta'].mean()*weight_seq ),\n         color='black' )\nplt.fill_between(\n    weight_seq, cred_intervals[:, 0], cred_intervals[:, 1], alpha=0.2, \n    label=r\"Uncertainty in $\\mu$\",\n)\nplt.fill_between(\n    weight_seq, pi_67[:, 0], pi_67[:, 1], \n    alpha=0.2, color='blue' \n)\nplt.fill_between(\n    weight_seq, pi_89[:, 0], pi_89[:, 1], \n    alpha=0.1, color='blue' \n)\nplt.fill_between(\n    weight_seq, pi_97[:, 0], pi_97[:, 1], \n    alpha=0.05, color='blue' \n)\nplt.xlabel(d2.columns[1])\nplt.ylabel(d2.columns[0])\nplt.axvline(np.mean(d2.weight), ls=\"--\", c=\"r\", label=\"Mean weight\")\nplt.xlim(d2.weight.min(), d2.weight.max());\nplt.legend()\n\n<matplotlib.legend.Legend>\n\n\n\n\n\nAnother way to generate the same plot.\n\n# Given that we have 2,000 samples let's use 200 for plotting \n# (or we can use all of them too if desired)\ntrace_iq_thinned = trace_iq[::10]\nmu_pred = np.zeros((len(weight_seq), len(trace_iq_thinned) * trace_iq.nchains))\nfor i, w in enumerate(weight_seq):\n    mu_pred[i] = ( trace_iq_thinned[\"alpha\"] + \n                  trace_iq_thinned[\"beta\"] * w )\n\n\ntrace_iq_thinned, mu_pred.shape\n\n(<MultiTrace: 2 chains, 100 iterations, 5 variables>, (71, 200))\n\n\n\nmu_mean = mu_pred.mean(1) \nmu_hdi = az.hdi(mu_pred.T)\nmu_mean.shape, mu_hdi.shape\n\n/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: FutureWarning: hdi currently interprets 2d data as (draw, shape) but this will change in a future release to (chain, draw) for coherence with other functions\n  \n\n\n((71,), (71, 2))\n\n\nGenerate heights from the posterior manually.\n\npost_samples = []\nfor _ in range(10000):  # number of samples from the posterior\n    i = np.random.randint(len(trace_iq))\n    mu_pr = trace_iq['alpha'][i] + trace_iq['beta'][i] * weight_seq\n    sigma_pred = trace_iq['sigma'][i]\n    post_samples.append(np.random.normal(mu_pr, sigma_pred))\n\n\nax=az.plot_hdi(weight_seq, mu_pred.T)\naz.plot_hdi(weight_seq, np.array(post_samples), ax=ax, hdi_prob=0.67)\naz.plot_hdi(weight_seq, np.array(post_samples), ax=ax, hdi_prob=0.89)\naz.plot_hdi(weight_seq, np.array(post_samples), ax=ax, hdi_prob=0.97)\nplt.scatter(d2.weight, d2.height)\nplt.plot(weight_seq, mu_mean, \"k\")\nplt.axvline(np.mean(d2.weight), ls=\"--\", c=\"r\", label=\"Mean weight\")\nplt.xlabel(\"weight\")\nplt.ylabel(\"height\")\nplt.xlim(d2.weight.min(), d2.weight.max());\n\n/usr/local/lib/python3.7/dist-packages/arviz/plots/hdiplot.py:157: FutureWarning: hdi currently interprets 2d data as (draw, shape) but this will change in a future release to (chain, draw) for coherence with other functions\n  hdi_data = hdi(y, hdi_prob=hdi_prob, circular=circular, multimodal=False, **hdi_kwargs)\n/usr/local/lib/python3.7/dist-packages/arviz/plots/hdiplot.py:157: FutureWarning: hdi currently interprets 2d data as (draw, shape) but this will change in a future release to (chain, draw) for coherence with other functions\n  hdi_data = hdi(y, hdi_prob=hdi_prob, circular=circular, multimodal=False, **hdi_kwargs)\n/usr/local/lib/python3.7/dist-packages/arviz/plots/hdiplot.py:157: FutureWarning: hdi currently interprets 2d data as (draw, shape) but this will change in a future release to (chain, draw) for coherence with other functions\n  hdi_data = hdi(y, hdi_prob=hdi_prob, circular=circular, multimodal=False, **hdi_kwargs)\n/usr/local/lib/python3.7/dist-packages/arviz/plots/hdiplot.py:157: FutureWarning: hdi currently interprets 2d data as (draw, shape) but this will change in a future release to (chain, draw) for coherence with other functions\n  hdi_data = hdi(y, hdi_prob=hdi_prob, circular=circular, multimodal=False, **hdi_kwargs)\n\n\n\n\n\n\ntrace_4M7_df = pm.trace_to_dataframe( trace_4M7 )\ntrace_4M7_df.cov().round(3)\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      alpha\n      beta\n      sigma\n    \n  \n  \n    \n      alpha\n      3.452\n      -0.076\n      -0.026\n    \n    \n      beta\n      -0.076\n      0.002\n      0.001\n    \n    \n      sigma\n      -0.026\n      0.001\n      0.041\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nThere doesn’t seem to be much of a covariance between the parameters. This is similar to what we saw in m_4_3 (shown below).\n\npm.trace_to_dataframe( trace_4_3 ).cov().round(3)\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      a\n      b\n      sigma\n    \n  \n  \n    \n      a\n      0.079\n      -0.000\n      -0.000\n    \n    \n      b\n      -0.000\n      0.002\n      0.000\n    \n    \n      sigma\n      -0.000\n      0.000\n      0.041\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nNext let’s examine the correlations.\n\ntrace_4M7_df.corr().round(3)\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      alpha\n      beta\n      sigma\n    \n  \n  \n    \n      alpha\n      1.00\n      -0.99\n      -0.07\n    \n    \n      beta\n      -0.99\n      1.00\n      0.07\n    \n    \n      sigma\n      -0.07\n      0.07\n      1.00\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nIntercept (alpha) and slope (beta) are negatively correlated unlike model m_4_3.\n\npm.trace_to_dataframe( trace_4_3 ).corr().round(3)\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      a\n      b\n      sigma\n    \n  \n  \n    \n      a\n      1.000\n      -0.041\n      -0.000\n    \n    \n      b\n      -0.041\n      1.000\n      0.051\n    \n    \n      sigma\n      -0.000\n      0.051\n      1.000"
  },
  {
    "objectID": "posts/2022-04-10-mcelreath-chapter-4.html#m8",
    "href": "posts/2022-04-10-mcelreath-chapter-4.html#m8",
    "title": "McElreath Chapter 4",
    "section": "4M8",
    "text": "4M8\nIn the chapter, we used 15 knots with the cherry blossom spline. Increase the number of knots and observe what happens to the resulting spline. Then adjust also the width of the prior on the weights – change the standard deviation of the prior and watch what happens. What do you think the combination of knot number and the prior on the weight controls?\nSuppose \\(D_{i}\\) be the date of year of the cherry blossom in year \\(i\\) then a formal description of the model is:\n\\[D_{i} \\sim N(\\mu_{i},\\sigma)\\] \\[\\mu_{i} = \\alpha + \\sum_{k=1}^{K}w_{k}B_{ki}\\] \\[\\alpha \\sim N(100,10)\\] \\[w_{k} \\sim \\text{Normal}(0,10)\\] \\[\\sigma \\sim \\text{Exponential}(1)\\]\n\\(B_{ki}\\) is the value of the \\(k\\)-th basis function for year \\(i\\).\n\nurl = 'https://raw.githubusercontent.com/rmcelreath/rethinking/'\\\n'master/data/cherry_blossoms.csv'\nd = pd.read_csv(url, sep=';', header=0)\nd.head()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      year\n      doy\n      temp\n      temp_upper\n      temp_lower\n    \n  \n  \n    \n      0\n      801\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      1\n      802\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      803\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      3\n      804\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      4\n      805\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nd2 = d.dropna(subset=['doy'])\nd2.head()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      year\n      doy\n      temp\n      temp_upper\n      temp_lower\n    \n  \n  \n    \n      11\n      812\n      92.0\n      NaN\n      NaN\n      NaN\n    \n    \n      14\n      815\n      105.0\n      NaN\n      NaN\n      NaN\n    \n    \n      30\n      831\n      96.0\n      NaN\n      NaN\n      NaN\n    \n    \n      50\n      851\n      108.0\n      7.38\n      12.1\n      2.66\n    \n    \n      52\n      853\n      104.0\n      NaN\n      NaN\n      NaN\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nnum_knots = 15\nknot_list = np.quantile(d2.year, np.linspace(0, 1, num_knots))\n\n\n\nCode\nknot_list\n\n\narray([ 812., 1036., 1174., 1269., 1377., 1454., 1518., 1583., 1650.,\n       1714., 1774., 1833., 1893., 1956., 2015.])\n\n\n\nB = dmatrix(\n    \"bs(year, knots=knots, degree=3, include_intercept=True) - 1\",\n    {\"year\": d2.year.values, \"knots\": knot_list[1:-1]},\n)\n\n\n\nCode\nnp.asarray( B ).shape\n\n\n(827, 17)\n\n\n\n\nCode\nB\n\n\nDesignMatrix with shape (827, 17)\n  Columns:\n    ['bs(year, knots=knots, degree=3, include_intercept=True)[0]',\n     'bs(year, knots=knots, degree=3, include_intercept=True)[1]',\n     'bs(year, knots=knots, degree=3, include_intercept=True)[2]',\n     'bs(year, knots=knots, degree=3, include_intercept=True)[3]',\n     'bs(year, knots=knots, degree=3, include_intercept=True)[4]',\n     'bs(year, knots=knots, degree=3, include_intercept=True)[5]',\n     'bs(year, knots=knots, degree=3, include_intercept=True)[6]',\n     'bs(year, knots=knots, degree=3, include_intercept=True)[7]',\n     'bs(year, knots=knots, degree=3, include_intercept=True)[8]',\n     'bs(year, knots=knots, degree=3, include_intercept=True)[9]',\n     'bs(year, knots=knots, degree=3, include_intercept=True)[10]',\n     'bs(year, knots=knots, degree=3, include_intercept=True)[11]',\n     'bs(year, knots=knots, degree=3, include_intercept=True)[12]',\n     'bs(year, knots=knots, degree=3, include_intercept=True)[13]',\n     'bs(year, knots=knots, degree=3, include_intercept=True)[14]',\n     'bs(year, knots=knots, degree=3, include_intercept=True)[15]',\n     'bs(year, knots=knots, degree=3, include_intercept=True)[16]']\n  Terms:\n    'bs(year, knots=knots, degree=3, include_intercept=True)' (columns 0:17)\n  (to view full data, use np.asarray(this_obj))\n\n\nLet’s plot the basis functions.\n\n\nCode\n_, ax = plt.subplots(1, 1, figsize=(12, 4))\nfor i in range( B.shape[1] ):\n    ax.plot(d2.year, (B[:, i]))\nax.set_xlabel(\"year\")\nax.set_ylabel(\"basis\");\n\n\n\n\n\nNext, let’s learn the parameters.\n\nwith pm.Model() as m_4M8:\n    a = pm.Normal(\"a\", 100, 10)\n    w = pm.Normal(\"w\", mu=0, sd=10, shape=B.shape[1])\n    mu = pm.Deterministic(\"mu\", a + pm.math.dot(B.base, w.T))\n    sigma = pm.Exponential(\"sigma\", 1)\n    D = pm.Normal(\"D\", mu, sigma, observed=d2.doy)\n    trace_m_4M8 = pm.sample(2000, tune=2000, chains=2, \n                            return_inferencedata=False)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nSequential sampling (2 chains in 1 job)\nNUTS: [sigma, w, a]\n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 00:40<00:00 Sampling chain 0, 0 divergences]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 00:40<00:00 Sampling chain 1, 0 divergences]\n    \n    \n\n\nSampling 2 chains for 2_000 tune and 2_000 draw iterations (4_000 + 4_000 draws total) took 81 seconds.\nThe number of effective samples is smaller than 25% for some parameters.\n\n\nPlot each basis weighted by it’s corresponding parameter.\n\n\nCode\n_, ax = plt.subplots(1, 1, figsize=(12, 4))\nwp = trace_m_4M8['w'].mean(0) #17 entries, one for each basis fn\nfor i in range( B.shape[1] ):\n    ax.plot(d2.year, (wp[i] * B[:, i]), color=\"C0\")\nax.set_xlim(812, 2015)\nax.set_ylim(-6, 6);\n\n\n\n\n\n\npost_pred = ( az.summary(trace_m_4M8, var_names=[\"mu\"], hdi_prob=0.94).\n             reset_index(drop=True) )\npost_pred.head()\n\nGot error No model on context stack. trying to find log_likelihood in translation.\n/usr/local/lib/python3.7/dist-packages/arviz/data/io_pymc3_3x.py:102: FutureWarning: Using `from_pymc3` without the model will be deprecated in a future release. Not using the model will return less accurate and less useful results. Make sure you use the model argument or call from_pymc3 within a model context.\n  FutureWarning,\n\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n      mcse_mean\n      mcse_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      0\n      100.320\n      3.179\n      93.831\n      105.847\n      0.050\n      0.035\n      4039.0\n      3348.0\n      1.0\n    \n    \n      1\n      100.406\n      2.994\n      94.330\n      105.655\n      0.047\n      0.033\n      4124.0\n      3292.0\n      1.0\n    \n    \n      2\n      100.822\n      2.182\n      96.796\n      105.020\n      0.032\n      0.023\n      4692.0\n      3168.0\n      1.0\n    \n    \n      3\n      101.262\n      1.555\n      98.286\n      104.116\n      0.021\n      0.015\n      5426.0\n      3262.0\n      1.0\n    \n    \n      4\n      101.302\n      1.514\n      98.484\n      104.162\n      0.021\n      0.015\n      5442.0\n      3287.0\n      1.0\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nplt.fill_between(\n    d2.year,\n    post_pred[\"hdi_3%\"],\n    post_pred[\"hdi_97%\"],\n    color=\"firebrick\",\n    alpha=0.4,\n);\n\nax = plt.gca()\nax.plot(d2.year, d2.doy, \"o\", alpha=0.3)\nfor knot in knot_list:\n    ax.axvline(knot, color=\"grey\", alpha=0.4);\nax.plot(d2.year, \n        post_pred['mean'],\n        lw=3, color=\"firebrick\")\n\nfig = plt.gcf()\nfig.set_size_inches(12, 4)\nax.set_xlabel(\"year\")\nax.set_ylabel(\"days in year\")\nax.set_title(f'knots={num_knots}')\n\nText(0.5, 1.0, 'knots=15')\n\n\n\n\n\nIncrease the number of knots to 30.\n\n\nText(0.5, 1.0, 'knots=30')\n\n\n\n\n\nChange the width of the prior on the weights.\n\n\nText(0.5, 1.0, 'knots=30; w ~ N(0,100)')\n\n\n\n\n\n\n\nText(0.5, 1.0, 'knots=30; w ~ N(0,1)')\n\n\n\n\n\n\n\nText(0.5, 1.0, 'knots=5; w ~ N(0,100)')\n\n\n\n\n\nThe larger the number of knots the more local variation we are able to capture. The prior on the weight controls how much weights are allowed to vary around the mean. A tighter (smaller) standard deviation on the weight will not allow the weight to vary much."
  },
  {
    "objectID": "posts/2021-08-06-mnist-threes-and-sevens.html",
    "href": "posts/2021-08-06-mnist-threes-and-sevens.html",
    "title": "MNIST: Distinguishing Threes from Sevens",
    "section": "",
    "text": "I am following along with Chapter 4 of [1].\n\n\n\nfrom fastai.vision.all import *\n\n\n#collapse-output\npath = untar_data(URLs.MNIST_SAMPLE)\n\nSo what got downloaded?\n\npath.ls()\n\n(#3) [Path('/data/kaushik/.fastai/data/mnist_sample/valid'),Path('/data/kaushik/.fastai/data/mnist_sample/train'),Path('/data/kaushik/.fastai/data/mnist_sample/labels.csv')]\n\n\nThat huge path is a pain to look at so shorten it by setting the BASE_PATH.\n\nPath.BASE_PATH = path\npath.ls()\n\n(#3) [Path('valid'),Path('train'),Path('labels.csv')]\n\n\nWhat do we have under train?\n\n(path/'train').ls()\n\n(#2) [Path('train/3'),Path('train/7')]\n\n\nWhat do we have under the 7?\n\n(path/'train'/'7').ls().sorted()\n\n(#6265) [Path('train/7/10002.png'),Path('train/7/1001.png'),Path('train/7/10014.png'),Path('train/7/10019.png'),Path('train/7/10039.png'),Path('train/7/10046.png'),Path('train/7/10050.png'),Path('train/7/10063.png'),Path('train/7/10077.png'),Path('train/7/10086.png')...]\n\n\nWe have 6,265 images of sevens. Look at one using the PIL library.\n\nImage.open((path/'train'/'7').ls().sorted()[0])\n\n\n\n\n\nseven_tensors = [tensor(Image.open(pic_path)).float()/255. for pic_path in (path/'train'/'7').ls().sorted()]\nthree_tensors = [tensor(Image.open(pic_path)).float()/255. for pic_path in (path/'train'/'3').ls().sorted()]\nlen(seven_tensors), len(three_tensors)\n\n(6265, 6131)\n\n\nUse Fastai convenience function show_image to display the tensor\n\nshow_image(seven_tensors[0])\n\n<AxesSubplot:>\n\n\n\n\n\n\nstacked_sevens = torch.stack(seven_tensors)\nstacked_threes = torch.stack(three_tensors)\nstacked_sevens.shape, stacked_threes.shape\n\n(torch.Size([6265, 28, 28]), torch.Size([6131, 28, 28]))"
  },
  {
    "objectID": "posts/2021-08-06-mnist-threes-and-sevens.html#batch-accuracy",
    "href": "posts/2021-08-06-mnist-threes-and-sevens.html#batch-accuracy",
    "title": "MNIST: Distinguishing Threes from Sevens",
    "section": "Batch Accuracy",
    "text": "Batch Accuracy\n\ndef batch_accuracy(preds, yb):\n    preds = preds.sigmoid() #note\n    correct = (preds > 0.5).float() == yb\n    return correct.float().mean()\n\n\nbatch_accuracy(linear1(batch), train_y[:4])\n\ntensor(0.5000)\n\n\n\ndef validate_epoch(model):\n    accs = [batch_accuracy(model(xb), yb) for xb, yb in valid_dl]\n    return round(torch.stack(accs).mean().item(), 4)\n\n\nvalidate_epoch(linear1)\n\n0.6533\n\n\nTrain one epoch\n\nlr = 1\n\nweights = init_params((28*28,1))\nbias = init_params(1)\nparams = weights, bias\n\ntrain_epoch(linear1, lr, params)\nvalidate_epoch(linear1)\n\n0.6836\n\n\nTrain multiple epochs\n\nlr = 1\n\nweights = init_params((28*28,1))\nbias = init_params(1)\nparams = weights, bias\n\nfor i in range(20):\n    train_epoch(linear1, lr, params)\n    print(validate_epoch(linear1), end=' ')\n\n0.6765 0.8387 0.8988 0.9267 0.9374 0.9462 0.954 0.9565 0.9599 0.9618 0.9638 0.9653 0.9667 0.9682 0.9682 0.9706 0.9711 0.9721 0.9721 0.9721"
  },
  {
    "objectID": "posts/2021-08-06-mnist-threes-and-sevens.html#replace-gradient-update-with-a-home-grown-optimizer",
    "href": "posts/2021-08-06-mnist-threes-and-sevens.html#replace-gradient-update-with-a-home-grown-optimizer",
    "title": "MNIST: Distinguishing Threes from Sevens",
    "section": "Replace gradient update with a home grown optimizer",
    "text": "Replace gradient update with a home grown optimizer\n\nclass BasicOptim(nn.Module):\n    def __init__(self,params,lr): self.params, self.lr = list(params), lr\n        \n    def step(self,*args,**kwargs): \n        for p in self.params: p.data -= p.grad*self.lr\n            \n    def zero_grad(self,*args,**kwargs): \n        for p in self.params: p.grad = None\n\n\nlr = 1\n\nweights = init_params((28*28,1))\nbias = init_params(1)\nparams = weights, bias\n\nopt = BasicOptim(params, lr)\n\ndef train_epoch(model, lr, params):\n    for xb, yb in dl:\n        calc_grad(xb,yb,model)\n        opt.step()\n        opt.zero_grad()\n\nfor i in range(20):\n    train_epoch(linear1, lr, params)\n    print(validate_epoch(linear1), end=' ')\n\n0.6269 0.8379 0.9174 0.9409 0.9496 0.9545 0.9589 0.9623 0.9658 0.9672 0.9687 0.9697 0.9692 0.9697 0.9721 0.9721 0.9721 0.9726 0.9726 0.9731"
  },
  {
    "objectID": "posts/2021-08-06-mnist-threes-and-sevens.html#replace-init_params-and-linear1-with-pytorch-nn.linear",
    "href": "posts/2021-08-06-mnist-threes-and-sevens.html#replace-init_params-and-linear1-with-pytorch-nn.linear",
    "title": "MNIST: Distinguishing Threes from Sevens",
    "section": "Replace init_params and linear1 with Pytorch nn.Linear",
    "text": "Replace init_params and linear1 with Pytorch nn.Linear\nnn.Linear holds both the weights and bias and takes care of initializing the parameters.\n\nlinear_model = nn.Linear(28*28,1)\nweights, bias = linear_model.parameters()\nweights.shape, bias.shape\n\n(torch.Size([1, 784]), torch.Size([1]))\n\n\n\nlr = 1\n\nlinear_model = nn.Linear(28*28,1)\nopt = BasicOptim(linear_model.parameters(), lr)\n\ndef train_epoch(model, lr, params):\n    for xb, yb in dl:\n        calc_grad(xb,yb,model)\n        opt.step()\n        opt.zero_grad()\n\nfor i in range(20):\n    train_epoch(linear_model, lr, params)\n    print(validate_epoch(linear_model), end=' ')\n\n0.4932 0.4932 0.6816 0.8687 0.9185 0.936 0.9502 0.958 0.9638 0.9658 0.9678 0.9697 0.9712 0.9741 0.9746 0.9761 0.9765 0.9775 0.9785 0.9785"
  },
  {
    "objectID": "posts/2021-08-06-mnist-threes-and-sevens.html#replace-home-grown-optimizer-with-fastai-sgd",
    "href": "posts/2021-08-06-mnist-threes-and-sevens.html#replace-home-grown-optimizer-with-fastai-sgd",
    "title": "MNIST: Distinguishing Threes from Sevens",
    "section": "Replace home grown optimizer with fastai SGD",
    "text": "Replace home grown optimizer with fastai SGD\n\nlr = 1\n\nlinear_model = nn.Linear(28*28,1)\nopt = SGD(linear_model.parameters(), lr)\n\ndef train_epoch(model, lr, params):\n    for xb, yb in dl:\n        calc_grad(xb,yb,model)\n        opt.step()\n        opt.zero_grad()\n\nfor i in range(20):\n    train_epoch(linear_model, lr, params)\n    print(validate_epoch(linear_model), end=' ')\n\n0.4932 0.8447 0.8398 0.9126 0.9336 0.9478 0.9551 0.9629 0.9658 0.9678 0.9692 0.9717 0.9741 0.9751 0.9761 0.9761 0.977 0.978 0.9785 0.9785"
  },
  {
    "objectID": "posts/2021-08-06-mnist-threes-and-sevens.html#replace-training-loop-with-fastai-learner.fit",
    "href": "posts/2021-08-06-mnist-threes-and-sevens.html#replace-training-loop-with-fastai-learner.fit",
    "title": "MNIST: Distinguishing Threes from Sevens",
    "section": "Replace training loop with Fastai Learner.fit",
    "text": "Replace training loop with Fastai Learner.fit\nObserve how we are able to pass in the mnist_loss and batch_accuracy functions. Recall that within these functions we pass the predictions through the sigmoid function.\n\n#collapse-output\ndls = DataLoaders(dl, valid_dl)\nlearn = Learner(dls, nn.Linear(28*28,1), loss_func=mnist_loss\n                , opt_func=SGD\n                , metrics = batch_accuracy)\nlearn.fit(20,lr=1)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      batch_accuracy\n      time\n    \n  \n  \n    \n      0\n      0.637023\n      0.503487\n      0.495584\n      00:00\n    \n    \n      1\n      0.524831\n      0.192524\n      0.839058\n      00:00\n    \n    \n      2\n      0.192777\n      0.178215\n      0.840039\n      00:00\n    \n    \n      3\n      0.084406\n      0.105942\n      0.912169\n      00:00\n    \n    \n      4\n      0.044523\n      0.077318\n      0.933268\n      00:00\n    \n    \n      5\n      0.028958\n      0.061924\n      0.947988\n      00:00\n    \n    \n      6\n      0.022560\n      0.052394\n      0.955839\n      00:00\n    \n    \n      7\n      0.019714\n      0.046067\n      0.962709\n      00:00\n    \n    \n      8\n      0.018272\n      0.041615\n      0.966143\n      00:00\n    \n    \n      9\n      0.017409\n      0.038327\n      0.967125\n      00:00\n    \n    \n      10\n      0.016804\n      0.035799\n      0.969578\n      00:00\n    \n    \n      11\n      0.016330\n      0.033789\n      0.972031\n      00:00\n    \n    \n      12\n      0.015934\n      0.032144\n      0.973503\n      00:00\n    \n    \n      13\n      0.015597\n      0.030771\n      0.974975\n      00:00\n    \n    \n      14\n      0.015305\n      0.029609\n      0.975957\n      00:00\n    \n    \n      15\n      0.015053\n      0.028614\n      0.976938\n      00:00\n    \n    \n      16\n      0.014832\n      0.027755\n      0.977429\n      00:00\n    \n    \n      17\n      0.014637\n      0.027009\n      0.977920\n      00:00\n    \n    \n      18\n      0.014463\n      0.026354\n      0.978410\n      00:00\n    \n    \n      19\n      0.014305\n      0.025776\n      0.978901\n      00:00"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Simple Neural Net Backward Pass\n\n\n\n\n\n\n\nneural-nets\n\n\n\n\nDeriving the math of the backward pass for a simple neural net.\n\n\n\n\n\n\nNov 13, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercises with Convex Sets\n\n\n\n\n\n\n\nnotes\n\n\n\n\nWorking through a few practice problems related to Convexity.\n\n\n\n\n\n\nMay 27, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLosses\n\n\n\n\n\n\n\n\n\n\n\n\nMay 26, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLambert W\n\n\n\n\n\nWhat value of \\(x\\) satisfies \\(x^x=2\\).\n\n\n\n\n\n\nMay 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMcElreath Chapter 4\n\n\n\n\n\n\n\nstatistical-rethinking\n\n\nbayesian\n\n\n\n\nWorking through a few practice problems on Linear Regression.\n\n\n\n\n\n\nApr 10, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMcElreath Chapter 3\n\n\n\n\n\n\n\nstatistical-rethinking\n\n\nbayesian\n\n\n\n\n\n\n\n\n\n\n\nMar 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMcElreath Chapter 2\n\n\n\n\n\n\n\nstatistical-rethinking\n\n\nbayesian\n\n\n\n\n\n\n\n\n\n\n\nMar 11, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChvátal Chapter 2 Simplex Implementation\n\n\n\n\n\n\n\nmath-program\n\n\njupyter\n\n\n\n\nA toy implementation of the Simplex algorithm for Linear Programming.\n\n\n\n\n\n\nDec 11, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChvátal Chapter 1 Problem 5\n\n\n\n\n\n\n\nmath-program\n\n\njupyter\n\n\n\n\nWork through a simple Linear Programming problem exercise.\n\n\n\n\n\n\nDec 8, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImage Embeddings with fastai\n\n\n\n\n\nExploring image embeddings with fastai\n\n\n\n\n\n\nOct 25, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReproducible runs with fastai\n\n\n\n\n\n\n\nfastai\n\n\n\n\nHow to replicate results from Fastai models.\n\n\n\n\n\n\nOct 23, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuadratic Curve Fit with Gradient Descent\n\n\n\n\n\n\n\nfastai\n\n\njupyter\n\n\n\n\nExample of using gradient descent to fit data.\n\n\n\n\n\n\nAug 27, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCollaborative Filtering With Cross Entropy Loss\n\n\n\n\n\n\n\nfastai\n\n\njupyter\n\n\n\n\nUsing Fastai for Collaborative Filtering on the Movie Lens dataset using cross-entropy loss.\n\n\n\n\n\n\nAug 14, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMulti-Label Classification\n\n\n\n\n\n\n\nfastai\n\n\njupyter\n\n\n\n\nUsing Fastai for Multi-Label classification.\n\n\n\n\n\n\nAug 12, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA note on Cross Entropy loss with PyTorch\n\n\n\n\n\n\n\nfastai\n\n\npytorch\n\n\njupyter\n\n\n\n\nWorking through the Cross Entropy Loss function and it’s gradient.\n\n\n\n\n\n\nAug 7, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMNIST: Distinguishing Threes from Sevens\n\n\n\n\n\n\n\nfastai\n\n\njupyter\n\n\n\n\nUsing Fastai to classify threes and sevens from the MNIST dataset.\n\n\n\n\n\n\nAug 6, 2021\n\n\n\n\n\n\n  \n\n\n\n\nBear Classification and Gradio\n\n\n\n\n\n\n\nfastai\n\n\ngradio\n\n\njupyter\n\n\n\n\nUsing Fastai to classify bears and demo-ing the result with gradio.\n\n\n\n\n\n\nJul 31, 2021\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Notes on things I find interesting."
  }
]