{
 "cells": [
  {
   "cell_type": "raw",
   "id": "aaada3b0",
   "metadata": {},
   "source": [
    "---\n",
    "aliases:\n",
    "- /fastai/jupyter/pytorch/2021/08/07/Cross-Entropy-Loss-PyTorch\n",
    "badges: true\n",
    "categories:\n",
    "- fastai\n",
    "- pytorch\n",
    "- jupyter\n",
    "date: '2021-08-07'\n",
    "description: Working through the Cross Entropy Loss function and it's gradient.\n",
    "output-file: 2021-08-07-cross-entropy-loss-pytorch.html\n",
    "title: A note on Cross Entropy loss with PyTorch\n",
    "bibliography: ../references.bib\n",
    "csl: ../control-and-automation.csl\n",
    "toc: true\n",
    "use_math: true\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "every-ukraine",
   "metadata": {},
   "source": [
    "# Maximizing the likelihood of the labels of the data\n",
    "\n",
    "Suppose we have $N$ training examples and we have a multi-class problem such that each training example belongs to one and only one out of $K$ possible classes. Let $C(i) \\in \\{1,\\ldots, K\\}$ be the correct class for the $i$-th training example and $o^{[C(i)]}_{i}$ is the probability assigned by a classifier to the correct class for the $i$-th training example. We want this classifier to maximize: $$\\prod_{i=1}^{N} o^{[C(i)]}_{i}$$ \n",
    "\n",
    "If the classifier assigns a probability of $1$ to the correct class for $N-1$ training examples and a probability of $0$ for the $N$-th example then the entire product shown above becomes zero. So to maximize this product of probabilities, the classifier has to assign a high probability to the correct class for each and every training example. \n",
    "\n",
    "Now, maximizing the product is equivalent to maximizing $$ln(\\prod_{i=1}^{N}o^{[C(i)]}_{i}) = \\sum_{i=1}^{N}ln(o^{[C(i)]}_{i})$$ \n",
    "\n",
    "This is the same as minimizing the sum of the negative log likelihoods $$-\\sum_{i=1}^{N}ln(o^{[C(i)]}_{i})$$\n",
    "\n",
    "The above can now serve as a loss function for an optimization routine.\n",
    "\n",
    "Recall that Cross Entropy = $-\\sum_{k=1}^{K}y^{[k]}ln(o^{[k]})$ where $y$ is the reference distribution over $K$ classes while our predictions over the $K$ classes is given by $o$. Observe that this summation will collapse to being a single term when, in the reference distribution $y$, only one of the classes has a probability of $1$.\n",
    "\n",
    "Thus $-\\sum_{i=1}^{N}ln(o^{[C(i)]}_{i})$ can be interpreted as the sum of cross entropy losses across all examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2Fdpuiszdc0r",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/fastai/fastai\n",
      "  Cloning https://github.com/fastai/fastai to /tmp/pip-req-build-0kh0zo5n\n",
      "  Running command git clone -q https://github.com/fastai/fastai /tmp/pip-req-build-0kh0zo5n\n",
      "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (from fastai==2.5.4) (21.1.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from fastai==2.5.4) (21.0)\n",
      "Collecting fastdownload<2,>=0.0.5\n",
      "  Downloading fastdownload-0.0.5-py3-none-any.whl (13 kB)\n",
      "Collecting fastcore<1.4,>=1.3.22\n",
      "  Downloading fastcore-1.3.26-py3-none-any.whl (56 kB)\n",
      "\u001b[K     |████████████████████████████████| 56 kB 2.0 MB/s \n",
      "\u001b[?25hRequirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from fastai==2.5.4) (0.10.0+cu111)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from fastai==2.5.4) (3.2.2)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from fastai==2.5.4) (1.1.5)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fastai==2.5.4) (2.23.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from fastai==2.5.4) (3.13)\n",
      "Requirement already satisfied: fastprogress>=0.2.4 in /usr/local/lib/python3.7/dist-packages (from fastai==2.5.4) (1.0.0)\n",
      "Requirement already satisfied: pillow>6.0.0 in /usr/local/lib/python3.7/dist-packages (from fastai==2.5.4) (7.1.2)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from fastai==2.5.4) (0.22.2.post1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from fastai==2.5.4) (1.4.1)\n",
      "Requirement already satisfied: spacy<4 in /usr/local/lib/python3.7/dist-packages (from fastai==2.5.4) (2.2.4)\n",
      "Requirement already satisfied: torch<1.11,>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from fastai==2.5.4) (1.9.0+cu111)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fastprogress>=0.2.4->fastai==2.5.4) (1.19.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai==2.5.4) (1.0.5)\n",
      "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai==2.5.4) (7.4.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai==2.5.4) (2.0.5)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai==2.5.4) (1.0.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai==2.5.4) (1.1.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai==2.5.4) (0.8.2)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai==2.5.4) (1.0.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai==2.5.4) (57.4.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai==2.5.4) (0.4.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai==2.5.4) (4.62.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai==2.5.4) (3.0.5)\n",
      "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<4->fastai==2.5.4) (4.8.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<4->fastai==2.5.4) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<4->fastai==2.5.4) (3.7.4.3)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fastai==2.5.4) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fastai==2.5.4) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fastai==2.5.4) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fastai==2.5.4) (2021.5.30)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai==2.5.4) (1.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai==2.5.4) (2.8.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai==2.5.4) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai==2.5.4) (0.10.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib->fastai==2.5.4) (1.15.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->fastai==2.5.4) (2018.9)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->fastai==2.5.4) (1.0.1)\n",
      "Building wheels for collected packages: fastai\n",
      "  Building wheel for fastai (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for fastai: filename=fastai-2.5.4-py3-none-any.whl size=186971 sha256=afddb62cb20534748cc2dd18752e02fb05a4ff7f8ed122600a78fa36a0748a15\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-l1rcznw1/wheels/40/be/4f/b7f2aec4df5712626ceed9f20a8996eb05e31244e57e58d632\n",
      "Successfully built fastai\n",
      "Installing collected packages: fastcore, fastdownload, fastai\n",
      "  Attempting uninstall: fastai\n",
      "    Found existing installation: fastai 1.0.61\n",
      "    Uninstalling fastai-1.0.61:\n",
      "      Successfully uninstalled fastai-1.0.61\n",
      "Successfully installed fastai-2.5.4 fastcore-1.3.26 fastdownload-0.0.5\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "fastai"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/fastai/fastcore\n",
      "  Cloning https://github.com/fastai/fastcore to /tmp/pip-req-build-gg_rdhd7\n",
      "  Running command git clone -q https://github.com/fastai/fastcore /tmp/pip-req-build-gg_rdhd7\n",
      "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (from fastcore==1.3.27) (21.1.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from fastcore==1.3.27) (21.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->fastcore==1.3.27) (2.4.7)\n",
      "Building wheels for collected packages: fastcore\n",
      "  Building wheel for fastcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for fastcore: filename=fastcore-1.3.27-py3-none-any.whl size=55581 sha256=84b7613e8f4899819debcf920a9381e2a31f6ee8fcc4fd0746ca5c6b356168d3\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-eam2z7qi/wheels/74/46/b7/0d3dddeb22ca1b6f226a3d5b096df11a632951327002d86f1f\n",
      "Successfully built fastcore\n",
      "Installing collected packages: fastcore\n",
      "  Attempting uninstall: fastcore\n",
      "    Found existing installation: fastcore 1.3.26\n",
      "    Uninstalling fastcore-1.3.26:\n",
      "      Successfully uninstalled fastcore-1.3.26\n",
      "Successfully installed fastcore-1.3.27\n"
     ]
    }
   ],
   "source": [
    "#| include: false\n",
    "!pip install git+https://github.com/fastai/fastai\n",
    "!pip install git+https://github.com/fastai/fastcore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intense-gateway",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continental-timothy",
   "metadata": {},
   "source": [
    "Pretend the following are the activations of each class of a multiclass classification problem. So we have 6 examples and in each row we have the activation for each class the example could belong to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brief-parking",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.6453,  1.8893],\n",
       "        [ 1.9800,  1.7681],\n",
       "        [ 2.8183,  4.6643],\n",
       "        [-0.3635, -0.0614],\n",
       "        [ 0.4064, -0.4668],\n",
       "        [-3.3801,  3.2484]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activations = torch.randn((6,2))*2\n",
    "activations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clear-current",
   "metadata": {},
   "source": [
    "Suppose the correct class of each example is as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polar-cannon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 0, 1, 1, 0])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = tensor([0,1,0,1,1,0])\n",
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "happy-version",
   "metadata": {},
   "source": [
    "Take the softmax of the activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bibliographic-clerk",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0283, 0.9717],\n",
       "        [0.5528, 0.4472],\n",
       "        [0.1363, 0.8637],\n",
       "        [0.4250, 0.5750],\n",
       "        [0.7054, 0.2946],\n",
       "        [0.0013, 0.9987]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_acts = torch.softmax(activations, dim=1)\n",
    "sm_acts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deluxe-transformation",
   "metadata": {},
   "source": [
    "Extract the probabilities predicted for the correct class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clean-interface",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = range(6)\n",
    "list(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technological-diary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0283, 0.4472, 0.1363, 0.5750, 0.2946, 0.0013])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_correct_class = sm_acts[idx, targets]\n",
    "p_correct_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sweet-announcement",
   "metadata": {},
   "source": [
    "Take the log of the softmax activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "voluntary-companion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.5634e+00, -2.8753e-02],\n",
       "        [-5.9281e-01, -8.0469e-01],\n",
       "        [-1.9925e+00, -1.4659e-01],\n",
       "        [-8.5559e-01, -5.5344e-01],\n",
       "        [-3.4895e-01, -1.2222e+00],\n",
       "        [-6.6298e+00, -1.3213e-03]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.log(sm_acts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twenty-kennedy",
   "metadata": {},
   "source": [
    "Computing the softmax of the activations and then taking the log is equivalent to applying PyTorch's log_softmax function directly to the original activations. We want to do the latter because it will faster and more accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civilian-dodge",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.5634e+00, -2.8753e-02],\n",
       "        [-5.9281e-01, -8.0469e-01],\n",
       "        [-1.9925e+00, -1.4659e-01],\n",
       "        [-8.5559e-01, -5.5344e-01],\n",
       "        [-3.4895e-01, -1.2222e+00],\n",
       "        [-6.6298e+00, -1.3213e-03]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.log_softmax(activations, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parallel-cosmetic",
   "metadata": {},
   "source": [
    "Let's compute the mean of cross entropy losses across the training examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "antique-gamma",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3.5634, 0.8047, 1.9925, 0.5534, 1.2222, 6.6298]), tensor(2.4610))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-1*torch.log(p_correct_class), (-1*torch.log(p_correct_class)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precise-method",
   "metadata": {},
   "source": [
    "We can just use Pytorch to compute this directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunset-kansas",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3.5634, 0.8047, 1.9925, 0.5534, 1.2222, 6.6298]), tensor(2.4610))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.CrossEntropyLoss(reduction='none')(activations, targets), nn.CrossEntropyLoss()(activations, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "administrative-seeking",
   "metadata": {},
   "source": [
    "or by using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diagnostic-trademark",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3.5634, 0.8047, 1.9925, 0.5534, 1.2222, 6.6298]), tensor(2.4610))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(activations, targets, reduction='none'), F.cross_entropy(activations, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functioning-estonia",
   "metadata": {},
   "source": [
    "# Gradient of Cross Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "realistic-bubble",
   "metadata": {},
   "source": [
    "We follow the exposition in @markusthill_ce_note."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quick-huntington",
   "metadata": {},
   "source": [
    "Let $z^{[1]},\\ldots, z^{[K]}$ denote the activations corresponding to the $K$ classes. The softmax activation for each class is given by:\n",
    "\n",
    "$$o^{[j]} = \\frac{e^{z^{[j]}}}{\\sum_{l=1}^{K} e^{z^{[l]}}}$$\n",
    "\n",
    "The cross-entropy loss across the $K$ classes is given by: \n",
    "\n",
    "$$E=-\\sum_{l=1}^{K}y^{[l]}ln(o^{[l]})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tough-particle",
   "metadata": {},
   "source": [
    "## Partial derivative of $o^{[j]}$ with respect to $z^{[i]}$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial z^{[i]}} o^{[j]}  = \\frac{\\partial}{\\partial z^{[i]}} \\frac{e^{z^{[j]}}}{\\sum_l e^{z^{[l]}}}\n",
    "= e^{z^{[j]}} \\frac{\\partial}{\\partial z^{[i]}} \\Bigg(\\sum_l e^{z^{[l]}} \\Bigg)^{-1} \\\\\n",
    "\\qquad = -e^{z^{[j]}} \\Bigg(\\sum_l e^{z^{[l]}} \\Bigg)^{-2} e^{z^{[i]}}\n",
    "= -o^{[j]} \\cdot o^{[i]}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "western-table",
   "metadata": {},
   "source": [
    "## Partial derivative of $o^{[i]}$ with respect to $z^{[i]}$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial z^{[i]}} o^{[i]} \n",
    "= \\frac{\\partial}{\\partial z^{[i]}} \\frac{e^{z^{[i]}}}{\\sum_l e^{z^{[l]}}} \n",
    "= \\frac{e^{z^{[i]}}}{\\sum_{l} e^{z^{[l]}}} + e^{z^{[i]}} \\frac{\\partial}{\\partial z^{[i]}} \\Bigg(\\sum_l e^{z^{[l]}} \\Bigg)^{-1}\\\\\n",
    "\\quad \\qquad \\qquad \\qquad = o^{[i]}-e^{z^{[i]}} \\Bigg(\\sum_l e^{z^{[l]}} \\Bigg)^{-2} e^{z^{[i]}}\n",
    "= o^{[i]} - o^{[i]} \\cdot o^{[i]} \n",
    "= o^{[i]} \\cdot (1 - o^{[i]})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sonic-southwest",
   "metadata": {},
   "source": [
    "Let's compute the gradient of the cross-entropy loss with respect to the activation of the $i$-the class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accurate-strip",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\x08egin{eqnarray*}\\n\\x0crac{\\\\partial E}{\\\\partial z^{[i]}} &=&- \\\\sum_{l=1}^{K} y^{[l]}\\\\cdot  \\x0crac{\\\\partial}{\\\\partial z^{[i]}}ln(o^{[l]})\\n=- \\\\sum_{j \\neq i} y^{[j]}\\\\cdot  \\x0crac{\\\\partial}{\\\\partial z^{[i]}}ln(o^{[j]}) - y^{[i]}\\\\cdot  \\x0crac{\\\\partial}{\\\\partial z^{[i]}}ln(o^{[i]})\\\\\\n&=&- \\\\sum_{j \\neq i} y^{[j]}\\\\cdot \\x0crac{1}{o^{[j]}} \\\\cdot \\x0crac{\\\\partial}{\\\\partial z^{[i]}}o^{[j]} - y^{[i]}\\\\cdot \\x0crac{1}{o^{[i]}} \\\\cdot \\x0crac{\\\\partial}{\\\\partial z^{[i]}}o^{[i]} \\\\\\n&=& - \\\\sum_{j \\neq i} y^{[j]}\\\\cdot \\x0crac{1}{o^{[j]}} \\\\cdot (-o^{[j]} \\\\cdot o^{[i]}) - y^{[i]}\\\\cdot \\x0crac{1}{o^{[i]}} \\\\cdot o^{[i]} \\\\cdot (1 - o^{[i]}) \\\\\\n&=& - \\\\sum_{j \\neq i} y^{[j]} (-o^{[i]}) - y^{[i]} (1 - o^{[i]}) \\\\\\n&=& - \\\\sum_{j \\neq i} y^{[j]} (-o^{[i]}) + y^{[i]} o^{[i]} - y^{[i]} \\\\\\n&=& o^{[i]} \\\\sum_{j} y^{[j]}  - y^{[i]} \\\\\\n&=& o^{[i]} - y^{[i]}\\n\\\\end{eqnarray*}\\n'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| include: false\n",
    "# The following renders fine as a markdown cell in Jupyter but not when said notebook is rendered by fastpages\n",
    "# So I used https://latexeditor.lagrida.com to render the equation take a screenshot and then put it in the page :(\n",
    "'''\n",
    "\\begin{eqnarray*}\n",
    "\\frac{\\partial E}{\\partial z^{[i]}} &=&- \\sum_{l=1}^{K} y^{[l]}\\cdot  \\frac{\\partial}{\\partial z^{[i]}}ln(o^{[l]})\n",
    "=- \\sum_{j \\neq i} y^{[j]}\\cdot  \\frac{\\partial}{\\partial z^{[i]}}ln(o^{[j]}) - y^{[i]}\\cdot  \\frac{\\partial}{\\partial z^{[i]}}ln(o^{[i]})\\\\\n",
    "&=&- \\sum_{j \\neq i} y^{[j]}\\cdot \\frac{1}{o^{[j]}} \\cdot \\frac{\\partial}{\\partial z^{[i]}}o^{[j]} - y^{[i]}\\cdot \\frac{1}{o^{[i]}} \\cdot \\frac{\\partial}{\\partial z^{[i]}}o^{[i]} \\\\\n",
    "&=& - \\sum_{j \\neq i} y^{[j]}\\cdot \\frac{1}{o^{[j]}} \\cdot (-o^{[j]} \\cdot o^{[i]}) - y^{[i]}\\cdot \\frac{1}{o^{[i]}} \\cdot o^{[i]} \\cdot (1 - o^{[i]}) \\\\\n",
    "&=& - \\sum_{j \\neq i} y^{[j]} (-o^{[i]}) - y^{[i]} (1 - o^{[i]}) \\\\\n",
    "&=& - \\sum_{j \\neq i} y^{[j]} (-o^{[i]}) + y^{[i]} o^{[i]} - y^{[i]} \\\\\n",
    "&=& o^{[i]} \\sum_{j} y^{[j]}  - y^{[i]} \\\\\n",
    "&=& o^{[i]} - y^{[i]}\n",
    "\\end{eqnarray*}\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satellite-morrison",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"https://github.com/nasheqlbrm/blog/blob/main/images/ce_derivative.png?raw=1\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eight-costume",
   "metadata": {},
   "source": [
    "Per the _Sylvain says_ section (page 203 Chapter 5) of @fastbook2020, \" _The gradient is proportional to the difference between the prediction and the target._... _Because the gradient is linear we won't see sudden jumps or exponential increases in gradients, which should lead to smoother training of models._\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "popular-alpha",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
