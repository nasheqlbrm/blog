{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7758082a-3e3f-4cac-81a9-f3ec86c05693",
   "metadata": {},
   "source": [
    "---\n",
    "categories: [neural-nets]\n",
    "date: '2022-11-13'\n",
    "description: Deriving the math of the backward pass for a simple neural net.\n",
    "output-file: 2021-11-13-backward-pass.html\n",
    "title: Simple Neural Net Backward Pass\n",
    "bibliography: ../references.bib\n",
    "toc: true\n",
    "use_math: true\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c70249-c04f-4474-a525-bea89d21fa9d",
   "metadata": {},
   "source": [
    "# Motivation\n",
    "In this post we attempt to connect the math to the code for what Jeremy Howard did in his `03_backprop.ipynb` notebook for the [2022 part 2 course](https://www.fast.ai/posts/part2-2022.html).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4309488-eeda-40ad-add3-c1552fd3e01e",
   "metadata": {},
   "source": [
    "# Notation\n",
    "* $N$ : number of training examples\n",
    "* $d$ : number of features\n",
    "* $\\mathbf{x}^{(i)}$ is the $i$-th training example and can be represented as a column vector $$\n",
    "\\mathbf{x}^{(i)} = \\begin{bmatrix}\n",
    "x^{(i)}_1 \\\\\n",
    "x^{(i)}_2 \\\\\n",
    "\\vdots \\\\\n",
    "x^{(i)}_d\n",
    "\\end{bmatrix}\n",
    "$$ \n",
    "* $\\mathbf{w}$ is the vector of weights for a single neuron $$\n",
    "\\mathbf{w} = \\begin{bmatrix}\n",
    "w_1 \\\\\n",
    "w_2 \\\\\n",
    "\\vdots \\\\\n",
    "w_d\n",
    "\\end{bmatrix}\n",
    "$$ \n",
    "* $b$ is the bias term for a single neuron\n",
    "* $\\mathbf{X}$ is an $N \\times d$ matrix with the training examples stacked in rows\n",
    "$$\n",
    "\\mathbf{X} = \\begin{bmatrix}\n",
    "x^{(1)}_1 & x^{(1)}_2 & \\cdots & x^{(1)}_d\\\\\n",
    "x^{(2)}_1 & x^{(2)}_2 & \\cdots & x^{(2)}_d\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "x^{(N)}_1 & x^{(N)}_2 & \\cdots & x^{(N)}_d\n",
    "\\end{bmatrix}\n",
    "$$ \n",
    "* $\\{y^{(i)}\\}_{i=1}^{N}$ are the targets for each of the $N$ training examples\n",
    "* $z^{(i)}$ is the output of our single neuron when the $i$-th training example is passed through it\n",
    "    * Specifically, $z^{(i)} = \\mathbf{w}^T\\mathbf{x}^{(i)} + b = b + \\sum_{j=1}^{d}w_{j}x^{(i)}_{j} = b + w_{1}x^{(i)}_{1} + \\ldots + w_{j}x^{(i)}_{j} + \\ldots + w_{d}x^{(i)}_{d}$. Additionally we see that,\n",
    "        * $\\frac{\\partial z^{(i)}}{\\partial w_{j}} = x^{(i)}_{j}$\n",
    "        * $\\frac{\\partial z^{(i)}}{\\partial b} = 1$\n",
    "        * $\\frac{\\partial z^{(i)}}{\\partial x^{(i)}_{j}} = w_{j}$\n",
    "* $a^{(i)} = \\phi(z^{(i)})$ is the activation when an input $z^{(i)}$ is passed through an activation function $\\phi$\n",
    "    * $\\{a^{(i)}\\}_{i=1}^{N}$ are the activations for each of the $N$ training examples when passed through a single neuron followed by the application of the activation function\n",
    "    * For the purposes of this page the activation function is considered to be a ReLU so $a^{(i)} = \\phi(z^{(i)}) = \\max\\{0, z^{(i)}\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04684a67-3503-4867-bdd7-82bb71188241",
   "metadata": {},
   "source": [
    "# Gradients \n",
    "\n",
    "::: {.callout-note}\n",
    "Reading through @eriklearnedmiller_note (especially page 7) was very helpful for this section. Another useful reference is Terence Parr and Jeremy Howard's [Matrix Calculus You Need For Deep Learning](https://explained.ai/matrix-calculus/).\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d460990-e8b0-4b0f-9ed0-0fbd418b31d6",
   "metadata": {},
   "source": [
    "## Gradient of loss with respect to the Activations $a^{(i)}$ \n",
    "\n",
    "$J\\left(\\{y^{(i)}\\}_{i=1}^{N},\\{a^{(i)}\\}_{i=1}^{N}\\right) = \\frac{1}{N}\\sum_{i=1}^{N}(y^{(i)}-a^{(i)})^{2}$ is the loss (mean squared error) across the $N$ training examples\n",
    "\n",
    "The `backward` function of the `Mse` class computes an estimate of how the loss function changes as the input activations change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bb62e7-d107-4d15-907e-d9559e183114",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mse():\n",
    "    def __call__(self, inp, targ):\n",
    "        self.inp,self.targ = inp,targ\n",
    "        self.out = mse(inp, targ)\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self):\n",
    "        N = self.targ.shape[0]\n",
    "        A = self.inp\n",
    "        Y = self.targ\n",
    "        dJ_dA = (2./N) * (A.squeeze() - Y).unsqueeze(-1)\n",
    "        self.inp.g = dJ_dA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6367305-22ff-4dff-b199-a03e3c156192",
   "metadata": {},
   "source": [
    "The change in the loss as the $i$-th activation changes is given by\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial a^{(i)}} = \\frac{2}{N}\\sum_{i=1}^{N} \\frac{\\partial (y^{(i)}-a^{(i)})^{2}}{\\partial a^{(i)}} = \\frac{2}{N}(y^{(i)}-a^{(i)})\\frac{\\partial (y^{(i)}-a^{(i)}) }{\\partial a^{(i)}} = \\frac{2}{N}(a^{(i)}-y^{(i)})$$\n",
    "\n",
    "where the last step follows because $\\frac{\\partial (y^{(i)}-a^{(i)}) }{\\partial a^{(i)}} = 0-1 =-1$.\n",
    "\n",
    "The change in the loss as a function of the change in activations from our training examples is captured by the $N \\times 1$ matrix:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\mathbf{a}} = \\begin{bmatrix}\n",
    "\\frac{\\partial J}{\\partial a^{(1)}} \\\\\n",
    "\\frac{\\partial J}{\\partial a^{(2)}} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial J}{\\partial a^{(N)}}\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "\\frac{2}{N}\\left(a^{(1)}-y^{(1)}\\right) \\\\\n",
    "\\frac{2}{N}\\left(a^{(2)}-y^{(2)}\\right) \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{2}{N}\\left(a^{(N)}-y^{(N)}\\right)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "From the implementation perspective, the activations $\\{a^{(i)}\\}_{i=1}^{N}$ and the targets $\\{y^{(i)}\\}_{i=1}^{N}$ are passed in and stored during the forward pass (specifically in the dunder `__call__` method). In the backward pass these are retrieved and $\\frac{\\partial J}{\\partial \\mathbf{a}}$ is computed and stored for access by the `backward` function of the prior layer. Hopefully the `backward` method for the `Mse` class makes sense now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c322c39-1e14-485c-8b52-d015405af0bf",
   "metadata": {},
   "source": [
    "## Gradient of loss with respect to the Linear Output $z^{(i)}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6863192c-2c41-4180-bcfa-c9895353cf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu():\n",
    "    def __call__(self, inp):\n",
    "        self.inp = inp\n",
    "        self.out = inp.clamp_min(0.)\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self): \n",
    "        dJ_dA = self.out.g\n",
    "        dA_dZ = (self.inp>0).float() \n",
    "        \n",
    "        # Note this is an elementwise multiplication         \n",
    "        dJ_dZ = dJ_dA * dA_dZ\n",
    "        \n",
    "        self.inp.g = dJ_dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c10a98-de13-45bd-8b1c-1e3c93eddeed",
   "metadata": {},
   "source": [
    "How does the loss change as the output of the linear unit changes?\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial z^{(i)}} = \\frac{\\partial J}{\\partial a^{(i)}} \\frac{\\partial a^{(i)}}{\\partial z^{(i)}} = \\frac{2}{N}\\left(a^{(i)}-y^{(i)}\\right)\\frac{\\partial a^{(i)}}{\\partial z^{(i)}}$$\n",
    "\n",
    "For the ReLU activation function we have that,\n",
    "$$\n",
    "\\frac{\\partial a^{(i)}}{\\partial z^{(i)}} = \n",
    "\\begin{cases}\n",
    "  1 & z^{(i)} \\gt 0 \\\\\n",
    "  0 & z^{(i)} \\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "and hence\n",
    "\n",
    "$$\\frac{\\partial \\mathbf{a}}{\\partial \\mathbf{z}} = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial a^{(1)}}{\\partial z^{(1)}} \\\\\n",
    "\\frac{\\partial a^{(2)}}{\\partial z^{(2)}} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial a^{(N)}}{\\partial z^{(N)}}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Thus the change in the loss as a function of the change in the output from the linear unit on our training examples is given by the $N \\times 1$ matrix:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\mathbf{z}} = \\begin{bmatrix}\n",
    "\\frac{\\partial J}{\\partial z^{(1)}} \\\\\n",
    "\\frac{\\partial J}{\\partial z^{(2)}} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial J}{\\partial z^{(N)}}\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "\\frac{\\partial J}{\\partial a^{(1)}}\\frac{\\partial a^{(1)}}{\\partial z^{(1)}} \\\\\n",
    "\\frac{\\partial J}{\\partial a^{(2)}}\\frac{\\partial a^{(2)}}{\\partial z^{(2)}} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial J}{\\partial a^{(N)}}\\frac{\\partial a^{(N)}}{\\partial z^{(N)}}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "So $\\frac{\\partial J}{\\partial \\mathbf{z}}$ ends up being an elementwise product between the corresponding entries of $\\frac{\\partial J}{\\partial \\mathbf{a}}$ and $\\frac{\\partial \\mathbf{a}}{\\partial \\mathbf{z}}$. \n",
    "\n",
    "From the implementation perspective, in the backward pass $\\frac{\\partial \\mathbf{a}}{\\partial \\mathbf{z}}$ will be computed locally and multiplied elementwise with $\\frac{\\partial J}{\\partial \\mathbf{a}}$ (this will have been computed in the backward pass in the `Mse` class and will be available to access when the backward function of the `Relu` function is called). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1322209-3bd2-4f1c-be99-9787566cb5a0",
   "metadata": {},
   "source": [
    "## Gradient of loss with respect to $w_{j}, b$ and $X$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e25ff3-4a8f-43f2-9ba9-0e86f8e6c85b",
   "metadata": {},
   "source": [
    "The next three subsections will explain the `backward` function of the `Lin` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a872e37-5f5a-436e-b3cb-26e767610620",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lin():\n",
    "    def __init__(self, w, b): self.w,self.b = w,b\n",
    "\n",
    "    def __call__(self, inp):\n",
    "        self.inp = inp\n",
    "        self.out = lin(inp, self.w, self.b)\n",
    "        return self.out\n",
    "\n",
    "    def backward(self):\n",
    "        # See Gradient of loss with respect to w_j         \n",
    "        dJ_dZ = self.out.g\n",
    "        X = self.inp\n",
    "        dJ_dW = X.t() @ dJ_dZ\n",
    "        self.w.g = dJ_dW\n",
    "        \n",
    "        # See Gradient of loss with respect to the bias b\n",
    "        dJ_db = dJ_dZ.sum(0)\n",
    "        self.b.g = dJ_db\n",
    "        \n",
    "        # See Gradient of loss with respect to X\n",
    "        dJ_dX = dJ_dZ @ self.w.t()\n",
    "        self.inp.g = dJ_dX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae64d144-a583-4044-8e13-9ce876504385",
   "metadata": {},
   "source": [
    "### Gradient of loss with respect to $w_{j}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bb6942-c9f3-4f67-a25b-bc079bd51d80",
   "metadata": {},
   "source": [
    "How does the loss react when we wiggle $w_{j}$?\n",
    "$$\\frac{\\partial J}{\\partial w_{j}} = \\frac{\\partial J}{\\partial \\mathbf{z}} \\frac{\\partial \\mathbf{z}}{\\partial w_{j}}= \\sum_{i=1}^{N}\\frac{\\partial J}{\\partial z^{(i)}} \\frac{\\partial z^{(i)}}{\\partial w_{j}} = \\frac{\\partial J}{\\partial z^{(1)}}x^{(1)}_{j} + \\frac{\\partial J}{\\partial z^{(2)}}x^{(2)}_{j} + \\ldots + \\frac{\\partial J}{\\partial z^{(N)}}x^{(N)}_{j}$$\n",
    "\n",
    "Thus,\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\mathbf{w}} = \\begin{bmatrix}\n",
    "\\frac{\\partial J}{\\partial w_{1}} \\\\\n",
    "\\frac{\\partial J}{\\partial w_{2}} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial J}{\\partial w_{d}}\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "x^{(1)}_{1}\\frac{\\partial J}{\\partial z^{(1)}} + x^{(2)}_{1}\\frac{\\partial J}{\\partial z^{(2)}} + \\ldots + x^{(N)}_{1}\\frac{\\partial J}{\\partial z^{(N)}} \\\\\n",
    "x^{(1)}_{2}\\frac{\\partial J}{\\partial z^{(1)}} + x^{(2)}_{2}\\frac{\\partial J}{\\partial z^{(2)}} + \\ldots + x^{(N)}_{2}\\frac{\\partial J}{\\partial z^{(N)}} \\\\\n",
    "\\vdots \\\\\n",
    "x^{(1)}_{d}\\frac{\\partial J}{\\partial z^{(1)}} + x^{(2)}_{d}\\frac{\\partial J}{\\partial z^{(2)}} + \\ldots + x^{(N)}_{d}\\frac{\\partial J}{\\partial z^{(N)}}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This is a matrix multiplication in disguise (each row is a dot product) and can be written more compactly as:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\mathbf{w}} = \\begin{bmatrix}\n",
    "x^{(1)}_1 & x^{(2)}_1 & \\cdots & x^{(N)}_1\\\\\n",
    "x^{(1)}_2 & x^{(2)}_2 & \\cdots & x^{(N)}_2\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "x^{(1)}_d & x^{(2)}_d & \\cdots & x^{(N)}_d\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial J}{\\partial z^{(1)}} \\\\\n",
    "\\frac{\\partial J}{\\partial z^{(2)}} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial J}{\\partial z^{(N)}}\n",
    "\\end{bmatrix}\n",
    "=\\mathbf{X}^{T}\\frac{\\partial J}{\\partial \\mathbf{z}}\n",
    "$$\n",
    "\n",
    "From the implementation perspective $\\mathbf{X}^{T}$ is computed locally in the `backward` function of the `Lin` class while $\\frac{\\partial J}{\\partial \\mathbf{z}}$ is ready and waiting to be accessed. Recall the latter was computed in the `backward` function of the `Relu` class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0018e2e2-0859-42f8-8070-17ae5c885f63",
   "metadata": {},
   "source": [
    "### Gradient of loss with respect to the bias $b$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68af455-d050-4846-ac2c-dfaa6adb0a89",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial J}{\\partial b} = \\frac{\\partial J}{\\partial \\mathbf{z}} \\frac{\\partial \\mathbf{z}}{\\partial b}= \\sum_{i=1}^{N}\\frac{\\partial J}{\\partial z^{(i)}} \\frac{\\partial z^{(i)}}{\\partial b}= \\sum_{i=1}^{N}\\frac{\\partial J}{\\partial z^{(i)}} 1 = \\frac{\\partial J}{\\partial z^{(1)}} + \\frac{\\partial J}{\\partial z^{(2)}} + \\ldots + \\frac{\\partial J}{\\partial z^{(N)}}$$\n",
    "\n",
    "From the implementation perspective we need to access $\\frac{\\partial J}{\\partial \\mathbf{z}}$ and sum across the first axis. The local derivative computation of $\\frac{\\partial \\mathbf{z}}{\\partial b}$ is particulary simple (since each $\\frac{\\partial z^{(i)}}{\\partial b}$ is just $1$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7120b6-94a0-4573-969b-52fceff856aa",
   "metadata": {},
   "source": [
    "### Gradient of loss with respect to $x^{(i)}_{j}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd99958-286d-4785-99de-1606c3d03229",
   "metadata": {},
   "source": [
    "Let's understand how the loss will change as we twiddle the $j$-th feature of the $i$-th training example.\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial x^{(i)}_{j}} = \\frac{\\partial J}{\\partial \\mathbf{z}} \\frac{\\partial \\mathbf{z}}{\\partial x^{(i)}_{j}}= \\sum_{k=1}^{N}\\frac{\\partial J}{\\partial z^{(k)}} \\frac{\\partial z^{(k)}}{\\partial x^{(i)}_{j}} = \\frac{\\partial J}{\\partial z^{(i)}}\\frac{\\partial z^{(i)}}{\\partial x^{(i)}_{j}} + \\sum_{k: k \\neq i}^{N} \\frac{\\partial J}{\\partial z^{(k)}}\\frac{\\partial z^{(k)}}{\\partial x^{(i)}_{j}}$$\n",
    "\n",
    "Since $\\frac{\\partial z^{(k)}}{\\partial x^{(i)}_{j}} = 0$ for any $k \\neq i$ we get\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial x^{(i)}_{j}} = \\frac{\\partial J}{\\partial z^{(i)}}w_{j}$$\n",
    "\n",
    "Thus the $N \\times d$ matrix of these gradients are,\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\mathbf{X}} = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial J}{\\partial x^{(1)}_{1}} & \\frac{\\partial J}{\\partial x^{(1)}_{2}} & \\cdots & \\frac{\\partial J}{\\partial x^{(1)}_{d}} \\\\\n",
    "\\frac{\\partial J}{\\partial x^{(2)}_{1}} & \\frac{\\partial J}{\\partial x^{(2)}_{2}} & \\cdots & \\frac{\\partial J}{\\partial x^{(2)}_{d}} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial J}{\\partial x^{(N)}_{1}} & \\frac{\\partial J}{\\partial x^{(N)}_{2}} & \\cdots & \\frac{\\partial J}{\\partial x^{(N)}_{d}}\n",
    "\\end{bmatrix}= \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial J}{\\partial z^{(1)}}w_{1} & \\frac{\\partial J}{\\partial z^{(1)}}w_{2} & \\cdots & \\frac{\\partial J}{\\partial z^{(1)}}w_{d} \\\\\n",
    "\\frac{\\partial J}{\\partial z^{(2)}}w_{1} & \\frac{\\partial J}{\\partial z^{(2)}}w_{2} & \\cdots & \\frac{\\partial J}{\\partial z^{(2)}}w_{d} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial J}{\\partial z^{(N)}}w_{1} & \\frac{\\partial J}{\\partial z^{(N)}}w_{2} & \\cdots & \\frac{\\partial J}{\\partial z^{(N)}}w_{d}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "More compactly this can be represented as an outer product,\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\mathbf{X}} =\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial J}{\\partial z^{(1)}} \\\\\n",
    "\\frac{\\partial J}{\\partial z^{(2)}} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial J}{\\partial z^{(N)}}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "w_{1} & w_{2} & \\cdots & w_{d}\n",
    "\\end{bmatrix}\n",
    "=\\frac{\\partial J}{\\partial \\mathbf{z}}\\mathbf{w}^{T}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bca9cf-4545-4803-a55e-fa9046f98b81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
