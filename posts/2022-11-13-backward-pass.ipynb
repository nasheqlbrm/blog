{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7758082a-3e3f-4cac-81a9-f3ec86c05693",
   "metadata": {},
   "source": [
    "---\n",
    "categories: [neural-nets]\n",
    "date: '2022-11-13'\n",
    "description: Implementing the forward and backward pass for a simple neural net.\n",
    "output-file: 2021-11-13-backward-pass.html\n",
    "title: Backward Pass\n",
    "bibliography: ../references.bib\n",
    "csl: ../control-and-automation.csl\n",
    "toc: true\n",
    "use_math: true\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4309488-eeda-40ad-add3-c1552fd3e01e",
   "metadata": {},
   "source": [
    "# Notation\n",
    "* $N$ : number of training examples\n",
    "* $d$ : number of features\n",
    "* $\\mathbf{x}^{(i)}$ is the $i$-th training example $$\n",
    "\\mathbf{x}^{(i)} = \\begin{bmatrix}\n",
    "x^{(i)}_1 \\\\\n",
    "x^{(i)}_2 \\\\\n",
    "\\vdots \\\\\n",
    "x^{(i)}_d\n",
    "\\end{bmatrix}\n",
    "$$ \n",
    "* $\\mathbf{w}$ is the vector of weights for a single neuron $$\n",
    "\\mathbf{w} = \\begin{bmatrix}\n",
    "w_1 \\\\\n",
    "w_2 \\\\\n",
    "\\vdots \\\\\n",
    "w_d\n",
    "\\end{bmatrix}\n",
    "$$ \n",
    "* $b$ is the bias term for a single neuron\n",
    "* $\\mathbf{X}$ is an $N \\times d$ matrix with the training examples stacked in rows\n",
    "$$\n",
    "\\mathbf{X} = \\begin{bmatrix}\n",
    "x^{(1)}_1 & x^{(1)}_2 & \\cdots & x^{(1)}_d\\\\\n",
    "x^{(2)}_1 & x^{(2)}_2 & \\cdots & x^{(2)}_d\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "x^{(N)}_1 & x^{(N)}_2 & \\cdots & x^{(N)}_d\n",
    "\\end{bmatrix}\n",
    "$$ \n",
    "* $\\{y^{(i)}\\}_{i=1}^{N}$ are the targets for each of the $N$ training examples\n",
    "* $z^{(i)}$ is the output of our single neuron when the $i$-th training example is passed through it\n",
    "    * $z^{(i)} = \\mathbf{w}^T\\mathbf{x}^{(i)} + b = b + \\sum_{j=1}^{d}w_{j}x^{(i)}_{j}$\n",
    "    * $\\frac{\\partial z^{(i)}}{\\partial w_{j}} = x^{(i)}_{j}$\n",
    "* $a^{(i)} = \\phi(z^{(i)})$ is the activation when an input $z^{(i)}$ is passed through an activation function $\\phi$\n",
    "    * $\\{a^{(i)}\\}_{i=1}^{N}$ are the activations for each of the $N$ training examples when passed through a single neuron followed by the application of the activation function\n",
    "    * For the purposes of this page the activation function is considered to be a ReLU then $a^{(i)} = \\phi(z^{(i)}) = \\max\\{0, z^{(i)}\\}$\n",
    "* $J\\left(\\{y^{(i)}\\}_{i=1}^{N},\\{a^{(i)}\\}_{i=1}^{N}\\right) = \\frac{1}{N}\\sum_{i=1}^{N}(y^{(i)}-a^{(i)})^{2}$ is the mean squared error across the $N$ training examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bfc3f4-e80b-47e0-9dee-0d2fd9cc23e0",
   "metadata": {},
   "source": [
    "# Derivatives \n",
    "\n",
    "## Loss with respect to the Activations $a^{(i)}$ \n",
    "So how does the loss change as the $i$-th activation changes:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial a^{(i)}} = \\frac{2}{N}\\sum_{i=1}^{N} \\frac{\\partial (y^{(i)}-a^{(i)})^{2}}{\\partial a^{(i)}} = \\frac{2}{N}(y^{(i)}-a^{(i)})\\frac{\\partial (y^{(i)}-a^{(i)}) }{\\partial a^{(i)}} = \\frac{2}{N}(a^{(i)}-y^{(i)})$$\n",
    "\n",
    "where the last step follows because $\\frac{\\partial (y^{(i)}-a^{(i)}) }{\\partial a^{(i)}} = 0-1 =-1$.\n",
    "\n",
    "The change in the loss as a function of the change in activations from our training examples is captured by the $N \\times 1$ matrix:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\mathbf{a}} = \\begin{bmatrix}\n",
    "\\frac{\\partial J}{\\partial a^{(1)}} \\\\\n",
    "\\frac{\\partial J}{\\partial a^{(2)}} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial J}{\\partial a^{(N)}}\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "\\frac{2}{N}\\left(a^{(1)}-y^{(1)}\\right) \\\\\n",
    "\\frac{2}{N}\\left(a^{(2)}-y^{(2)}\\right) \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{2}{N}\\left(a^{(N)}-y^{(N)}\\right)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "## Loss with respect to the Linear Output $z^{(i)}$ \n",
    "How does the loss change as the output of the linear unit changes?\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial z^{(i)}} = \\frac{\\partial J}{\\partial a^{(i)}} \\frac{\\partial a^{(i)}}{\\partial z^{(i)}} = \\frac{2}{N}\\left(a^{(i)}-y^{(i)}\\right)\\frac{\\partial a^{(i)}}{\\partial z^{(i)}}$$\n",
    "\n",
    "For the ReLU activation function we have that,\n",
    "$$\n",
    "\\frac{\\partial a^{(i)}}{\\partial z^{(i)}} = \n",
    "\\begin{cases}\n",
    "  0 & z^{(i)} \\leq 0 \\\\\n",
    "  1 & z^{(i)} \\gt 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The change in the loss as a function of the change in the output from the linear unit on our training examples is given by the $N \\times 1$ matrix:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\mathbf{z}} = \\begin{bmatrix}\n",
    "\\frac{\\partial J}{\\partial z^{(1)}} \\\\\n",
    "\\frac{\\partial J}{\\partial z^{(2)}} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial J}{\\partial z^{(N)}}\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "\\frac{2}{N}\\left(a^{(1)}-y^{(1)}\\right)\\frac{\\partial a^{(1)}}{\\partial z^{(1)}} \\\\\n",
    "\\frac{2}{N}\\left(a^{(2)}-y^{(2)}\\right)\\frac{\\partial a^{(2)}}{\\partial z^{(2)}} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{2}{N}\\left(a^{(N)}-y^{(N)}\\right)\\frac{\\partial a^{(N)}}{\\partial z^{(N)}}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "## Loss with respect to $w_{j}$ \n",
    "\n",
    "$$\\frac{\\partial J}{\\partial w_{j}} = \\frac{\\partial J}{\\partial \\mathbf{z}} \\frac{\\partial \\mathbf{z}}{\\partial w_{j}}= \\sum_{i=1}^{N}\\frac{\\partial J}{\\partial z^{(i)}} \\frac{\\partial z^{(i)}}{\\partial w_{j}} = \\frac{\\partial J}{\\partial z^{(1)}}x^{(1)}_{j} + \\frac{\\partial J}{\\partial z^{(2)}}x^{(2)}_{j} + \\ldots + \\frac{\\partial J}{\\partial z^{(N)}}x^{(N)}_{j}$$\n",
    "\n",
    "Thus,\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\mathbf{w}} = \\begin{bmatrix}\n",
    "\\frac{\\partial J}{\\partial w_{1}} \\\\\n",
    "\\frac{\\partial J}{\\partial w_{2}} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial J}{\\partial w_{d}}\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "x^{(1)}_{1}\\frac{\\partial J}{\\partial z^{(1)}} + x^{(2)}_{1}\\frac{\\partial J}{\\partial z^{(2)}} + \\ldots + x^{(N)}_{1}\\frac{\\partial J}{\\partial z^{(N)}} \\\\\n",
    "x^{(1)}_{2}\\frac{\\partial J}{\\partial z^{(1)}} + x^{(2)}_{2}\\frac{\\partial J}{\\partial z^{(2)}} + \\ldots + x^{(N)}_{2}\\frac{\\partial J}{\\partial z^{(N)}} \\\\\n",
    "\\vdots \\\\\n",
    "x^{(1)}_{d}\\frac{\\partial J}{\\partial z^{(1)}} + x^{(2)}_{d}\\frac{\\partial J}{\\partial z^{(2)}} + \\ldots + x^{(N)}_{d}\\frac{\\partial J}{\\partial z^{(N)}}\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "x^{(1)}_1 & x^{(2)}_1 & \\cdots & x^{(N)}_1\\\\\n",
    "x^{(1)}_2 & x^{(2)}_2 & \\cdots & x^{(N)}_2\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "x^{(1)}_d & x^{(2)}_d & \\cdots & x^{(N)}_d\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial J}{\\partial z^{(1)}} \\\\\n",
    "\\frac{\\partial J}{\\partial z^{(2)}} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial J}{\\partial z^{(N)}}\n",
    "\\end{bmatrix}\n",
    "=\\mathbf{X}^{T}\\begin{bmatrix}\n",
    "\\frac{\\partial J}{\\partial z^{(1)}} \\\\\n",
    "\\frac{\\partial J}{\\partial z^{(2)}} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial J}{\\partial z^{(N)}}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bca9cf-4545-4803-a55e-fa9046f98b81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
