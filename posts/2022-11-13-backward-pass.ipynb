{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7758082a-3e3f-4cac-81a9-f3ec86c05693",
   "metadata": {},
   "source": [
    "---\n",
    "categories: [neural-nets]\n",
    "date: '2022-11-13'\n",
    "description: Implementing the forward and backward pass for a simple neural net.\n",
    "output-file: 2021-11-13-backward-pass.html\n",
    "title: Backward Pass\n",
    "bibliography: ../references.bib\n",
    "csl: ../control-and-automation.csl\n",
    "toc: true\n",
    "use_math: true\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4309488-eeda-40ad-add3-c1552fd3e01e",
   "metadata": {},
   "source": [
    "# Notation\n",
    "* $N$ : number of training examples\n",
    "* $d$ : number of features\n",
    "* $\\mathbf{x}^{(i)}$ is the $i$-th training example $$\n",
    "\\mathbf{x}^{(i)} = \\begin{bmatrix}\n",
    "x^{(i)}_1 \\\\\n",
    "x^{(i)}_2 \\\\\n",
    "\\vdots \\\\\n",
    "x^{(i)}_d\n",
    "\\end{bmatrix}\n",
    "$$ \n",
    "* $\\mathbf{w}$ is the vector of weights for a single neuron $$\n",
    "\\mathbf{w} = \\begin{bmatrix}\n",
    "w_1 \\\\\n",
    "w_2 \\\\\n",
    "\\vdots \\\\\n",
    "w_d\n",
    "\\end{bmatrix}\n",
    "$$ \n",
    "* $b$ is the bias term for a single neuron\n",
    "* $\\mathbf{X}$ is an $N \\times d$ matrix with the training examples stacked in rows\n",
    "$$\n",
    "\\mathbf{X} = \\begin{bmatrix}\n",
    "x^{(1)}_1 & x^{(1)}_2 & \\cdots & x^{(1)}_d\\\\\n",
    "x^{(2)}_1 & x^{(2)}_2 & \\cdots & x^{(2)}_d\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "x^{(N)}_1 & x^{(N)}_2 & \\cdots & x^{(N)}_d\n",
    "\\end{bmatrix}\n",
    "$$ \n",
    "* $\\{y^{(i)}\\}_{i=1}^{N}$ are the targets for each of the $N$ training examples\n",
    "* $z^{(i)} = \\mathbf{w}^T\\mathbf{x}^{(i)} + b$ is the output of our single neuron when the $i$-th training example is passed through it\n",
    "* $a^{(i)} = \\phi(z^{(i)})$ is the activation when an input $z^{(i)}$ is passed through an activation function $\\phi$\n",
    "    * $\\{a^{(i)}\\}_{i=1}^{N}$ are the activations for each of the $N$ training examples when passed through a single neuron followed by the application of the activation function\n",
    "* $J\\left(\\{y^{(i)}\\}_{i=1}^{N},\\{a^{(i)}\\}_{i=1}^{N}\\right) = \\frac{1}{N}\\sum_{i=1}^{N}(y^{(i)}-a^{(i)})^{2}$ is the mean squared error across the $N$ training examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bfc3f4-e80b-47e0-9dee-0d2fd9cc23e0",
   "metadata": {},
   "source": [
    "# Derivatives \n",
    "\n",
    "## Loss with respect to the Activations\n",
    "So how does the loss change as the $i$-th activation changes:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial a^{(i)}} = \\frac{2}{N}\\sum_{i=1}^{N} \\frac{\\partial (y^{(i)}-a^{(i)})^{2}}{\\partial a^{(i)}}  = \\frac{2}{N}(y^{(i)}-a^{(i)})\\frac{\\partial (y^{(i)}-a^{(i)}) }{\\partial a^{(i)}} = \\frac{2}{N}(y^{(i)}-a^{(i)})(0-1) = \\frac{2}{N}(y^{(i)}-a^{(i)})\\frac{\\partial (y^{(i)}-a^{(i)}) }{\\partial a_i} = \\frac{2}{N}(a^{(i)}-y^{(i)})$$\n",
    "\n",
    "The change in the loss as a function of the change in activations from our training examples is captured by the $N \\times 1$ matrix:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\mathbf{a}} = \\begin{bmatrix}\n",
    "\\frac{\\partial J}{\\partial a^{(1)}} \\\\\n",
    "\\frac{\\partial J}{\\partial a^{(2)}} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial J}{\\partial a^{(N)}}\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "\\frac{2}{N}\\left(a^{(1)}-y^{(1)}\\right) \\\\\n",
    "\\frac{2}{N}\\left(a^{(2)}-y^{(2)}\\right) \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{2}{N}\\left(a^{(N)}-y^{(N)}\\right)\n",
    "\\end{bmatrix}\n",
    "$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bca9cf-4545-4803-a55e-fa9046f98b81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
