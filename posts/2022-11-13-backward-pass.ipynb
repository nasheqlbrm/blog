{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7758082a-3e3f-4cac-81a9-f3ec86c05693",
   "metadata": {},
   "source": [
    "---\n",
    "categories: [neural-nets]\n",
    "date: '2022-11-13'\n",
    "description: Implementing the forward and backward pass for a simple neural net.\n",
    "output-file: 2021-11-13-backward-pass.html\n",
    "title: Backward Pass\n",
    "bibliography: ../references.bib\n",
    "csl: ../control-and-automation.csl\n",
    "toc: true\n",
    "use_math: true\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4309488-eeda-40ad-add3-c1552fd3e01e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Notation\n",
    "* $N$ : number of training examples\n",
    "* $d$ : number of features\n",
    "* $\n",
    "\\mathbf{x}^{(i)} = \\begin{bmatrix}\n",
    "x^{(i)}_1 \\\\\n",
    "x^{(i)}_2 \\\\\n",
    "\\vdots \\\\\n",
    "x^{(i)}_d\n",
    "\\end{bmatrix}\n",
    "$ is the $i$-th training example\n",
    "* $\n",
    "\\mathbf{w} = \\begin{bmatrix}\n",
    "w_1 \\\\\n",
    "w_2 \\\\\n",
    "\\vdots \\\\\n",
    "w_d\n",
    "\\end{bmatrix}\n",
    "$ is the vector of weights and $b$ is the bias for a single neuron.\n",
    "* $\n",
    "\\mathbf{X} = \\begin{bmatrix}\n",
    "x^{(1)}_1 & x^{(1)}_2 & \\cdots & x^{(1)}_d\\\\\n",
    "x^{(2)}_1 & x^{(2)}_2 & \\cdots & x^{(2)}_d\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "x^{(N)}_1 & x^{(N)}_2 & \\cdots & x^{(N)}_d\n",
    "\\end{bmatrix}\n",
    "$ is an $N \\times d$ matrix with the training examples stacked in rows.\n",
    "* $\\{y^{(i)}\\}_{i=1}^{N}$ are the targets for each of the $N$ training examples\n",
    "* $z^{(i)} = \\mathbf{w}^T\\mathbf{x}^{(i)} + b$ is the output of our single neuron when the $i$-th training example is passed through it\n",
    "* $a^{(i)} = \\phi(z^{(i)})$ is the activation when an input $z^{(i)}$ is passed through an activation function $\\phi$\n",
    "    * $\\{a^{(i)}\\}_{i=1}^{N}$ are the activations for each of the $N$ training examples when passed through a single neuron followed by the application of the activation function\n",
    "* $J\\left(\\{y^{(i)}\\}_{i=1}^{N},\\{a^{(i)}\\}_{i=1}^{N}\\right) = \\frac{1}{N}\\sum_{i=1}^{N}(y^{(i)}-a^{(i)})^{2}$ is the mean squared error across the $N$ training examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bfc3f4-e80b-47e0-9dee-0d2fd9cc23e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Derivative of Loss with respect to the Activations\n",
    "\n",
    "$\\frac{\\partial f}{\\partial x_i}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bca9cf-4545-4803-a55e-fa9046f98b81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
