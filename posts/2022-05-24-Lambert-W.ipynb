{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "aliases:\n",
    "- /2022/05/24/Lambert-W\n",
    "badges: true\n",
    "date: '2022-05-24'\n",
    "output-file: 2022-05-24-lambert-w.html\n",
    "title: Lambert W\n",
    "toc: true\n",
    "use_math: true\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sNhMMJKVXPp4"
   },
   "source": [
    "From {% cite youtube:Lambert_W_Function_Intro %}\n",
    "\n",
    "The Lambert W Function is denoted $W(x)$ and is the inverse of the function $f(x) = xe^{x}$. \n",
    "\n",
    "Thus, $W(x) = f^{-1}(x)$. \n",
    "\n",
    "For $f(x)$ we have that:\n",
    "* The derivative $f'(x) = e^{x}(x+1)$ \n",
    "* $f''(x) = 2e^{x}  + xe^{x}$ and $f''(-1) = 1$\n",
    "* $f(x)$ is minimized at $x = -1$ \n",
    "* Domain of $f(x)$ is $[-1,\\infty)$\n",
    "* Range of $f(x)$ is $[-\\frac{1}{e},\\infty)$\n",
    "\n",
    "The last two bullet points mean that the domain and range of $W(x)$ are $[-\\frac{1}{e},\\infty)$ and $[-1,\\infty)$ respectively.\n",
    "\n",
    "For $W(x)$ we also have that:\n",
    "\n",
    "1.   $W(f(x)) = W(xe^{x}) = x$\n",
    "2.   $f(W(x)) = W(x)e^{W(x)} = x$. In other words, $e^{W(x)} = \\frac{x}{W(x)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3rpwOaaAgKjJ"
   },
   "source": [
    "## Solve $x^{x} = 2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xZldI6Kbb-zS"
   },
   "source": [
    "Take the log of both sides to get $x\\ln(x) = \\ln(2)$ or that $e^{\\ln(x)}\\ln(x) = \\ln(2)$. Rearranging we get $\\ln(x)e^{\\ln(x)} = \\ln(2)$. \n",
    "\n",
    "Applying the Lambert W function on both sides produces $W(\\ln(x)e^{\\ln(x)}) = W(\\ln(2))$ or that $\\ln(x) = W(\\ln(2))$.\n",
    "\n",
    "Finally, we get that $x = e^{W(\\ln(2))}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l3vWkhXPUgjU"
   },
   "outputs": [],
   "source": [
    "from scipy.special import lambertw\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x5MjBApwdm-J",
    "outputId": "38c21962-e3d2-4027-c516-093192221fbd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1.5596104694623694+0j), True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.exp( lambertw( np.log(2) ) )\n",
    "x, np.allclose( np.power(x,x) , 2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kfktDHQZepMJ"
   },
   "source": [
    "Thus, the solution of $x^{x} = 2$ is $x = 1.55961$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FFQJ_WXngXhj"
   },
   "source": [
    "## Solve $x^{2}e^{x} = 2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jv0YAZg3ewzG"
   },
   "source": [
    "Take the square root in both sides to get $xe^{x/2} = \\sqrt 2$. Divide both sides by $2$ to get $0.5xe^{0.5x} = \\frac{1}{\\sqrt 2}$. \n",
    "\n",
    "Applying the Lambert W function on both sides produces $W(0.5xe^{0.5x}) = W(\\frac{1}{\\sqrt 2})$ or $0.5x = W(\\frac{1}{\\sqrt 2})$.\n",
    "\n",
    "Finally, we get $x = 2W(\\frac{1}{\\sqrt 2})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "if51OpnJei2z",
    "outputId": "ffae59eb-caf2-4f50-a151-860f2d18005a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0.9012010317296661+0j), True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 2 * lambertw(1./np.sqrt(2))\n",
    "x, np.allclose( x*x*np.exp(x), 2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d-4B19Vvgb5B"
   },
   "source": [
    "## Solve $x + e^{x} = 2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iFSSl6eshB4Z"
   },
   "source": [
    "Exponentiating both sides we get $e^{x}e^{e^{x}} = e^{2}$. Applying the Lambert W function on both sides produces $W( e^{x}e^{e^{x}} ) = W(e^{2})$ or $e^{x} = W(e^{2})$\n",
    "\n",
    "Thus, $x = \\ln(W(e^{2}))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MI23zXSXhvVD",
    "outputId": "a759afbe-6bd4-4456-8a16-95d9f41bfcc1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0.4428544010023887+0j), True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.log( lambertw( np.exp(2) ) )\n",
    "x, np.allclose( x + np.exp(x) , 2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "31zOZ0zmgjWq"
   },
   "source": [
    "## Find the minimizer of $x \\ln(\\frac{x}{u}) + \\frac{1}{2\\lambda}(x-v)^{2}$ (suppose $\\lambda >0$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dVYicOF2F5ec"
   },
   "source": [
    "This is a sum of convex functions so it is convex and a minimizer exists.\n",
    "\n",
    "Taking the derivative and setting equal to zero we get: $$1 + \\ln x - \\ln u + \\frac{x}{\\lambda} - \\frac{v}{\\lambda} = 0$$\n",
    "\n",
    "Hence, $\\ln x + \\frac{x}{\\lambda} = \\ln u  + \\frac{v}{\\lambda} - 1$. \n",
    "\n",
    "Exponentiating both sides we get:\n",
    "\n",
    "$e^{\\ln x + \\frac{x}{\\lambda}} = e^{\\ln u  + \\frac{v}{\\lambda} - 1}$ or $xe^{\\frac{x}{\\lambda}} = ue^{\\frac{v}{\\lambda} - 1}$. \n",
    "\n",
    "Dividing both sides by $\\lambda$ we have $\\frac{x}{\\lambda}e^{\\frac{x}{\\lambda}} = \\frac{u}{\\lambda}e^{\\frac{v}{\\lambda} - 1}$.\n",
    "\n",
    "Applying the Lambert W function on both sides gives: $$ W(\\frac{x}{\\lambda}e^{\\frac{x}{\\lambda}}) = W(\\frac{u}{\\lambda}e^{\\frac{v}{\\lambda} - 1})$$ or\n",
    "\n",
    "$\\frac{x}{\\lambda} = W(\\frac{u}{\\lambda}e^{\\frac{v}{\\lambda} - 1})$.\n",
    "\n",
    "Thus, $$x = \\lambda W(\\frac{u}{\\lambda}e^{\\frac{v}{\\lambda} - 1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z0bdzkRiVBF5"
   },
   "source": [
    "References\n",
    "\n",
    "{% bibliography --cited %}"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "2022-05-24-Lambert-W.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
