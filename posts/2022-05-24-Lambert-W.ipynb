{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "aliases:\n",
    "- /2022/05/24/Lambert-W\n",
    "badges: true\n",
    "date: '2022-05-24'\n",
    "output-file: 2022-05-24-lambert-w.html\n",
    "title: Lambert W\n",
    "description: What value of $x$ satisfies $x^x=2$.\n",
    "bibliography: ../references.bib\n",
    "csl: ../control-and-automation.csl\n",
    "toc: true\n",
    "use_math: true\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From @youtube:Lambert_W_Function_Intro:\n",
    "\n",
    "The Lambert W Function is denoted $W(x)$ and is the inverse of the function $f(x) = xe^{x}$. \n",
    "\n",
    "Thus, $W(x) = f^{-1}(x)$. \n",
    "\n",
    "For $f(x)$ we have that:\n",
    "\n",
    "* The derivative $f'(x) = e^{x}(x+1)$ \n",
    "* $f''(x) = 2e^{x}  + xe^{x}$ and $f''(-1) = 1$\n",
    "* $f(x)$ is minimized at $x = -1$ \n",
    "* Domain of $f(x)$ is $[-1,\\infty)$\n",
    "* Range of $f(x)$ is $[-\\frac{1}{e},\\infty)$\n",
    "\n",
    "The last two bullet points mean that the domain and range of $W(x)$ are $[-\\frac{1}{e},\\infty)$ and $[-1,\\infty)$ respectively.\n",
    "\n",
    "For $W(x)$ we also have that:\n",
    "\n",
    "1.   $W(f(x)) = W(xe^{x}) = x$\n",
    "2.   $f(W(x)) = W(x)e^{W(x)} = x$. In other words, $e^{W(x)} = \\frac{x}{W(x)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solve $x^{x} = 2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the log of both sides to get $x\\ln(x) = \\ln(2)$ or that $e^{\\ln(x)}\\ln(x) = \\ln(2)$. Rearranging we get $\\ln(x)e^{\\ln(x)} = \\ln(2)$. \n",
    "\n",
    "Applying the Lambert W function on both sides produces $W(\\ln(x)e^{\\ln(x)}) = W(\\ln(2))$ or that $\\ln(x) = W(\\ln(2))$.\n",
    "\n",
    "Finally, we get that $x = e^{W(\\ln(2))}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import lambertw\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1.5596104694623694+0j), True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.exp( lambertw( np.log(2) ) )\n",
    "x, np.allclose( np.power(x,x) , 2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the solution of $x^{x} = 2$ is $x = 1.55961$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solve $x^{2}e^{x} = 2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the square root in both sides to get $xe^{x/2} = \\sqrt 2$. Divide both sides by $2$ to get $0.5xe^{0.5x} = \\frac{1}{\\sqrt 2}$. \n",
    "\n",
    "Applying the Lambert W function on both sides produces $W(0.5xe^{0.5x}) = W(\\frac{1}{\\sqrt 2})$ or $0.5x = W(\\frac{1}{\\sqrt 2})$.\n",
    "\n",
    "Finally, we get $x = 2W(\\frac{1}{\\sqrt 2})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0.9012010317296661+0j), True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 2 * lambertw(1./np.sqrt(2))\n",
    "x, np.allclose( x*x*np.exp(x), 2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solve $x + e^{x} = 2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exponentiating both sides we get $e^{x}e^{e^{x}} = e^{2}$. Applying the Lambert W function on both sides produces $W( e^{x}e^{e^{x}} ) = W(e^{2})$ or $e^{x} = W(e^{2})$\n",
    "\n",
    "Thus, $x = \\ln(W(e^{2}))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0.4428544010023887+0j), True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.log( lambertw( np.exp(2) ) )\n",
    "x, np.allclose( x + np.exp(x) , 2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the minimizer of $x \\ln(\\frac{x}{u}) + \\frac{1}{2\\lambda}(x-v)^{2}$ (suppose $\\lambda >0$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a sum of convex functions so it is convex and a minimizer exists.\n",
    "\n",
    "Taking the derivative and setting equal to zero we get: $$1 + \\ln x - \\ln u + \\frac{x}{\\lambda} - \\frac{v}{\\lambda} = 0$$\n",
    "\n",
    "Hence, $\\ln x + \\frac{x}{\\lambda} = \\ln u  + \\frac{v}{\\lambda} - 1$. \n",
    "\n",
    "Exponentiating both sides we get:\n",
    "\n",
    "$e^{\\ln x + \\frac{x}{\\lambda}} = e^{\\ln u  + \\frac{v}{\\lambda} - 1}$ or $xe^{\\frac{x}{\\lambda}} = ue^{\\frac{v}{\\lambda} - 1}$. \n",
    "\n",
    "Dividing both sides by $\\lambda$ we have $\\frac{x}{\\lambda}e^{\\frac{x}{\\lambda}} = \\frac{u}{\\lambda}e^{\\frac{v}{\\lambda} - 1}$.\n",
    "\n",
    "Applying the Lambert W function on both sides gives: $$ W(\\frac{x}{\\lambda}e^{\\frac{x}{\\lambda}}) = W(\\frac{u}{\\lambda}e^{\\frac{v}{\\lambda} - 1})$$ or\n",
    "\n",
    "$\\frac{x}{\\lambda} = W(\\frac{u}{\\lambda}e^{\\frac{v}{\\lambda} - 1})$.\n",
    "\n",
    "Thus, $$x = \\lambda W(\\frac{u}{\\lambda}e^{\\frac{v}{\\lambda} - 1})$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
