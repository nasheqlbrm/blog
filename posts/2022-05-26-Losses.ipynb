{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "aliases:\n",
    "- /2022/05/26/Losses\n",
    "badges: true\n",
    "date: '2022-05-26'\n",
    "output-file: 2022-05-26-losses.html\n",
    "title: Losses\n",
    "bibliography: ../references.bib\n",
    "csl: ../control-and-automation.csl\n",
    "toc: true\n",
    "use_math: true\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some notes while reading @barratt2021optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting git+https://github.com/cvxgrp/rsw\n",
      "  Cloning https://github.com/cvxgrp/rsw to /tmp/pip-req-build-l2ggr2g2\n",
      "  Running command git clone -q https://github.com/cvxgrp/rsw /tmp/pip-req-build-l2ggr2g2\n",
      "Requirement already satisfied: cvxpy>=1.0 in /usr/local/lib/python3.7/dist-packages (from rsw==0.1) (1.0.31)\n",
      "Requirement already satisfied: scipy>=1.4 in /usr/local/lib/python3.7/dist-packages (from rsw==0.1) (1.4.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from rsw==0.1) (1.3.5)\n",
      "Requirement already satisfied: qdldl>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from rsw==0.1) (0.1.5.post2)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from rsw==0.1) (3.2.2)\n",
      "Requirement already satisfied: osqp>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from cvxpy>=1.0->rsw==0.1) (0.6.2.post0)\n",
      "Requirement already satisfied: ecos>=2 in /usr/local/lib/python3.7/dist-packages (from cvxpy>=1.0->rsw==0.1) (2.0.10)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from cvxpy>=1.0->rsw==0.1) (0.70.12.2)\n",
      "Requirement already satisfied: scs>=1.1.3 in /usr/local/lib/python3.7/dist-packages (from cvxpy>=1.0->rsw==0.1) (3.2.0)\n",
      "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from cvxpy>=1.0->rsw==0.1) (1.21.6)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->rsw==0.1) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->rsw==0.1) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->rsw==0.1) (1.4.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->rsw==0.1) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->rsw==0.1) (4.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->rsw==0.1) (1.15.0)\n",
      "Requirement already satisfied: dill>=0.3.4 in /usr/local/lib/python3.7/dist-packages (from multiprocess->cvxpy>=1.0->rsw==0.1) (0.3.5.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->rsw==0.1) (2022.1)\n",
      "Building wheels for collected packages: rsw\n",
      "  Building wheel for rsw (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for rsw: filename=rsw-0.1-py3-none-any.whl size=10114 sha256=cbfa43b8adfd26ead50ac4e811bf24e9bba49d76a9f86f9e5e38c882c8f19a2e\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-4a3i8ae1/wheels/de/2d/85/9a7a052a72fbed524aecfff7fc037432cae253a69e9883ba45\n",
      "Successfully built rsw\n",
      "Installing collected packages: rsw\n",
      "Successfully installed rsw-0.1\n"
     ]
    }
   ],
   "source": [
    "#| include: false\n",
    "!pip install git+https://github.com/cvxgrp/rsw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy as cp\n",
    "import rsw\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 36\n",
      "drwxr-xr-x 3 root root 4096 May 28 11:50 .\n",
      "drwxr-xr-x 1 root root 4096 May 28 11:50 ..\n",
      "-rw-r--r-- 1 root root   78 May 28 11:50 __init__.py\n",
      "-rw-r--r-- 1 root root 3460 May 28 11:50 losses.py\n",
      "drwxr-xr-x 2 root root 4096 May 28 11:50 __pycache__\n",
      "-rw-r--r-- 1 root root 2230 May 28 11:50 regularizers.py\n",
      "-rw-r--r-- 1 root root 1909 May 28 11:50 rsw.py\n",
      "-rw-r--r-- 1 root root 4645 May 28 11:50 solver.py\n"
     ]
    }
   ],
   "source": [
    "#| include: false\n",
    "!ls -al /usr/local/lib/python3.7/dist-packages/rsw/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equality Loss\n",
    "Constrain $x = f_{\\text{des}}$ so let $l(x, f_{des}) = +\\infty$ for $x \\neq f_{des}$ and $0$ when $x = f_{des}$. Here $l$ represents our loss function.\n",
    "\n",
    "### Convex Optimization Version\n",
    "\n",
    "$$\\min \\frac{1}{\\lambda}\\sum_{i=1}^{m}( \\hat{f}_{i}-f^{i})^{2} = \\frac{1}{\\lambda}||\\hat{f}-f||^{2}_{2}$$\n",
    "$$\\text{subject to:}\\quad \\hat{f}_{i} = f^{i}_{\\text{des}},\\quad i=1,\\ldots,m$$\n",
    "\n",
    "### Proximal Version\n",
    "$\\mathbf{prox}_{\\lambda l}(f_{\\text{des}}) = argmin_{x} \\left( l(x, f_{des}) + \\frac{1}{2\\lambda}||x-f_{\\text{des}}||^{2}_{2} \\right)$\n",
    "\n",
    "So we have to set $x_{i} = f^{i}_{\\text{des}}$for each $i$ in order to avoid the loss of $\\infty$ which we incur for any other choice.\n",
    "\n",
    "Thus, $\\mathbf{prox}_{\\lambda l}(f_{\\text{des}})_{i} = f^{i}_{\\text{des}}$ is the $i$-th component of the minimizer $x$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of \"marginals\" we want to match.\n",
    "m = 10\n",
    "\n",
    "# Expected values of these functions under the induced distribution.\n",
    "# There is one for each marginal that we want to match.\n",
    "f = np.random.randn(m) \n",
    "\n",
    "# The desired or target values we want to match by applying weights\n",
    "#  to f.\n",
    "fdes = np.random.randn(m)\n",
    "\n",
    "lam = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following demonstrates that the $\\hat{f}$ obtained via Convex optimization is the same as the minimizer $x$ obtained using the Proximal route."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fhat = cp.Variable(m)\n",
    "cp.Problem(cp.Minimize(1 / lam * cp.sum_squares(fhat - f)),\n",
    "            [fhat == fdes]).solve()\n",
    "\n",
    "equality = rsw.EqualityLoss(fdes)   \n",
    "np.testing.assert_allclose(fhat.value, equality.prox(f, lam))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inequality Loss\n",
    "Constrain $x$ to a range around $f_{\\text{des}}$ so let $l(x, f_{des}) = 0$ for $x \\in [f_{des}+\\text{lower}, f_{des}+\\text{upper}]$ and $+\\infty$ otherwise. Here the lower and upper refer to some acceptable range around each entry in $f_{des}.$\n",
    "\n",
    "### Convex Optimization Version\n",
    "\n",
    "$$\\min \\frac{1}{\\lambda}||\\hat{f}-f||^{2}_{2}$$\n",
    "$$\\text{subject to:}\\quad f^{i}_{\\text{des}} + \\text{lower}^{i} \\leq \\hat{f}_{i} \\leq f^{i}_{\\text{des}} + \\text{upper}^{i},\\quad i=1,\\ldots,m$$\n",
    "\n",
    "So ideally we want to set each $\\hat{f}_{i}$ to $f_{i}$ to minimize the objective function. But what if $f_{i} > f^{i}_{\\text{des}} + \\text{upper}^{i}$? In this case, accepting such an $\\hat{f}_{i}$ leads to infeasibility and to avoid this we will clip $\\hat{f}_{i}$ to $f^{i}_{\\text{des}} + \\text{upper}^{i}$. We cannot clip any lower because then we would be incurring additional loss (over and above that resulting from setting to $f^{i}_{\\text{des}} + \\text{upper}^{i}$) in the objective function unnecessarily. \n",
    "\n",
    "Similary, we clip $\\hat{f}_{i}$ to $f^{i}_{\\text{des}} + \\text{lower}^{i}$ if we find that $f_{i} < f^{i}_{\\text{des}} + \\text{lower}^{i}$.\n",
    "\n",
    "### Proximal Version\n",
    "$\\mathbf{prox}_{\\lambda l}(f_{\\text{des}}) = argmin_{x} \\left( l(x, f_{des}) + \\frac{1}{2\\lambda}||x-f_{\\text{des}}||^{2}_{2} \\right)$\n",
    "\n",
    "Thus, $\\mathbf{prox}_{\\lambda l}(f_{\\text{des}})_{i} = \\begin{cases} f^{i}_{\\text{des}} + \\text{lower}^{i}, \\text{if } f_{i} \\leq f^{i}_{\\text{des}} + \\text{lower}^{i}\\\\ f^{i}_{\\text{des}} + \\text{upper}^{i}, \\text{if } f_{i} \\geq f^{i}_{\\text{des}} + \\text{upper}^{i}\\\\ f_{i}, \\text{ otherwise}\\end{cases}$\n",
    "\n",
    "So we see that the Proximal version when passed the input (vector) $f$ and the Convex optimization version will both end up with the same minimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower = np.array([-.3])\n",
    "upper = np.array([.3])\n",
    "\n",
    "fhat = cp.Variable(m)\n",
    "cp.Problem(cp.Minimize(1 / lam * cp.sum_squares(fhat - f)),\n",
    "            [lower <= fhat - fdes, fhat - fdes <= upper]).solve()\n",
    "\n",
    "inequality = rsw.InequalityLoss(fdes, lower, upper)           \n",
    "np.testing.assert_allclose(fhat.value, inequality.prox(f, lam))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boolean Loss\n",
    "\n",
    "$l(x) = \\begin{cases}0 \\quad x \\in \\{0, 1/k\\}^{n} \\\\ \\infty \\quad \\text{Otherwise}\\end{cases}$\n",
    "\n",
    "We want $x_{i} = 1/k$ for $k < n$ samples (and $0$ for the others). Choosing any other value results an infinite loss (think infeasibility).\n",
    "\n",
    "$$\\mathbf{prox}_{\\lambda l}(f_{\\text{des}}) = argmin_{x}( l(x) + \\frac{1}{2\\lambda}||x - f_{des}||^{2}_{2} )$$\n",
    "\n",
    "The proximal operator of $l$ is the projection of $f_{des}$ onto the (nonconvex) set $\\{x \\in \\{0, 1/k\\}^{n} | \\mathbf{1}^{T}x=1\\}$. If each component of $x$ is either $0$ or $1/k$ then the constraint $\\mathbf{1}^{T}x=1$ means that exactly $k$ of them are set to $1/k$ and the remaining are set to $0$ (as was desired).\n",
    "\n",
    "The solution here is to set the largest $k$ entries of $f_{des}$ to $1/k$ and the remaining to $0$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boolean = rsw.BooleanRegularizer(3)\n",
    "np.testing.assert_allclose(np.array([1/3, 0, 0, 1/3, 1/3]), boolean.prox(np.array([5, 1, 0, 2, 4]), lam))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Weighted) Least Squares Loss\n",
    "\n",
    "$$\\min \\frac{1}{2}\\sum_{i=1}^{m}d^{2}_{i}(\\hat{f}_{i} - f^{i}_{des})^2 + \\frac{1}{2\\lambda}||\\hat{f} - f_{des}||^{2}_{2}$$\n",
    "\n",
    "Taking the derivative with respect to $\\hat{f}_{i}$ and setting equal to zero we get:\n",
    "\n",
    "$$d^{2}_{i}(\\hat{f}_{i} - f^{i}_{des}) + \\frac{1}{\\lambda}(\\hat{f}_{i} - f^{i}_{des}) = 0$$\n",
    "\n",
    "$$\\hat{f}_{i}( d^{2}_{i} + \\frac{1}{\\lambda} ) = \\frac{f^{i}_{des}}{\\lambda} + d^{2}_{i}f^{i}_{des}$$\n",
    "\n",
    "Finally we get that $\\hat{f}_{i} = \\frac{ d^{2}_{i}f^{i}_{des} + \\frac{f^{i}_{des}}{\\lambda} }{ ( d^{2}_{i} + \\frac{1}{\\lambda} ) }$\n",
    "\n",
    "The $d$'s are called the diagonals in the code and are used to weight each sample (row) in the data. Imagine a matrix with zeros everywhere but for the diagonal entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.random.uniform(0, 1, size=m)\n",
    "\n",
    "fhat = cp.Variable(m)\n",
    "cp.Problem(cp.Minimize(1 / 2 * cp.sum_squares(cp.multiply(d, fhat - fdes)) +\n",
    "                        1 / (2 * lam) * cp.sum_squares(fhat - f))).solve()\n",
    "\n",
    "lstsq = rsw.LeastSquaresLoss(fdes, d)                        \n",
    "np.testing.assert_allclose(fhat.value, lstsq.prox(f, lam))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy Loss\n",
    "\n",
    "$$\\min \\sum_{i=1}^{m}\\hat{f}_{i}\\ln\\hat{f}_{i} + \\frac{1}{2\\lambda}||\\hat{f} - f||^{2}_{2}$$\n",
    "\n",
    "Taking the derivative with respect to $\\hat{f}_{i}$ and setting equal to zero we get:\n",
    "\n",
    "$$1 + \\ln\\hat{f}_{i} + \\frac{1}{\\lambda}(\\hat{f}_{i} - f_{i}) = 0$$\n",
    "\n",
    "Rearranging terms:\n",
    "$$ \\ln\\hat{f}_{i} + \\frac{\\hat{f}_{i}}{\\lambda} = \\frac{f_{i}}{\\lambda} - 1 $$ \n",
    "\n",
    "Exponentiating both sides and then dividing both sides by $\\lambda$:\n",
    "$$\\frac{\\hat{f}_{i}}{\\lambda}e^{\\frac{\\hat{f}_{i}}{\\lambda}} = \\frac{1}{\\lambda}e^{\\frac{f_{i}}{\\lambda} - 1 }$$\n",
    "\n",
    "Applying the Lambert W function on both sides:\n",
    "$$W(\\frac{\\hat{f}_{i}}{\\lambda}e^{\\frac{\\hat{f}_{i}}{\\lambda}}) = W(\\frac{1}{\\lambda}e^{\\frac{f_{i}}{\\lambda} - 1 })$$\n",
    "\n",
    "Finally, $\\hat{f}_{i} = \\lambda W(\\frac{1}{\\lambda}e^{\\frac{f_{i}}{\\lambda} - 1 })$.\n",
    "\n",
    "Just for fun, suppose that each instance of $\\lambda$ and $f_{i}$ in the above result were replaced by $0.5\\lambda$ and $f_{i} + 0.5\\lambda \\ln f^{i}_{des}$ respectively then:\n",
    "\n",
    "$\\hat{f}_{i} = 0.5\\lambda W(\\frac{1}{0.5\\lambda}e^{\\frac{f_{i} + 0.5\\lambda \\ln f^{i}_{des}}{0.5\\lambda} - 1 }) = 0.5\\lambda W(\\frac{f^{i}_{des}}{0.5\\lambda}e^{\\frac{f_{i}}{0.5\\lambda} - 1 })$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = np.random.uniform(0, 1, size=m)\n",
    "f /= f.sum()\n",
    "\n",
    "fhat = cp.Variable(m)\n",
    "cp.Problem(cp.Minimize(cp.sum(-cp.entr(fhat)) +\n",
    "                        1 / (2 * lam) * cp.sum_squares(fhat - f))).solve()\n",
    "np.testing.assert_allclose(\n",
    "    fhat.value, rsw.losses._entropy_prox(f, lam), atol=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy Regularizer\n",
    "\n",
    "Minimize $r(w)$ where\n",
    "\n",
    "$$r(w) = \\begin{cases} \\sum_{i=1}^{n} w_{i}\\ln w_{i}, \\quad (1/(\\kappa n))\\mathbf{1} \\leq w \\leq (\\kappa/n )\\mathbf{1} \\\\ \\infty \\quad \\text{Otherwise}\\end{cases}$$\n",
    "\n",
    "Here $\\kappa > 1$ is a hyper-parameter (also called _limit_ in the code). Observe that there is a constraint on $w$ that says they must lie within $[1/(\\kappa n), \\kappa/n]$ otherwise the loss is infinite (i.e., we have an infeasible solution). If we interpret $w_i$ as a weight then the constraint says that no item can be down-weighted by less than $1/\\kappa$ or up-weighted by more than $\\kappa$.\n",
    "\n",
    "The final solution is $\\mathbf{prox}_{\\lambda r}(w_{\\text{des}})_{i} = \\begin{cases} 1/(\\kappa n), \\text{if } \\hat{w}_{i} \\leq 1/(\\kappa n)\\\\ \\kappa/n, \\text{if } \\hat{w}_{i} \\geq \\kappa/n\\\\ \\hat{w}_{i}, \\text{ otherwise}\\end{cases}$\n",
    "where $\\hat{w} = \\lambda W(\\frac{1}{\\lambda}e^{\\frac{w^{des}_{i}}{\\lambda} - 1 })$ from the Entropy Loss section ($w^{des}$ is some desired weight vector, provided as input, which we want $\\hat{w}$ to be close to)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KL Loss\n",
    "\n",
    "$$\\min \\frac{1}{2}\\left[ \\sum_{i=1}^{m}\\hat{f}_{i}\\ln\\hat{f}_{i} - \\sum_{i=1}^{m}\\hat{f}_{i}\\ln f^{i}_{des}  \\right] + \\frac{1}{2\\lambda}||\\hat{f} - f||^{2}_{2}$$\n",
    "\n",
    "Taking the derivative with respect to $\\hat{f}_{i}$ and setting equal to zero we get:\n",
    "\n",
    "$$\\frac{1}{2}\\left[ 1 + \\ln\\hat{f}_{i} - \\ln f^{i}_{des} \\right] + \\frac{1}{\\lambda}(\\hat{f}_{i} - f_{i}) = 0$$\n",
    "\n",
    "Multiplying both sides by $2$ and then rearrange terms to obtain:\n",
    "$$\\ln\\hat{f}_{i} + \\frac{2\\hat{f}_{i}}{\\lambda} = \\frac{2f_{i}}{\\lambda} -1 + \\ln f^{i}_{des}$$\n",
    "\n",
    "Next, exponentiate both sides to get:\n",
    "$$\\hat{f}_{i}e^{\\frac{\\hat{f}_{i}}{0.5\\lambda}} = f^{i}_{des}e^{\\frac{f_{i}}{0.5\\lambda} -1}$$\n",
    "\n",
    "Divide both sides by $0.5\\lambda$ and then apply the Lambert W function to get:\n",
    "\n",
    "$$W(\\frac{\\hat{f}_{i}}{0.5\\lambda}e^{\\frac{\\hat{f}_{i}}{0.5\\lambda}}) = W(\\frac{f^{i}_{des}}{0.5\\lambda}e^{\\frac{f_{i}}{0.5\\lambda} -1})$$\n",
    "\n",
    "Thus, $\\hat{f}_{i} = 0.5\\lambda W(\\frac{f^{i}_{des}}{0.5\\lambda}e^{\\frac{f_{i}}{0.5\\lambda} -1}).$\n",
    "\n",
    "Hopefully, this reminds you of the last expression in the Entropy Loss section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = np.random.uniform(0, 1, size=m)\n",
    "f /= f.sum()\n",
    "\n",
    "fdes = np.random.uniform(0, 1, size=m)\n",
    "fdes /= fdes.sum()\n",
    "\n",
    "kl = rsw.KLLoss(fdes, scale=.5)\n",
    "fhat = cp.Variable(m, nonneg=True)\n",
    "cp.Problem(cp.Minimize(.5 * (cp.sum(-cp.entr(fhat) - cp.multiply(fhat, np.log(fdes)))) +\n",
    "                        1 / (2 * lam) * cp.sum_squares(fhat - f))).solve()\n",
    "np.testing.assert_allclose(fhat.value, kl.prox(f, lam), atol=1e-5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
